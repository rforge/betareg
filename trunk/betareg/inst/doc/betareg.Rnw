\documentclass[nojss,letter]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\author{Francisco Cribari-Neto\\Universidade Federal de Pernambuco \And
        Achim Zeileis\\WU Wirtschaftsuniversit\"at Wien}
\Plainauthor{Francisco Cribari-Neto, Achim Zeileis}

\title{Beta Regression in \proglang{R}}
\Plaintitle{Beta Regression in R}

\Keywords{beta regression, rates, proportions, \proglang{R}}
\Plainkeywords{beta regression, rates, proportions, R}

\Abstract{The class of beta regression models is commonly used by 
practitioners to model variates that assume values in the standard 
unit interval $(0,1)$. It is based on the assumption that the 
variate is beta-distributed and that its mean is related to a linear 
predictor that involves regressors and unknown repression parameters
through a link function. The model also includes a precision parameter. 
This paper describes the \pkg{betareg} package for \proglang{R}, which 
implements the class of beta regressions. 
}

\Address{
  Francisco Cribari-Neto\\
  Departamento de Estat{\'{\i}}stica, CCEN\\
  Universidade Federal de Pernambuco\\
  Cidade Universit{\'a}ria\\
  Recife/PE 50740-540, Brazil\\
  E-mail: \email{cribari@ufpe.br}\\
  URL: \url{http://www.de.ufpe.br/~cribari/}\\
  
  Achim Zeileis\\
  Department of Statistics and Mathematics\\
  WU Wirtschaftsuniversit\"at Wien\\
  Augasse 2--6\\
  1090 Wien, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://statmath.wu-wien.ac.at/~zeileis/}
}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{Beta Regression in R}
%\VignetteDepends{stats,betareg,car,lmtest,sandwich}
%\VignetteKeywords{beta regression, rates, proportions, R}
%\VignettePackage{betareg}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
library("betareg")
@


\begin{document}


\section{Introduction} \label{sec:intro}

How should one perform a regression analysis in which the dependent variable
(response), $y$, assume values in the standard unit interval $(0,1)$? The usual
practice used to be to transform the data so that the transformed response, say
$\tilde y$, assumes values in the real line and then use it in a standard linear
regression analysis. A commonly used transformation is the logit $\tilde y = 
\log(y/(1-y))$. This approach, nonetheless, has shortcomings. First, the
regression parameters are interpretable in terms of the mean of $\tilde y$, and
not in terms of the mean of $y$ (given Jensen's inequality). Second, regressions
involving rates and proportions are typically heteroskedastic: they display more
variation around the mean and less variation as we approach the lower and upper
limits of the standard unit interval. Finally, the distributions of rates and
proportions are typically asymmetric, and thus Gaussian-based approximations for
interval estimation and hypothesis testing can be quite inaccurate in small
samples. \cite{betareg:Ferrari+Cribari-Neto:2004} proposed a regression model
for \fixme{wording:} rates, proportions, income concetration indices and other continuous
variates that assume values in the standard unit interval. Since the model is
based on the assumption that the response is beta-distributed, they called their
model \textit{the beta regression model}. In their model, the regression
parameters are interpretable in terms of the mean of $y$ (the variable of
interest). Additionally, the model is naturally heteroskedastic and easily
accomodates asymmetries. A variant of the beta regression model that allows for
nonlinearities and variable dispersion was proposed by
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}. In particular, in this more
general model, the parameter that accounts for the precision of the data is not
assumed constant across observations is allowed to vary, which leads to the
\textit{variable dispersion beta regression model}. 

In this paper, we describe the \pkg{betareg} package for \proglang{R}, which can
be used to perform inference in (fixed and variable dispersion) beta
regressions. The paper unfolds as follows. In Section~\ref{sec:model}, we
introduce the model. In Section~\ref{sec:illustrations}, we present empirical
illustrations that use that from previously published research. Section
\ref{sec:conclusion} contains concluding remarks and directions for future
research and implementation. 


\section{Beta regression}\label{sec:model} 

The class of beta regregression models, as introduced by \cite{betareg:Ferrari+Cribari-Neto:2004}, is useful for modeling continuous variables that assume 
values in the standard unit interval $(0,1)$.\footnote{If the variable takes on 
values in $(a,b)$, $a<b$ ($a$ and $b$ known), one can model $(y-a)/(b-a)$.} 
\fixme{Comment on [0, 1], document in readme for FCN.} The 
model is based on an alternative parameterization of the beta density in terms of the 
variate mean and of a precision parameter. The beta density is usually expressed as 
\begin{equation*}
%\label{dens}
f(y;p,q) = \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)}y^{p-1}(1-y)^{q-1}, 
\quad 0<y<1, p,q >0, 
\end{equation*}
where $\Gamma(\cdot)$ is the gamma function. \cite{betareg:Ferrari+Cribari-Neto:2004} proposed a different parameterization by setting $\mu = p/(p+q)$ and $\phi = p+q$:
\begin{equation*}
f(y;\mu,\phi) = \frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}, \quad 0<y<1,
\end{equation*}
$0<\mu<1$ and $\phi>0$.
We write $y \,\sim\, \mathcal{B}(\mu, \phi)$. Here, $\E(y) = \mu$ and $\VAR(y) = \mu(1-\mu)/(1+\phi)$. The parameter $\phi$ is known as the precision parameter, since for fixed $\mu$, the larger $\phi$ the smaller the variance of $y$; $\phi^{-1}$ is a dispersion parameter. 

Let $y_1,\ldots,y_n$ be a random sample such that $y_i \sim 
{\mathcal{B}}(\mu_i,\phi)$, $i=1,\ldots,n$. The beta regression model is 
defined as 
\begin{equation*}
g(\mu_i)=x_{i}^\top \beta_i=\eta_i,
\end{equation*}
where $\beta=(\beta_1,\ldots,\beta_k)^\top$ is a $k \times 1$ vector of unknown 
regression parameters ($k<n$), $\eta_i$ is a linear 
predictor and $x_{i1},\ldots,x_{ik}$ are indepenedent variables (covariates). Here, $g(\cdot): (0,1) \mapsto \mathrm{I\! R}$ is a link function, which is strictly increasing and twice differentiable. Some usueful link functions are: (i) logit: 
$g(\mu) = \log(\mu/(1-\mu))$; (ii) probit: $g(\mu) = \Phi^{-1}(\mu)$, where $\Phi(\cdot)$ is the standard normal distribution function; (iii) complementary log-log:
$g(\mu) = \log\{-\log(1-\mu)\}$; (iv) log-log: $g(\mu) = -\log\{-\log(\mu)\}$; and (v) Cauchy: $g(\mu) = \tan\{\pi(\mu - 0.5)\}$.
Note that the variance of $y$ is a function of $\mu$ which renders the regression model based on this parameterization naturally heteroskedastic. In particular, 
\begin{equation}
\label{eqn:variance}
\VAR(y_i) = \frac{\mu_i(1-\mu_i)}{1+\phi} = 
\frac{g^{-1}(x_i^{\top}\beta)[1-g^{-1}(x_i^{\top}\beta)]}{1+\phi}, 
\end{equation} 
where $x_i = (x_{i1},\ldots,x_{ik})^{\top}$. 
  
The log-likelihood function is $\ell(\beta,\phi)=\sum_{i=1}^n\ell_i(\mu_i,\phi)$, where 
\begin{align*}
\ell_i(\mu_i,\phi)&=\log \Gamma(\phi)-\log\Gamma(\mu_i\phi)-\log\Gamma((1-\mu_i)\phi) +(\mu_i\phi-1)\log y_i\\ &+\{(1-\mu_i)\phi-1\}\log(1-y_i).
\end{align*}
Notice that $\mu_i=g^{-1}(\eta_i)$ is a function of $\beta$, the vector of regression parameters. Parameter estimation is performed by numerically maximizing the log-likelihood function. 

An extension of the beta regression model described above was proposed by \cite{betareg:Simas+Barreto-Souza+Rocha:2010}. They considered a variable dispersion beta regression model in which the precision parameter is not constant for all observations. Instead, it is modelled in a similar fashion as the mean parameter. They also allow for nonlinearities. In their model, $y\,\sim\, {\mathcal B}(\mu_i, \phi_i)$, $i=1,\ldots,n$, and 
\begin{align}
g_1(\mu_i) &= \eta_{1t} = f_1(x_i^{\top}; \beta),\notag\\
g_2(\phi_i) &= \eta_{2t} = f_2(z_i^{\top}; \gamma),\label{eqn:variabledispersion}
\end{align}
where $\beta=(\beta_1, \ldots, \beta_k)^{\top}$, $\gamma=(\gamma_1,\ldots,\gamma_h)^{\top}$, $k+h<n$, $\eta_{1t}$ and $\eta_{2t}$ are predictors, and $x_{i1},\ldots,x_{iq_1}$ and $z_{i1}, \ldots, z_{iq_2}$ are covariates. It is assumed that the matrices $\partial\eta_1/\partial\beta$ and $\partial\eta_2/\partial\gamma$ have full column rank. Again, estimation is carried out by maximum likelihood. As before, the maximum likelihood estimators do not have closed form and are computed by numerically maximizing the log-likelihood function, which is as given above with $\phi$ replaced by $\phi_i$.\footnote{They have also obtained analytical bias corrections for the maximum likelihood estimators of the parameters, thus generalizing the results of \cite{betareg:Ospina+Cribari-Neto+Vasconcellos:2006}, who derived bias corrections for fixed dispersion beta regressions.} 

\cite{betareg:Ferrari+Cribari-Neto:2004} defined the following \textit{`standardized ordinary residual'}: 
$$r_i = \frac{y_i - \hat{\mu}_i}{\sqrt{\widehat{\rm var}(y_i)}},$$
where
$\widehat{\rm var}(y_i) = \hat{\mu}_i(1-\hat{\mu}_i)/
(1+\widehat{\phi})$. Here, $\hat{\mu}_i = g^{-1}(x_i^{\!\top}\,\hat{\beta})$,
$\hat{\beta}$ and $\hat\phi$ being the maximum likelihood estimators of $\beta$ and $\phi$, respectively. A better residual was proposed by \cite{betareg:Espinheira+Ferrari+Cribari-Neto:2008a}, which they named \textit{`standardized residual 2'}:
\begin{equation*}
r^{ww}_i = \frac{y^*_i - {\hat{{\mu}}^*}_i}{\sqrt{\hat{v}_i(1 - h_{ii})}},
\label{eqn:residual2}
\end{equation*}
where $y_i^* = \log\{ y_i / (1-y_i)\}$ and  $\mu_i^* = \psi(\mu_i\phi)- \psi((1-\mu_i)\phi)$, $\psi(\cdot)$ denoting the digamma function, i.e., $\psi(z) = {\rm d} \log \Gamma(z)/ {\rm d}z$ for $z > 0$.\footnote{It is noteworthy that ${\mu}^*_i = \E(y^*_i)$.} Also, $h_{ii}$ denotes the $t$th diagonal element of $H ={\hat{W}}^{1/2}X(X^{\!\top}\hat{W}X)^{-1}X^{\!\top} {\hat{W}}^{1/2}$. Here, $W$ is $n\times n$ diagonal matrix with typical element given by $w_i = \phi v_i [1 / \{g'(\mu_i)\}^2]$, with $v_i = \left\{ \psi'(\mu_i\phi) + \psi'((1-\mu_i)\phi)\right\}$. Finally, hats denote evaluation at the maximum likelihood estimates. 

A RESET-type misspecification test for fixed dispersion beta regression, similar to that of \cite{betareg:Ramsey:1969} for linear regressions, was proposed by \cite{betareg:Cribari-Neto+Lima:2007}. The authors considered different variants of the test and concluded that the best performing testing strategy in finite samples is to include $\hat\eta_i^2=(x_i^{\top}\hat\beta)^2$, where $\hat{\beta}$ denotes the maximum likelihood estimator of $\beta$, as an additional regressor in an augmented (artificial) regression, and then test its exclusion using a score test. Rejection of the null hypothesis suggests that the model is misspecified. Misspecification can follow, e.g., from incorrectly specifying the link function, from neglecting nonlinearities or from omitting important regressors. 

%other approaches by \cite{betareg:Paolino:2001} and
%\cite{betareg:Kieschnick+McCullough:2003}

%related to generalized linear models \citep[GLM,][]{betareg:McCullagh+Nelder:1989}
%but not part of the GLM family


\fixme{Z: basics of implementation approach}

implementation ideas from \cite{betareg:Zeileis+Kleiber+Jackman:2008},
employs two-part formulas based on \cite{betareg:Zeileis+Croissant:2009},
interface including estimating functions from \cite{betareg:Zeileis:2004,betareg:Zeileis:2006a},
can be plugged into generic inference tools from \pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}
and \pkg{car} \citep{betareg:Fox:2002} such as \code{coeftest}, \code{lrtest}, \code{waldtest},
or \code{linear.hypothesis}.

package for the \proglang{R} system for statistical computing \citep{betareg:R:2009}
available from the Comprehensive \proglang{R} Archive Network (CRAN) at
\url{http://CRAN.R-project.org/package=betareg}.

The initial version was written by \cite{betareg:Simas+Rocha:2006} up to version~1.2
which was orphaned and archived on CRAN in mid-2009. Achim Zeileis took over maintenance
after rewriting the code, starting from version~2.0-0.


\section[Implementation in R]{Implementation in \proglang{R}}

\fixme{Z: description of implementation, text still needs improvements}

To turn the model described in the previous section into computational tools,
a fairly standard approach in \proglang{R} is employed: There is a model-fitting
function \fct{betareg} which takes \code{formula} plus \code{data} for data
specification, then sets up the likelihood and corresponding gradient (or estimating
function), calls \fct{optim} for maximizing the likelihood, and finally returns
an object of \proglang{S}3 class \class{betareg} for which a large set of methods to standard
generics is available. The workhorse function is \fct{betareg.fit} which provides
the core computations without \code{formula}-related data pre- and post-processing.

The model-fitting function \fct{betareg} and its associated class are designed to
be as similar as possible to the standard \fct{glm} function \citep{betareg:R:2009}
for fitting GLMs. An important difference is that there are potentially two equations
for mean and precision \fixme{formula ref}, respectively, and consequently two regressor matrices,
two linear predictors, two sets of coefficients, etc. The design of \fct{betareg}
is similar to the functions described by \cite{betareg:Zeileis+Kleiber+Jackman:2008}
for fitting zero-inflation and hurdle models which also have two model components.
The arguments of \fct{betareg} are
%
\begin{Sinput}
betareg(formula, data, subset, na.action, weights, offset,
  link = "logit", link.phi = NULL, control = betareg.control(...),
  model = TRUE, y = TRUE, x = FALSE, ...)
\end{Sinput}
%
where the first line contains the standard model-frame specifications
\citep[see][]{betareg:Chambers+Hastie:1992},
the second and third lines have the arguments specific to beta regression models and
the arguments in the last line control some components of the return value.

If a \code{formula} of type \code{y ~ x1 + x2} is supplied, it describes $y_i$ and $x_i$ 
for the mean equation of the beta regression \fixme{formula ref}. In this case a constant
$\phi_i$ is assumed, i.e., $z_i = 1$ and $g_2$ is the identity link, corresponding
to the basic beta regression model as introduced in \cite{betareg:Ferrari+Cribari-Neto:2004}.
However, a second set of regressors can be specified by a two-part formula of type
\code{y ~ x1 + x2 | z1 + z2 + z3} as provided in the \pkg{Formula} package
\citep{betareg:Zeileis+Croissant:2009}. This model has the same mean equation as above but
the regressors $z_i$ in the precision equation are taken from the \code{~ z1 + z2 + z3} part.
The default link function in this case is the log link $g_2(\cdot) = \log(\cdot)$.
Consequently, \code{y ~ x1 + x2} and \code{y ~ x1 + x2 | 1} correspond to equivalent
beta likelihoods but use different parametrizations for $\phi_i$: simply $\phi_i = \gamma_1$
in the former case and $\log(\phi_i) = \gamma_1$ in the latter case. The link for the
$\phi_i$ precision equation can be changed by \code{link.phi} in both cases where
\code{"identity"}, \code{"log"}, and \code{"sqrt"} are allowed as admissible values.
The default for the $\mu_i$ mean equation is always the logit link but all link
functions for the \code{binomial} family in \fct{glm} are allowed as well as the log-log
link: \code{"logit"}, \code{"probit"}, \code{"cloglog"}, \code{"cauchit"},
\code{"log"}, and \code{"loglog"}.

ML estimation of all parameters employing analytical gradients is carried out
using \proglang{R}'s \fct{optim} with control options set in \fct{betareg.control}.
Starting values can be user-supplied, otherwise the $\beta$ starting values are estimated by 
a regression of $g_1(y_i)$ on $x_i$ and similarly
the $\gamma$ starting values are obtained from a regression of a transformed
$y_i$ on the $z_i$. The transformed $y_i$ are derived in \cite{betareg:Ferrari+Cribari-Neto:2004}
where only their mean is used as the starting values (corresponding to constant $\phi_i$
with identity link). The covariance matrix estimate is derived analytically as in
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}. However, by setting \code{hessian = TRUE}
the numerical Hessian matrix returned by \fct{optim} can also be obtained.

The returned fitted-model object of class \class{betareg} is a list similar
to \class{glm} objects. Some of its elements---such as \code{coefficients} or
\code{terms}---are lists with a mean and precision component,
respectively.

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|p{10cm}|}
\hline
Function & Description \\ \hline
\fct{print} & simple printed display with coefficient estimates\\
\fct{summary} & standard regression output (coefficient estimates, standard errors, partial Wald tests);
                returns an object of class \class{summary.betareg}
                containing the relevant summary statistics (which has a \fct{print} method) \\ 	\hline
\fct{coef} & extract coefficients of model (full or mean/precision components), a single vector of all coefficients by default \\
\fct{vcov} & associated covariance matrix (with matching names) \\
\fct{predict} & predictions (of means $\mu_i$, linear predictors $\eta_i$,
                precision parameter $\phi_i$, or variances $\mu_i (1 - \mu_i) / (1 + \phi_i)$) for new data \\
\fct{fitted} & fitted means for observed data \\
\fct{residuals} & extract residuals \citep[deviance, Pearson, response,
  or different weighted residuals, see][]{betareg:Espinheira+Ferrari+Cribari-Neto:2008a} \\ 
\fct{estfun} & compute empirical estimating functions (or score functions),
  evaluated at observed data and estimated parameters \citep[see][]{betareg:Zeileis:2006a} \\
\fct{bread} & extract ``bread'' matrix for sandwich estimators
  \citep[see][]{betareg:Zeileis:2006a} \\ \hline
\fct{terms} & extract terms of model components \\
\fct{model.matrix} & extract model matrix of model components \\
\fct{model.frame} & extract full original model frame \\
\fct{logLik} & extract fitted log-likelihood \\ \hline
\fct{plot} & diagnostic plots of residuals, predictions, leverages etc. \\
\fct{hatvalues} & hat values (diagonal of hat matrix) \\
\fct{cooks.distance} & (approximation of) Cook's distance \\
\fct{gleverage} & compute generalized leverage \citep{betareg:Wei+Hu+Fung:1998};
  based on the formula derived for fixed $\phi$ \\ \hline
\fct{coeftest} & partial Wald tests of coefficients \\
\fct{waldtest} & Wald tests of nested models \\
\fct{linear.hypothesis} & Wald tests of linear hypotheses \\
\fct{lrtest} & likelihood ratio tests of nested models \\
\fct{AIC} & compute information criteria (AIC, BIC, \dots) \\ \hline
\end{tabular}
\caption{\label{tab:methods} Functions and methods for objects of class \class{betareg}.
  The first four blocks refer to methods, the last block contains generic
  functions whose default methods work because of the information supplied by the methods above.}
\end{center}
\end{table}

A set of standard extractor functions for fitted model objects is available for
objects of class \class{betareg}, including the usual \fct{summary} method that
provides partial Wald tests for all coefficients. No \fct{anova} method is provided,
but the general \fct{coeftest}, \fct{waldtest} from \pkg{lmtest}, and \fct{linear.hypothesis}
from \pkg{car} can be used for Wald tests and \fct{lrtest} from \pkg{lmtest}
for likelihood-ratio tests of nested models. See Table~\ref{tab:methods} for a list
of all available methods. Most of these are standard in base \proglang{R}, however, methods
to a few less standard generics are also provided. Specifically, there are tools related
to specification testing and computation of sandwich covariance matrices as discussed
by \cite{betareg:Zeileis:2006a} as well as a method to a new generic for computing
generalized leverages \citep{betareg:Wei+Hu+Fung:1998}.


\section{Beta regression in practice} \label{sec:illustrations}

\fixme{Z/FCN: illustrate most important usage cases, replicate results
from other papers -- some text already written, still needs refinement}

To illustrate the usage of \pkg{betareg} in practice we replicate
(and slightly extend) some of the analyses from the original papers
that suggested the methodology. More specifically, we estimate and
compare various flavours of beta regression models for the gasoline
yield of \cite{betareg:Prater:1956}, see Figure~\ref{fig:GasolineYield},
and for the household food expenditure data taken from
\cite{betareg:Griffiths+Hill+Judge:1993}, see Figure~\ref{fig:FoodExpenditure}. 


\subsection{The basic model: Estimation, inference, diagnostics}

\subsubsection{Prater's gasoline yield data}

<<GasolineYield-betareg, echo=FALSE>>=
data("GasolineYield", package = "betareg")
gy_logit <- betareg(yield ~ batch + temp, data = GasolineYield)
@

<<GasolineYield-loglog, echo=FALSE>>=
gy_loglog <- betareg(yield ~ batch + temp, data = GasolineYield,
  link = "loglog")
@

The basic beta regression model as suggested by \cite{betareg:Ferrari+Cribari-Neto:2004}
is illustrated in the Section~4 of their paper using two empirical examples.
The first example employs the well-known gasoline yield taken from
\cite{betareg:Prater:1956}. The variable interest is \code{yield},
the proportion of crude oil converted to gasoline after distillation
and fractionation, for which a beta regression model seems to be rather
natural. \cite{betareg:Ferrari+Cribari-Neto:2004} employ two explanatory
variables: \code{temp}, the temperature (in degrees Fahrenheit) at which all
gasoline has vaporized, and \code{batch}, a factor indicating ten unique batches
of conditions in the experiments (depending on further variables). The
data, encompassing \Sexpr{nrow(GasolineYield)}~observations, is visualized
in Figure~\ref{fig:GasolineYield}.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.75\textwidth}
<<GasolineYield-visualization, echo=FALSE, fig=TRUE, width=6, height=5.5>>=
plot(yield ~ temp, data = GasolineYield, type = "n",
  ylab = "Proportion of crude oil converted to gasoline",
  xlab = "Temperature at which all gasoline has vaporized",
  main = "Prater's gasoline yield data")
points(yield ~ temp, data = GasolineYield, cex = 1.75, 
  pch = 19, col = rev(gray.colors(10))[as.numeric(batch)])
points(yield ~ temp, data = GasolineYield, cex = 1.75)
legend("topleft", as.character(1:10), title = "Batch",
  col = rev(gray.colors(10)), pch = 19, bty = "n")
legend("topleft", as.character(1:10), title = "Batch", pch = 1, bty = "n")
lines(150:500, predict(gy_logit, 
  newdata = data.frame(temp = 150:500, batch = "6")),
  col = 4, lwd = 2, lty = 2)
lines(150:500, predict(gy_loglog, 
  newdata = data.frame(temp = 150:500, batch = "6")),
  col = 2, lwd = 2)
legend("bottomright", c("log-log", "logit"),
  col = c(2, 4), lty = 1:2, lwd = 2, bty = "n")
@
\caption{\label{fig:GasolineYield} Gasoline yield from \cite{betareg:Prater:1956}:
  Proportion of crude oil converted to gasoline explained by temperature
  (in degrees Fahrenheit) at which all gasoline has vaporized and given batch
  (indicated by gray level). Fitted curves correspond to beta regressions 
  \code{gy\_loglog} with log-log link (solid, red) and \code{gy\_logit} with
  logit link (dashed, blue). Both curves were evaluated at varying temperature
  with the intercept for batch 6 (i.e., roughly the average intercept).}
\end{center}
\end{figure}

\cite{betareg:Ferrari+Cribari-Neto:2004} start out with a model where
\code{yield} depends on \code{batch} and \code{temp}, employing the standard
logit link. In \pkg{betareg}, this can be fitted via
%
<<GasolineYield-betareg1>>=
<<GasolineYield-betareg>>
summary(gy_logit)
@
%
which replicates their Table~1. The goodness of fit is assessed using
different types of diagnostic displays shown in their Figure~2. This
graphic can be reproduced (in a slightly different order) using the
\fct{plot} method for \class{betareg} objects, see Figure~\ref{fig:GasolineYield-plot}.
\fixme{Z: omit nsim before release, just used at the moment to make recompilation
of the .Rnw more convenient}
%
<<GasolineYield-plot, eval=FALSE>>=
set.seed(123)
plot(gy_logit, which = 1:4)
plot(gy_logit, which = 5, type = "deviance", sub.caption = "", nsim = 5)
plot(gy_logit, which = 1, type = "deviance", sub.caption = "")
@
%
As observation~4 corresponds to a large Cook's distance and large
residual, \cite{betareg:Ferrari+Cribari-Neto:2004} decided to refit
the model excluding this observation. While this does not change the
coefficients in the mean model very much, the precision parameter
$\phi$ increases clearly.
%
<<GasolineYield-update>>=
gy_logit4 <- update(gy_logit, subset = -4)
coef(gy_logit, model = "precision")
coef(gy_logit4, model = "precision")
@

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<GasolineYield-plot1, echo=FALSE, fig=TRUE, width=8.5, height=10>>=
par(mfrow = c(3, 2))
<<GasolineYield-plot>>
@
\caption{\label{fig:GasolineYield-plot} Diagnostic plots for
  beta regression model \code{gy\_logit}.}
\end{center}
\end{figure}


\subsubsection{Household food expenditures}

<<FoodExpenditure-lm, echo=FALSE>>=
data("FoodExpenditure", package = "betareg")
fe_lm <- lm(I(food/income) ~ income + persons, data = FoodExpenditure)
@

<<FoodExpenditure-betareg, echo=FALSE>>=
fe_beta <- betareg(I(food/income) ~ income + persons,
  data = FoodExpenditure)
@

<<FoodExpenditure-betareg2, echo=FALSE>>=
fe_beta2 <- betareg(I(food/income) ~ income + persons | persons,
  data = FoodExpenditure)
@

\cite{betareg:Ferrari+Cribari-Neto:2004} also consider a second
example: household food expenditure data for \Sexpr{nrow(FoodExpenditure)}~households
taken from \citet[][Table~15.4]{betareg:Griffiths+Hill+Judge:1993}.
The dependent variable is \code{food/income}, the proportion of household
\code{income} spent on \code{food}. Two explanatory variables are available:
the previously mentioned household \code{income} and the number of \code{persons}
living in the household. All three variables are visualized in Figure~\ref{fig:FoodExpenditure}.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.75\textwidth}
<<FoodExpenditure-visualization, echo=FALSE, fig=TRUE, width=6, height=5.5>>=
plot(I(food/income) ~ income, data = FoodExpenditure,
  xlab = "Household income", ylab = "Proportion of food expenditures",
  main = "Food expenditures data", type = "n", ylim = c(0.04, 0.57))
points(I(food/income) ~ income, data = FoodExpenditure, cex = 1.75, pch = 19,
  col = rev(gray.colors(7))[persons])
points(I(food/income) ~ income, data = FoodExpenditure, cex = 1.75)
legend("bottomleft", rev(as.character(sort(unique(FoodExpenditure$persons)))),
  title = "Persons", col = gray.colors(7), pch = 19, bty = "n")
legend("bottomleft", rev(as.character(sort(unique(FoodExpenditure$persons)))),
  title = "Persons", pch = 1, bty = "n")
lines(10:100, predict(fe_lm, 
  newdata = data.frame(income = 10:100, persons = mean(FoodExpenditure$persons))),
  col = 1, lwd = 2, lty = 2)
lines(10:100, predict(fe_beta, 
  newdata = data.frame(income = 10:100, persons = mean(FoodExpenditure$persons))),
  col = 4, lwd = 2, lty = 5)
lines(10:100, predict(fe_beta2, 
  newdata = data.frame(income = 10:100, persons = mean(FoodExpenditure$persons))),
  col = 2, lwd = 2)
legend("topright", c("logit, var. disp.", "logit, fix. disp.", "lm"),
  col = c(2, 4, 1), lty = c(1, 5, 2), lwd = 2, bty = "n")
@
\caption{\label{fig:FoodExpenditure} Household food expenditure data from
  \cite{betareg:Griffiths+Hill+Judge:1993}: Proportion of household income
  spent on food explained by household income and number of persons in
  household (indicated by gray level). Fitted curves correspond to beta regressions 
  \code{fe\_beta} with fixed dispersion (long-dashed, blue),
  \code{fe\_beta2} with variable dispersion (solid, red),
  and the linear regression \code{fe\_lin} (dashed, black). All curves were evaluated
  at varying income with the intercept for mean number of persons
  ($ = \Sexpr{round(mean(FoodExpenditure$persons), digits = 2)}$).}
\end{center}
\end{figure}

To start their analysis, \cite{betareg:Ferrari+Cribari-Neto:2004} consider
a simple linear regression model fitted by ordinary least squares (OLS)
%
<<FoodExpenditure-lm1>>=
<<FoodExpenditure-lm>>
@
%
To show that this model exhibits heteroskedasticity, they employ 
the studentized \cite{betareg:Breusch+Pagan:1979} test of \cite{betareg:Koenker:1981}
which is available in \proglang{R} in the \pkg{lmtest} package
\citep{betareg:Zeileis+Hothorn:2002}.
%
<<FoodExpenditure-bptest>>=
library("lmtest")
bptest(fe_lm)
@
%
One alternative would be to consider a logit-transformed response in
a traditional OLS regression but this would make the residuals
asymmetric. However, both issues -- heteroskedasticity and skewness --
can be alleviated when a beta regression model with a logit link for
the mean is used.
%
<<FoodExpenditure-betareg1>>=
<<FoodExpenditure-betareg>>
summary(fe_beta)
@
%
This replicates Table~2 from \cite{betareg:Ferrari+Cribari-Neto:2004}.
The predicted means of the linear and the beta regression model, respecitvely,
are very similar: the proportion of household income spent on food decreases
with the overall income level but increases in the number of persons in the
household (see also Figure~\ref{fig:FoodExpenditure}).

Below, further extended models will be considered for these data sets and
hence all model comparisons are deferred.


\subsection{Variable dispersion model}

\subsubsection{Prater's gasoline yield data}

Although the beta model already incorporates naturally a certain
pattern in the variances of the response (see Equation~\ref{eqn:variance}), it might
be natural to incorporate further regressors to account for heteroskedasticity
as in Equation~\ref{eqn:variabledispersion} \citep{betareg:Simas+Barreto-Souza+Rocha:2010}.
For illustration of this approach, the example from Section~3 of the online
supplements to \cite{betareg:Simas+Barreto-Souza+Rocha:2010} is considered.
This investigates Prater's gasoline yield data based on the same
mean equation as above, but now with temperature \code{temp} as an 
additional regressor for the precision parameter $\phi_i$:
%
<<GasolineYield-phireg>>=
gy_logit2 <- betareg(yield ~ batch + temp | temp, data = GasolineYield)
@
%
for which \code{summary(gy_logit2)} yields the MLE column in Table~19
of \cite{betareg:Simas+Barreto-Souza+Rocha:2010}. To save space, only
the parameters pertaining to $\phi_i$ are reported here
%
<<GasolineYield-phireg-coef, echo=FALSE>>=
printCoefmat(summary(gy_logit2)$coefficients$precision)
@
%
that signal a significant improvement by including the \code{temp} regressor.
Instead of using this Wald test, the models can also be compared by means
of a likelihood-ratio test (see their Table~18) that confirms the results:
%
<<GasolineYield-lrtest>>=
lrtest(gy_logit, gy_logit2)
@

\subsubsection{Household food expenditures}

For the household food expenditure data, the Breusch-Pagan test carried out
above illustrated that there is heteroskedasticity that can be captured
by the regressors \code{income} and \code{persons}. Closer investigation
reveals that this is mostly due to the number of persons in the household,
also brought out graphically by some of the outliers with high values in
this variable in Figure~\ref{fig:FoodExpenditure}. Hence, it seems natural
to consider the model employed above with \code{persons} as an additional
regressor in the precision equation.
%
<<FoodExpenditure-betareg2a>>=
<<FoodExpenditure-betareg2>>
@
%
This leads to significant improvements in terms of the likelihood and
the associated BIC.
%
<<FoodExpenditure-comparison>>=
lrtest(fe_beta, fe_beta2)
AIC(fe_beta, fe_beta2, k = log(nrow(FoodExpenditure)))
@
%
Thus, the model \code{fe_beta2} seems to be preferable. As visualized in
Figure~\ref{fig:FoodExpenditure}, it describes a similar relationship between
response and explanatory variables although with a somewhat shrinked
\code{income} slope.


\subsection{Selection of different link functions}

\subsubsection{Prater's gasoline yield data}

As in binomial GLMs, selection of an appropriate link function 
can greatly improve the model fit \citep{betareg:McCullagh+Nelder:1989},
especially if extreme proportions (close to $0$ or $1$) have been
observed in the data. To illustrate this problem in beta regressions,
we replicate parts of the analysis in Section~5 of \cite{betareg:Cribari-Neto+Lima:2007}.
This reconsiders Prater's gasoline yield data but employs a log-log
link instead of the previously used (default) logit link
%
<<GasolineYield-loglog1>>=
<<GasolineYield-loglog>>
@
%
which clearly improves pseudo $R^2$ of the model:
%
<<GasolineYield-Rsquared>>=
summary(gy_logit)$pseudo.r.squared
summary(gy_loglog)$pseudo.r.squared
@
Similarly, the AIC\footnote{Note that \cite{betareg:Cribari-Neto+Lima:2007} did not
account for estimation of $\phi$ in their degrees of freedom. Hence, their reported
AICs differ by 2.} (and BIC) of the fitted model is not only
superior to the logit model with fixed dispersion \code{gy_logit}
but also to the logit model with variable dispersion \code{gy_logit2} considered
in the previous section.
%
<<GasolineYield-AIC>>=
AIC(gy_logit, gy_logit2, gy_loglog)
@
Moreover, if \code{temp} were included as a regressor in the precision
equation of \code{gy_loglog}, it would no longer yield significant improvements.
Thus, improvement of the model fit in the mean equation by adoption of the log-log
link have waived the need for an extended precision equation.

To underline the appropriateness of the log-log specification, 
\cite{betareg:Cribari-Neto+Lima:2007} consider a sequence of diagnostic
tests inspired by the RESET in linear regression models \citep{betareg:Ramsey:1969}.
To check for misspecifications, they consider powers of responses or linear predictors
to be included as auxiliary regressors in the mean equation. In well-specified
models, these should not yield significant improvements. For the gasoline yield
model, this can only be obtained for the log-log link while all other link
functions result in significant results indicating misspecification. Below,
this is exemplified for a likelihood-ratio test of squared linear predictors.
Analogous results can be obtained for \code{type = "response"} or higher powers.
%
<<GasolineYield-reset>>=
lrtest(gy_logit, . ~ . + I(predict(gy_logit, type = "link")^2))
lrtest(gy_loglog, . ~ . + I(predict(gy_loglog, type = "link")^2))
@
%
The improvement of the model fit can also be brought out graphically in a
display of predicted vs.\ observed values (see Figure~\ref{fig:GasolineYield-diagnostics}).
%
<<GasolineYield-diagnostics, eval=FALSE>>=
plot(gy_logit, which = 6)
plot(gy_loglog, which = 6)
@
%
This shows that especially for the extreme observations, the log-log link
leads to better predictions.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<GasolineYield-diagnostics1, echo=FALSE, fig=TRUE, width=8.5, height=4>>=
par(mfrow = c(1, 2))
<<GasolineYield-diagnostics>>
@
\caption{\label{fig:GasolineYield-diagnostics} Diagnostic plots:
  Predicted vs.\ observed values for beta regression model
  \code{gy\_logit} with logit link (left) and \code{gy\_loglog}
  with log-log link (right).}
\end{center}
\end{figure}

In principle, the link function $g_2$ in the precision equation could
also influence the model fit. However, as the best-fitting model 
\code{gy_loglog} has a constant $\phi$, all links $g_2$ lead to 
estimates of $\phi$ and thus to equivalent fitted log-likelihoods.
However, the link function can have consequences in terms of the
inference about $\phi$ and in terms of convergence of the optimization.
Typically, a log-link leads to somewhat improved quadratic approximations
of the likelihood and less iterations in the optimization. For example,
refitting \code{gy_loglog} with $g_2(\cdot) = \log(\cdot)$ converges
more quickly
%
<<GasolineYield-loglog>>=
gy_loglog2 <- update(gy_loglog, link.phi = "log")
summary(gy_loglog2)$iterations
@
%
with a lower number of iterations than for \code{gy_loglog}
which had \Sexpr{summary(gy_loglog)$iterations} iterations.

%% equivalently with
%% \code{betareg(yield ~ batch + temp | 1, data = GasolineYield, link = "loglog")}


\subsubsection{Household food expenditures}

In principle, one could conduct a similar analysis as above for the household
foox expenditure data. However, as the response takes less extreme observations
than for the gasoline yield data, the choice of link function is less important.
In fact, refitting the model with various link functions shows no large
differences in the resulting log-likelihoods.
%
<<FoodExpenditure-links>>=
sapply(c("logit", "probit", "cloglog", "cauchit", "loglog"), function(x)
  logLik(betareg(I(food/income) ~ income + persons | persons,
  link = x, data = FoodExpenditure)))
@
%
However, the Cauchy link performs slightly better than the logit link and might
hence deserve further investigation.



\section{Further replication exercises} \label{sec:replications}

We also consider an application that models reading accuracy data for nondyslexic
and dyslexic Australian children \citep{betareg:Smithson+Verkuilen:2006}.  


\subsection{Dyslexia and IQ predicting reading accuracy}

We shall now consider the data analyzed by \cite{betareg:Smithson+Verkuilen:2006}.\footnote{These
data were also analyzed by \cite{betareg:Espinheira+Ferrari+Cribari-Neto:2008b}, who have also
concluded that the dispersion is variable.}
The variable of interest ($y$) are scores on a test of 
reading accuracy taken by 44 children, and the independent variables 
are dyslexia versus non-dyslexia status ($x_2$), nonverbal IQ converted to 
$z$-scores ($x_3$) and an interaction variable ($x_4$). 
The sample includes 19 dyslexics and 25 controls who were recruited from primary 
schools in the Australian Capital Territory. The children's ages ranged from 
eight years five months to twelve years three months.   
The covariate $x_2$ equals 1 when the child is dyslexic and $-1$ otherwise. 
The mean accuracy score was 0.900 for non-dyslexic readers and 0.606 for dyslexics. 
The scores ranged from 0.459 to 0.990, and averaged 0.773. The variable dispersion beta
regression model is $\mathrm{logit}(\mu_i) = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} +
\beta_4 x_{i4}$ and $\log(\phi_i) = \gamma_1 + \gamma_2 x_3 + \gamma_3 x_4$.  
%
<<ReadingSkills-beta>>=
data("ReadingSkills", package = "betareg")
rs_beta <- betareg(accuracy ~ dyslexia * iq | dyslexia + iq,
  data = ReadingSkills, hessian = TRUE)
summary(rs_beta)
@
%
The point estimates above replicate those in Table~5 of \cite{betareg:Smithson+Verkuilen:2006},
except for the signs of the coefficients of the dispersion submodel, since they define such
a submodel in a different fashion.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.75\textwidth}
<<ReadingSkills-visualization, echo=FALSE, fig=TRUE, width=6, height=5.5>>=
rs_ols <- lm(qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)
cl1 <- hcl(c(260, 0), 90, 40)
cl2 <- hcl(c(260, 0), 10, 95)
plot(accuracy ~ iq, data = ReadingSkills, col = cl2[as.numeric(dyslexia)],
  pch = 19, xlab = "IQ score", ylab = "Reading accuracy", cex = 1.5)
points(accuracy ~ iq, data = ReadingSkills, col = cl1[as.numeric(dyslexia)], cex = 1.5)
nd <- data.frame(dyslexia = "no", iq = -30:30/10)
lines(nd$iq, predict(rs_beta, nd), col = cl1[1], lwd = 2)
lines(nd$iq, plogis(predict(rs_ols, nd)), col = cl1[1], lty = 2, lwd = 2)
nd <- data.frame(dyslexia = "yes", iq = -30:30/10)
lines(nd$iq, predict(rs_beta, nd), col = cl1[2], lwd = 2)
lines(nd$iq, plogis(predict(rs_ols, nd)), col = cl1[2], lty = 2, lwd = 2)
legend("topleft", c("control", "dyslexic", "betareg", "lm"),
  lty = c(NA, NA, 1:2), pch = c(19, 19, NA, NA), lwd = 2,
  col = c(cl2, 1, 1), bty = "n")
legend("topleft", c("control", "dyslexic", "betareg", "lm"),
  lty = c(NA, NA, 1:2), pch = c(1, 1, NA, NA),
  col = c(cl1, NA, NA), bty = "n")
@
\caption{\label{fig:ReadingSkills} FIXME.}
\end{center}
\end{figure}


\subsection{Structural change testing in beta regressions}

As already illustrated above, \class{betareg} objects can be plugged into
various inference functions from other packages because they provide suitable
methods to standard generic functions (see Table~\ref{tab:methods}). Hence
\fct{lrtest} could be used for performing likelihood-ratio tests and similarly
\fct{coeftest}, \fct{waldtest} from \pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}
and \fct{linear.hypothesis} from \pkg{car} \citep{betareg:Fox:2002} can be
employed for carrying out different flavors of Wald tests.

In this section, we illustrate yet another generic inference approach implemented
in the \pkg{strucchange} package for structural change testing. While originally
written for linear regression models \citep{betareg:Zeileis+Leisch+Hornik:2002},
\pkg{strucchange} was extended by \cite{betareg:Zeileis:2006} to compute generalized
fluctation tests for structural change in models that are based on suitable
estimating functions. If these estimating functions can be extracted by an
\fct{estfun} method, models can simply be plugged into the \fct{gefp} function
for computing generalized empirical fluctuation processes. To illustrate
this, we replicate the example from Section~5.3 in \cite{betareg:Zeileis:2006}.

Two artificial data sets are considered: a series \code{y1} with a change in
the mean $\mu$, and a series \code{y2} with a change in the precision $\phi$.
Both simulated series start with the parameters $\mu = 0.3$ and $\phi = 4$
and for the first series $\mu$ changes to $0.5$ at after 75\% of the observations
while $\phi$ remains constant whereas for the second series $\phi$ changes to $8$
after 50\% of the observations and $\mu$ remains constant.
%
<<strucchange-data>>=
set.seed(123)
y1 <- c(rbeta(150, 0.3 * 4, 0.7 * 4), rbeta(50, 0.5 * 4, 0.5 * 4))
y2 <- c(rbeta(100, 0.3 * 4, 0.7 * 4), rbeta(100, 0.3 * 8, 0.7 * 8))
@
%
To capture instabilities in the parameters over ``time'' (i.e., the ordering of
the observations), the generalized empirical fluctuation processes can be derived
via
%
<<strucchange-gefp>>=
library("strucchange")
y1_gefp <- gefp(y1 ~ 1, fit = betareg)
y2_gefp <- gefp(y2 ~ 1, fit = betareg)
@
%
and visualized by
%
<<strucchange-plot1, echo=FALSE, eval=FALSE>>=
plot(y1_gefp, aggregate = FALSE)
@
<<strucchange-plot2, echo=FALSE, eval=FALSE>>=
plot(y2_gefp, aggregate = FALSE)
@
<<strucchange-plot, echo=TRUE, eval=FALSE>>=
<<strucchange-plot1>>
<<strucchange-plot2>>
@
%
The resulting Figure~\ref{fig:strucchange} \citep[replicating Figure~4 from][]{betareg:Zeileis:2006}
shows two 2-dimensional fluctuation processes: one for \code{y1} (left) and one for \code{y2} (right).
Both fluctuation processes behave as expected: There is no excessive fluctuation of the 
process pertaining to the parameter that remained constant while there is a significant instability
in the parameter that changed signalled by a boundary crossing and a peak at about the time
of the change in the corresponding parameter.


\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.49\textwidth}
<<strucchange-plot1a, echo=FALSE, fig=TRUE, echo=FALSE, width=4.5, height=5>>=
<<strucchange-plot1>>
@
<<strucchange-plot2a, echo=FALSE, fig=TRUE, echo=FALSE, width=4.5, height=5>>=
<<strucchange-plot2>>
@
\caption{\label{fig:strucchange} Structural change tests
  for artificial data \code{y1} with change in $\mu$ (left)
  and \code{y2} with change in $\phi$ (right).}
\end{center}
\end{figure}


\section{Summary}\label{sec:conclusion}

\fixme{FCN/Z: todo}

This paper addressed the \proglang{R} implementation of the class of beta regression models available in the
\pkg{betareg} package. We have presented the fixed and variable dispersion beta regression models, described
how one can model rates and proportions using \pkg{betareg} and presented a few empirical examples. Future
research and implementation shall focus on the situation where the data contain zeros and/or ones
\citep[see, e.g.,][]{betareg:Kieschnick+McCullough:2003}. An additional line of research and
implementation is that of dynamic beta regression models, such as the class of BARMA models
proposed by \cite{betareg:Rocha+Cribari-Neto:2010}.  




\section*{Acknowledgments}

FCN gratefully acknowledges financial support from CNPq/Brazil. 
\fixme{Z: underline previous work by Simas/Rocha}


\bibliography{betareg}

\end{document}
