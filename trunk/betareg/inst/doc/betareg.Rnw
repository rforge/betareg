\documentclass[nojss,letter]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\author{Francisco Cribari-Neto\\Universidade Federal de Pernambuco \And
        Achim Zeileis\\WU Wirtschaftsuniversit\"at Wien}
\Plainauthor{Francisco Cribari-Neto, Achim Zeileis}

\title{Beta Regression in \proglang{R}}
\Plaintitle{Beta Regression in R}

\Keywords{beta regression, rates, proportions, \proglang{R}}
\Plainkeywords{beta regression, rates, proportions, R}

\Abstract{
  The class of beta regression models is commonly used by 
  practitioners to model variables that assume values in the standard 
  unit interval $(0, 1)$. It is based on the assumption that the 
  dependent variable is beta-distributed and that its mean is related to a set
  of regressors through a linear predictor with unknown coefficients
  and a link function. The model also includes a precision parameter
  which may be constant or depend as well on a (potentially different)
  set of regressors through a link function. This approach naturally
  incorporates features such as heteroskedasticity or skewness
  which are commonly observed in data taking values in the standard
  unit interval, such as rates or proportions. This paper describes
  the \pkg{betareg} package which provides the class of beta regressions
  in the \proglang{R}~system for statistical computing. The
  underlying theory is briefly outlined, the implementation discussed
  and illustrated in various replication exercises.
}

\Address{
  Francisco Cribari-Neto\\
  Departamento de Estat{\'{\i}}stica, CCEN\\
  Universidade Federal de Pernambuco\\
  Cidade Universit{\'a}ria\\
  Recife/PE 50740-540, Brazil\\
  E-mail: \email{cribari@ufpe.br}\\
  URL: \url{http://www.de.ufpe.br/~cribari/}\\
  
  Achim Zeileis\\
  Department of Statistics and Mathematics\\
  WU Wirtschaftsuniversit\"at Wien\\
  Augasse 2--6\\
  1090 Wien, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://statmath.wu-wien.ac.at/~zeileis/}
}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{Beta Regression in R}
%\VignetteDepends{stats,betareg,car,lmtest,sandwich}
%\VignetteKeywords{beta regression, rates, proportions, R}
%\VignettePackage{betareg}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
library("betareg")
@


\begin{document}


\section{Introduction} \label{sec:intro}

How should one perform a regression analysis in which the dependent variable
(or response variable), $y$, assumes values in the standard unit interval $(0,1)$? The usual
practice used to be to transform the data so that the transformed response, say
$\tilde y$, assumes values in the real line and then use it in a standard linear
regression analysis. A commonly used transformation is the logit $\tilde y = 
\log(y/(1-y))$. This approach, nonetheless, has shortcomings. First, the
regression parameters are interpretable in terms of the mean of $\tilde y$, and
not in terms of the mean of $y$ (given Jensen's inequality). Second, regressions
involving rates and proportions are typically heteroskedastic: they display more
variation around the mean and less variation as we approach the lower and upper
limits of the standard unit interval. Finally, the distributions of rates and
proportions are typically asymmetric, and thus Gaussian-based approximations for
interval estimation and hypothesis testing can be quite inaccurate in small
samples. \cite{betareg:Ferrari+Cribari-Neto:2004} proposed a regression model
for continuous variates that assume values in the standard unit interval, e.g.,
rates, proportions, or income concentration indices. Since the model is
based on the assumption that the response is beta-distributed, they called their
model \emph{the beta regression model}. In their model, the regression
parameters are interpretable in terms of the mean of $y$ (the variable of
interest). Additionally, the model is naturally heteroskedastic and easily
accomodates asymmetries. A variant of the beta regression model that allows for
nonlinearities and variable dispersion was proposed by
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}. In particular, in this more
general model, the parameter that accounts for the precision of the data is not
assumed to be constant across observations but it is allowed to vary, which leads to the
\emph{variable dispersion beta regression model}. 

In this paper, we describe the \pkg{betareg} package which can
be used to perform inference in both fixed and variable dispersion beta
regressions. The package is implemented in the \proglang{R} system
for statistical computing \citep{betareg:R:2009} and available
from the Comprehensive \proglang{R} Archive Network (CRAN) at
\url{http://CRAN.R-project.org/package=betareg}. The initial version of the
package was written by \cite{betareg:Simas+Rocha:2006} up to version~1.2
which was orphaned and archived on CRAN in mid-2009. Achim Zeileis took over maintenance
after rewriting the code, starting from version~2.0-0.

The paper unfolds as follows:
Section~\ref{sec:model} outlines the theory underlying the beta regression model
before Section~\ref{sec:implementation} describes its implementation in \proglang{R}.
Sections~\ref{sec:illustrations} and \ref{sec:replications} provide various
empirical applications: The former focuses on illustrating various aspects
of beta regressions in practice while the latter provides further replications
of previously published empirical research. Section~\ref{sec:dispersiontesting} describes a score test for fixed dispersion and Section~\ref{sec:conclusion} 
contains concluding remarks and directions for future research and 
implementation. 


\section{Beta regression}\label{sec:model} 

The class of beta regregression models, as introduced by
\cite{betareg:Ferrari+Cribari-Neto:2004}, is useful for modeling continuous
variables $y$ that assume  values in the open standard unit interval $(0,1)$.
Note that if the variable takes on values in $(a, b)$ (with $a < b$ known) one
can model $(y - a)/(b - a)$. Furthermore, if $y$ also assumes the extremes $0$ and $1$, 
a useful transformation in practice is $(y \times (n - 1) + 0.5) / n$
where $n$ is the sample size \citep{betareg:Smithson+Verkuilen:2006}.
\readme{Z: I think this is useful in practice but maybe here is a little bit too prominent? F: A colleague of mine is developing an inflated beta regression model for situations where the data contain zeros and/or ones.}

The beta regression model is based on an alternative parameterization of the beta density in
terms of the  variate mean and of a precision parameter. The beta density is
usually expressed as 
\begin{equation*}
%\label{dens}
f(y;p,q) = \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)}y^{p-1}(1-y)^{q-1}, \quad 0<y<1, 
\end{equation*}
where $p,q >0$ and $\Gamma(\cdot)$ is the gamma function. \cite{betareg:Ferrari+Cribari-Neto:2004} proposed a different parameterization by setting $\mu = p/(p+q)$ and $\phi = p+q$:
\begin{equation*}
f(y;\mu,\phi) = \frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}, \quad 0<y<1,
\end{equation*}
with $0<\mu<1$ and $\phi>0$.
We write $y \,\sim\, \mathcal{B}(\mu, \phi)$. Here, $\E(y) = \mu$ and $\VAR(y) = \mu(1-\mu)/(1+\phi)$. The parameter $\phi$ is known as the precision parameter, since for fixed $\mu$, the larger $\phi$ the smaller the variance of $y$; $\phi^{-1}$ is a dispersion parameter. 

Let $y_1,\ldots,y_n$ be a random sample such that $y_i \sim 
{\mathcal{B}}(\mu_i,\phi)$, $i=1,\ldots,n$. The beta regression model is 
defined as 
\begin{equation*}
g(\mu_i)=x_{i}^\top \beta_i=\eta_i,
\end{equation*}
where $\beta=(\beta_1,\ldots,\beta_k)^\top$ is a $k \times 1$ vector of unknown 
regression parameters ($k<n$), $\eta_i$ is a linear 
predictor and $x_i = (x_{i1},\ldots,x_{ik})^\top$ is the vector of $k$ regressors
(or independent variables or covariates). Here, $g(\cdot): (0,1) \mapsto
\mathrm{I\! R}$ is a link function, which is strictly increasing and twice
differentiable. Some useful link functions are: logit $g(\mu) = \log(\mu/(1-\mu))$;
probit $g(\mu) = \Phi^{-1}(\mu)$, where $\Phi(\cdot)$ is the standard normal
distribution function; complementary log-log $g(\mu) = \log\{-\log(1-\mu)\}$;
log-log $g(\mu) = -\log\{-\log(\mu)\}$; and Cauchy $g(\mu) = \tan\{\pi(\mu - 0.5)\}$.
Note that the variance of $y$ is a function of $\mu$ which renders the regression
model based on this parameterization naturally heteroskedastic. In particular, 
\begin{equation} \label{eqn:variance}
   \VAR(y_i) = \frac{\mu_i(1-\mu_i)}{1+\phi}
             = \frac{g^{-1}(x_i^{\top}\beta)[1-g^{-1}(x_i^{\top}\beta)]}{1+\phi}.
\end{equation} 
  
The log-likelihood function is $\ell(\beta,\phi)=\sum_{i=1}^n\ell_i(\mu_i,\phi)$, where 
\begin{eqnarray*} \label{eqn:loglik}
  \ell_i(\mu_i,\phi) & = & \log \Gamma(\phi)-\log\Gamma(\mu_i\phi) - 
                           \log \Gamma((1-\mu_i)\phi) +(\mu_i\phi-1)\log y_i \\
                     &   & + \{(1-\mu_i)\phi-1\}\log(1-y_i).
\end{eqnarray*}
Notice that $\mu_i=g^{-1}(x_i^{\top}\beta)$ is a function of $\beta$, the vector
of regression parameters. Parameter estimation is performed by maximum likelihood (ML).

An extension of the beta regression model above which was employed by
\cite{betareg:Smithson+Verkuilen:2006} and formally introduced (along
with further extensions) by \cite{betareg:Simas+Barreto-Souza+Rocha:2010}
is the variable dispersion beta regression model. In this model the
precision parameter is not constant for all observations but instead
modelled in a similar fashion as the mean parameter. More specifically,
$y_i\,\stackrel{\mathrm{ind.}}{\sim}\, {\mathcal B}(\mu_i, \phi_i)$, $i=1,\ldots,n$, and 
\begin{eqnarray}
  g_1(\mu_i) & = & \eta_{1i} = x_i^\top \beta, \label{eqn:meanmodel} \\
  g_2(\phi_i) & = & \eta_{2i} = z_i^\top \gamma,\label{eqn:precisionmodel}
\end{eqnarray}
where $\beta=(\beta_1, \ldots, \beta_k)^{\top}$,
$\gamma=(\gamma_1,\ldots,\gamma_h)^{\top}$, $k+h<n$, are the sets of
regression coefficients in the two equations, $\eta_{1i}$ and $\eta_{2i}$ are
the linear predictors, and $x_i$ and $z_i$ are regressor vectors. As before,
both coefficient vectors are estimated by ML, simply replacing $\phi$ by $\phi_i$
in Equation~\ref{eqn:loglik}.

\cite{betareg:Simas+Barreto-Souza+Rocha:2010} further extend the model above
by allowing nonlinear predictors in Equations~\ref{eqn:meanmodel}
and \ref{eqn:precisionmodel}. Also, they have obtained analytical bias
corrections for the ML estimators of the parameters, thus generalizing the results of
\cite{betareg:Ospina+Cribari-Neto+Vasconcellos:2006}, who derived bias
corrections for fixed dispersion beta regressions. However, as these extensions
are not (yet) part of the \pkg{betareg} package, we confine ourselve to these
short references and do not provide detailed formulas.

Various types of residuals are available for beta regression models. The raw
response residuals $y_i - \hat \mu_i$ are typically not used due to the heteroskedasticity
inherent in the model (see Equation~\ref{eqn:variance}). Hence, a natural
alternative are Pearson residuals which \cite{betareg:Ferrari+Cribari-Neto:2004}
call \emph{standardized ordinary residual} and define as:
\begin{equation*}
  r_i = \frac{y_i - \hat{\mu}_i}{\sqrt{\widehat{\VAR}(y_i)}},
\end{equation*}
where $\widehat{\VAR}(y_i) = \hat{\mu}_i(1-\hat{\mu}_i)/
(1+\widehat{\phi_i})$, $\hat{\mu}_i = g_1^{-1}(x_i^\top\,\hat{\beta})$,
and $\hat{\phi}_i = g_2^{-1}(z_i^\top\,\hat{\beta})$. Similarly, deviance residuals
can be defined in the standard way via signed contributions to the excess likelihood.
Further residuals were proposed by \cite{betareg:Espinheira+Ferrari+Cribari-Neto:2008a},
in particular one residual with better properties that they named
\emph{standardized weighted residual 2} \readme{Z: Should this be the default in the residuals() method? F: In my opinion, yes.}:
\begin{equation*}
  r^{ww}_i = \frac{y^*_i - {\hat{{\mu}}^*}_i}{\sqrt{\hat{v}_i(1 - h_{ii})}},
\label{eqn:residual2}
\end{equation*}
where $y_i^* = \log\{ y_i / (1-y_i)\}$ and  $\mu_i^* = \psi(\mu_i\phi)- \psi((1-\mu_i)\phi)$,
$\psi(\cdot)$ denoting the digamma function. Standardization is then by
$v_i = \left\{ \psi'(\mu_i\phi) + \psi'((1-\mu_i)\phi)\right\}$
and $h_{ii}$, the $i$th diagonal element of hat matrix
\citep[for details see][]{betareg:Ferrari+Cribari-Neto:2004,betareg:Espinheira+Ferrari+Cribari-Neto:2008a}.
As before, hats denote evaluation at the ML estimates.
%% $H ={\hat{W}}^{1/2}X(X^\top\hat{W}X)^{-1}X^\top {\hat{W}}^{1/2}$.
%% Here, $W$ is $n\times n$ diagonal matrix with typical element given by
%% $w_i = \phi v_i [1 / \{g'(\mu_i)\}^2]$, with  

\readme{Z: I have omitted the description of the RESET-type test here. It is
briefly introduced with reference in the illustrations section. F: OK.}
%% A RESET-type misspecification test for fixed dispersion beta regression, similar
%% to that of \cite{betareg:Ramsey:1969} for linear regressions, was proposed by
%% \cite{betareg:Cribari-Neto+Lima:2007}. The authors considered different variants
%% of the test and concluded that the best performing testing strategy in finite
%% samples is to include $\hat\eta_i^2=(x_i^{\top}\hat\beta)^2$, where
%% $\hat{\beta}$ denotes the ML estimator of $\beta$, as an
%% additional regressor in an augmented (artificial) regression, and then test its
%% exclusion using a score test. Rejection of the null hypothesis suggests that the
%% model is misspecified. Misspecification can follow, e.g., from incorrectly
%% specifying the link function, from neglecting nonlinearities or from omitting
%% important regressors. 


\section[Implementation in R]{Implementation in \proglang{R}} \label{sec:implementation}

To turn the conceptual model from the previous section into computational tools
in \proglang{R}, it helps to emphasize some properties of the model: It is a standard
maximum likelihood (ML) task for which there is no closed-form solution but numerical
optimization is required. Furthermore, the model shares some properties (such as
linear predictor, link function, dispersion parameter) with
generalized linear models \citep[GLMs][]{betareg:McCullagh+Nelder:1989}, but it
is not a special case of this framework (not even for fixed dispersion). There are various
models with implementations in \proglang{R} that have similar features -- here, we
specifically reuse some of the ideas employed for generalized count data regression
by \cite{betareg:Zeileis+Kleiber+Jackman:2008}.

The main model-fitting function in \pkg{betareg} is \fct{betareg} which takes
a fairly standard approach for implementing ML regression models in \proglang{R}:
\code{formula} plus \code{data} is used for data
specification, then the likelihood and corresponding gradient (or estimating
function) is set up, \fct{optim} is called for maximizing the likelihood, and finally
an object of \proglang{S}3 class \class{betareg} is returned for which a large set of methods to standard
generics is available. The workhorse function is \fct{betareg.fit} which provides
the core computations without \code{formula}-related data pre- and post-processing.

The model-fitting function \fct{betareg} and its associated class are designed to
be as similar as possible to the standard \fct{glm} function \citep{betareg:R:2009}
for fitting GLMs. An important difference is that there are potentially two equations
for mean and precision (Equations~\ref{eqn:meanmodel} and \ref{eqn:precisionmodel},
respectively), and consequently two regressor matrices,
two linear predictors, two sets of coefficients, etc. In this respect, the design
of \fct{betareg} is similar to the functions described by
\cite{betareg:Zeileis+Kleiber+Jackman:2008} for fitting zero-inflation and hurdle
models which also have two model components. The arguments of \fct{betareg} are
%
\begin{Sinput}
betareg(formula, data, subset, na.action, weights, offset,
  link = "logit", link.phi = NULL, control = betareg.control(...),
  model = TRUE, y = TRUE, x = FALSE, ...)
\end{Sinput}
%
where the first line contains the standard model-frame specifications
\citep[see][]{betareg:Chambers+Hastie:1992},
the second line have the arguments specific to beta regression models and
the arguments in the last line control some components of the return value.

If a \code{formula} of type \code{y ~ x1 + x2} is supplied, it describes $y_i$ and $x_i$ 
for the mean equation of the beta regression (\ref{eqn:meanmodel}). In this case a constant
$\phi_i$ is assumed, i.e., $z_i = 1$ and $g_2$ is the identity link, corresponding
to the basic beta regression model as introduced in \cite{betareg:Ferrari+Cribari-Neto:2004}.
However, a second set of regressors can be specified by a two-part formula of type
\code{y ~ x1 + x2 | z1 + z2 + z3} as provided in the \pkg{Formula} package
\citep{betareg:Zeileis+Croissant:2009}. This model has the same mean equation as above but
the regressors $z_i$ in the precision equation (\ref{eqn:precisionmodel}) are taken from
the \code{~ z1 + z2 + z3} part.
The default link function in this case is the log link $g_2(\cdot) = \log(\cdot)$.
Consequently, \code{y ~ x1 + x2} and \code{y ~ x1 + x2 | 1} correspond to equivalent
beta likelihoods but use different parametrizations for $\phi_i$: simply $\phi_i = \gamma_1$
in the former case and $\log(\phi_i) = \gamma_1$ in the latter case. The link for the
$\phi_i$ precision equation can be changed by \code{link.phi} in both cases where
\code{"identity"}, \code{"log"}, and \code{"sqrt"} are allowed as admissible values.
The default for the $\mu_i$ mean equation is always the logit link but all link
functions for the \code{binomial} family in \fct{glm} are allowed as well as the log-log
link: \code{"logit"}, \code{"probit"}, \code{"cloglog"}, \code{"cauchit"},
\code{"log"}, and \code{"loglog"}.

\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|p{10cm}|}
\hline
Function & Description \\ \hline
\fct{print} & simple printed display with coefficient estimates\\
\fct{summary} & standard regression output (coefficient estimates, standard errors, partial Wald tests);
                returns an object of class \class{summary.betareg}
                containing the relevant summary statistics (which has a \fct{print} method) \\ 	\hline
\fct{coef} & extract coefficients of model (full or mean/precision components), a single vector of all coefficients by default \\
\fct{vcov} & associated covariance matrix (with matching names) \\
\fct{predict} & predictions (of means $\mu_i$, linear predictors $\eta_{1i}$,
                precision parameter $\phi_i$, or variances $\mu_i (1 - \mu_i) / (1 + \phi_i)$) for new data \\
\fct{fitted} & fitted means for observed data \\
\fct{residuals} & extract residuals \citep[deviance, Pearson, response,
  or different weighted residuals, see][]{betareg:Espinheira+Ferrari+Cribari-Neto:2008a} \\ 
\fct{estfun} & compute empirical estimating functions (or score functions),
  evaluated at observed data and estimated parameters \citep[see][]{betareg:Zeileis:2006a} \\
\fct{bread} & extract ``bread'' matrix for sandwich estimators
  \citep[see][]{betareg:Zeileis:2006a} \\ \hline
\fct{terms} & extract terms of model components \\
\fct{model.matrix} & extract model matrix of model components \\
\fct{model.frame} & extract full original model frame \\
\fct{logLik} & extract fitted log-likelihood \\ \hline
\fct{plot} & diagnostic plots of residuals, predictions, leverages etc. \\
\fct{hatvalues} & hat values (diagonal of hat matrix) \\
\fct{cooks.distance} & (approximation of) Cook's distance \\
\fct{gleverage} & compute generalized leverage \citep{betareg:Wei+Hu+Fung:1998};
  based on the formula derived for fixed $\phi$ \\ \hline
\fct{coeftest} & partial Wald tests of coefficients \\
\fct{waldtest} & Wald tests of nested models \\
\fct{linear.hypothesis} & Wald tests of linear hypotheses \\
\fct{lrtest} & likelihood ratio tests of nested models \\
\fct{AIC} & compute information criteria (AIC, BIC, \dots) \\ \hline
\end{tabular}
\caption{\label{tab:methods} Functions and methods for objects of class \class{betareg}.
  The first four blocks refer to methods, the last block contains generic
  functions whose default methods work because of the information supplied by the methods above.}
\end{center}
\end{table}

ML estimation of all parameters employing analytical gradients is carried out
using \proglang{R}'s \fct{optim} with control options set in \fct{betareg.control}.
Starting values can be user-supplied, otherwise the $\beta$ starting values are estimated by 
a regression of $g_1(y_i)$ on $x_i$ and similarly
the $\gamma$ starting values are obtained from a regression of a transformed
$y_i$ on the $z_i$. The transformed $y_i$ are derived in \cite{betareg:Ferrari+Cribari-Neto:2004}
where only their mean is used as the starting values (corresponding to constant $\phi_i$
with identity link). The covariance matrix estimate is derived analytically as in
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}. However, by setting \code{hessian = TRUE}
the numerical Hessian matrix returned by \fct{optim} can also be obtained.

The returned fitted-model object of class \class{betareg} is a list similar
to \class{glm} objects. Some of its elements---such as \code{coefficients} or
\code{terms}---are lists with a mean and precision component,
respectively.
%
A set of standard extractor functions for fitted model objects is available for
objects of class \class{betareg}, including the usual \fct{summary} method that
provides partial Wald tests for all coefficients. No \fct{anova} method is provided,
but the general \fct{coeftest}, \fct{waldtest} from
\pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}, and \fct{linear.hypothesis}
from \pkg{car} \citep{betareg:Fox:2002} can be used for Wald tests and \fct{lrtest} from \pkg{lmtest}
for likelihood-ratio tests of nested models. See Table~\ref{tab:methods} for a list
of all available methods. Most of these are standard in base \proglang{R}, however, methods
to a few less standard generics are also provided. Specifically, there are tools related
to specification testing and computation of sandwich covariance matrices as discussed
by \cite{betareg:Zeileis:2004,betareg:Zeileis:2006a} as well as a method to a new generic for computing
generalized leverages \citep{betareg:Wei+Hu+Fung:1998}.


\section{Beta regression in practice} \label{sec:illustrations}

To illustrate the usage of \pkg{betareg} in practice we replicate
and slightly extend some of the analyses from the original papers
that suggested the methodology. More specifically, we estimate and
compare various flavours of beta regression models for the gasoline
yield of \cite{betareg:Prater:1956}, see Figure~\ref{fig:GasolineYield},
and for the household food expenditure data taken from
\cite{betareg:Griffiths+Hill+Judge:1993}, see Figure~\ref{fig:FoodExpenditure}. 
Further pure replication exercises are provided in Section~\ref{sec:replications}.

\subsection{The basic model: Estimation, inference, diagnostics}

\subsubsection{Prater's gasoline yield data}

<<GasolineYield-betareg, echo=FALSE>>=
data("GasolineYield", package = "betareg")
gy_logit <- betareg(yield ~ batch + temp, data = GasolineYield)
@

<<GasolineYield-loglog, echo=FALSE>>=
gy_loglog <- betareg(yield ~ batch + temp, data = GasolineYield,
  link = "loglog")
@

The basic beta regression model as suggested by \cite{betareg:Ferrari+Cribari-Neto:2004}
is illustrated in Section~4 of their paper using two empirical examples.
The first example employs the well-known gasoline yield data taken from
\cite{betareg:Prater:1956}. The variable interest is \code{yield},
the proportion of crude oil converted to gasoline after distillation
and fractionation, for which a beta regression model is rather
natural. \cite{betareg:Ferrari+Cribari-Neto:2004} employ two explanatory
variables: \code{temp}, the temperature (in degrees Fahrenheit) at which all
gasoline has vaporized, and \code{batch}, a factor indicating ten unique batches
of conditions in the experiments (depending on further variables). The
data, encompassing \Sexpr{nrow(GasolineYield)}~observations, is visualized
in Figure~\ref{fig:GasolineYield}.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.75\textwidth}
<<GasolineYield-visualization, echo=FALSE, fig=TRUE, width=6, height=5.5>>=
plot(yield ~ temp, data = GasolineYield, type = "n",
  ylab = "Proportion of crude oil converted to gasoline",
  xlab = "Temperature at which all gasoline has vaporized",
  main = "Prater's gasoline yield data")
points(yield ~ temp, data = GasolineYield, cex = 1.75, 
  pch = 19, col = rev(gray.colors(10))[as.numeric(batch)])
points(yield ~ temp, data = GasolineYield, cex = 1.75)
legend("topleft", as.character(1:10), title = "Batch",
  col = rev(gray.colors(10)), pch = 19, bty = "n")
legend("topleft", as.character(1:10), title = "Batch", pch = 1, bty = "n")
lines(150:500, predict(gy_logit, 
  newdata = data.frame(temp = 150:500, batch = "6")),
  col = 4, lwd = 2, lty = 2)
lines(150:500, predict(gy_loglog, 
  newdata = data.frame(temp = 150:500, batch = "6")),
  col = 2, lwd = 2)
legend("bottomright", c("log-log", "logit"),
  col = c(2, 4), lty = 1:2, lwd = 2, bty = "n")
@
\caption{\label{fig:GasolineYield} Gasoline yield from \cite{betareg:Prater:1956}:
  Proportion of crude oil converted to gasoline explained by temperature
  (in degrees Fahrenheit) at which all gasoline has vaporized and given batch
  (indicated by gray level). Fitted curves correspond to beta regressions 
  \code{gy\_loglog} with log-log link (solid, red) and \code{gy\_logit} with
  logit link (dashed, blue). Both curves were evaluated at varying temperature
  with the intercept for batch 6 (i.e., roughly the average intercept).}
\end{center}
\end{figure}

\cite{betareg:Ferrari+Cribari-Neto:2004} start out with a model where
\code{yield} depends on \code{batch} and \code{temp}, employing the standard
logit link. In \pkg{betareg}, this can be fitted via
%
<<GasolineYield-betareg1>>=
<<GasolineYield-betareg>>
summary(gy_logit)
@
%
which replicates their Table~1. The goodness of fit is assessed using
different types of diagnostic displays shown in their Figure~2. This
graphic can be reproduced (in a slightly different order) using the
\fct{plot} method for \class{betareg} objects, see Figure~\ref{fig:GasolineYield-plot}.
\fixme{Z: omit nsim before release, just used at the moment to make recompilation
of the .Rnw more convenient}
%
<<GasolineYield-plot, eval=FALSE>>=
set.seed(123)
plot(gy_logit, which = 1:4)
plot(gy_logit, which = 5, type = "deviance", sub.caption = "", nsim = 5)
plot(gy_logit, which = 1, type = "deviance", sub.caption = "")
@
%
As observation~4 corresponds to a large Cook's distance and large
residual, \cite{betareg:Ferrari+Cribari-Neto:2004} decided to refit
the model excluding this observation. While this does not change the
coefficients in the mean model very much, the precision parameter
$\phi$ increases clearly.
%
<<GasolineYield-update>>=
gy_logit4 <- update(gy_logit, subset = -4)
coef(gy_logit, model = "precision")
coef(gy_logit4, model = "precision")
@

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<GasolineYield-plot1, echo=FALSE, fig=TRUE, width=8.5, height=10>>=
par(mfrow = c(3, 2))
<<GasolineYield-plot>>
@
\caption{\label{fig:GasolineYield-plot} Diagnostic plots for
  beta regression model \code{gy\_logit}.}
\end{center}
\end{figure}


\subsubsection{Household food expenditures}

<<FoodExpenditure-lm, echo=FALSE>>=
data("FoodExpenditure", package = "betareg")
fe_lm <- lm(I(food/income) ~ income + persons, data = FoodExpenditure)
@

<<FoodExpenditure-betareg, echo=FALSE>>=
fe_beta <- betareg(I(food/income) ~ income + persons,
  data = FoodExpenditure)
@

<<FoodExpenditure-betareg2, echo=FALSE>>=
fe_beta2 <- betareg(I(food/income) ~ income + persons | persons,
  data = FoodExpenditure)
@

\cite{betareg:Ferrari+Cribari-Neto:2004} also consider a second
example: household food expenditure data for \Sexpr{nrow(FoodExpenditure)}~households
taken from \citet[][Table~15.4]{betareg:Griffiths+Hill+Judge:1993}.
The dependent variable is \code{food/income}, the proportion of household
\code{income} spent on \code{food}. Two explanatory variables are available:
the previously mentioned household \code{income} and the number of \code{persons}
living in the household. All three variables are visualized in Figure~\ref{fig:FoodExpenditure}.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.75\textwidth}
<<FoodExpenditure-visualization, echo=FALSE, fig=TRUE, width=6, height=5.5>>=
plot(I(food/income) ~ income, data = FoodExpenditure,
  xlab = "Household income", ylab = "Proportion of food expenditures",
  main = "Food expenditures data", type = "n", ylim = c(0.04, 0.57))
points(I(food/income) ~ income, data = FoodExpenditure, cex = 1.75, pch = 19,
  col = rev(gray.colors(7))[persons])
points(I(food/income) ~ income, data = FoodExpenditure, cex = 1.75)
legend("bottomleft", rev(as.character(sort(unique(FoodExpenditure$persons)))),
  title = "Persons", col = gray.colors(7), pch = 19, bty = "n")
legend("bottomleft", rev(as.character(sort(unique(FoodExpenditure$persons)))),
  title = "Persons", pch = 1, bty = "n")
lines(10:100, predict(fe_lm, 
  newdata = data.frame(income = 10:100, persons = mean(FoodExpenditure$persons))),
  col = 1, lwd = 2, lty = 2)
lines(10:100, predict(fe_beta, 
  newdata = data.frame(income = 10:100, persons = mean(FoodExpenditure$persons))),
  col = 4, lwd = 2, lty = 5)
lines(10:100, predict(fe_beta2, 
  newdata = data.frame(income = 10:100, persons = mean(FoodExpenditure$persons))),
  col = 2, lwd = 2)
legend("topright", c("logit, var. disp.", "logit, fix. disp.", "lm"),
  col = c(2, 4, 1), lty = c(1, 5, 2), lwd = 2, bty = "n")
@
\caption{\label{fig:FoodExpenditure} Household food expenditure data from
  \cite{betareg:Griffiths+Hill+Judge:1993}: Proportion of household income
  spent on food explained by household income and number of persons in
  household (indicated by gray level). Fitted curves correspond to beta regressions 
  \code{fe\_beta} with fixed dispersion (long-dashed, blue),
  \code{fe\_beta2} with variable dispersion (solid, red),
  and the linear regression \code{fe\_lin} (dashed, black). All curves were evaluated
  at varying income with the intercept for mean number of persons
  ($ = \Sexpr{round(mean(FoodExpenditure$persons), digits = 2)}$).}
\end{center}
\end{figure}

To start their analysis, \cite{betareg:Ferrari+Cribari-Neto:2004} consider
a simple linear regression model fitted by ordinary least squares (OLS)
%
<<FoodExpenditure-lm1>>=
<<FoodExpenditure-lm>>
@
%
To show that this model exhibits heteroskedasticity, they employ 
the studentized \cite{betareg:Breusch+Pagan:1979} test of \cite{betareg:Koenker:1981}
which is available in \proglang{R} in the \pkg{lmtest} package
\citep{betareg:Zeileis+Hothorn:2002}.
%
<<FoodExpenditure-bptest>>=
library("lmtest")
bptest(fe_lm)
@
%
One alternative would be to consider a logit-transformed response in
a traditional OLS regression but this would make the residuals
asymmetric. However, both issues -- heteroskedasticity and skewness --
can be alleviated when a beta regression model with a logit link for
the mean is used.
%
<<FoodExpenditure-betareg1>>=
<<FoodExpenditure-betareg>>
summary(fe_beta)
@
%
This replicates Table~2 from \cite{betareg:Ferrari+Cribari-Neto:2004}.
The predicted means of the linear and the beta regression model, respectively,
are very similar: the proportion of household income spent on food decreases
with the overall income level but increases in the number of persons in the
household (see also Figure~\ref{fig:FoodExpenditure}).

Below, further extended models will be considered for these data sets and
hence all model comparisons are deferred.


\subsection{Variable dispersion model}

\subsubsection{Prater's gasoline yield data}

Although the beta model already incorporates naturally a certain
pattern in the variances of the response (see Equation~\ref{eqn:variance}), it might
be necessary to incorporate further regressors to account for heteroskedasticity
as in Equation~\ref{eqn:precisionmodel} \citep{betareg:Simas+Barreto-Souza+Rocha:2010}.
For illustration of this approach, the example from Section~3 of the online
supplements to \cite{betareg:Simas+Barreto-Souza+Rocha:2010} is considered.
This investigates Prater's gasoline yield data based on the same
mean equation as above, but now with temperature \code{temp} as an 
additional regressor for the precision parameter $\phi_i$:
%
<<GasolineYield-phireg>>=
gy_logit2 <- betareg(yield ~ batch + temp | temp, data = GasolineYield)
@
%
for which \code{summary(gy_logit2)} yields the MLE column in Table~19
of \cite{betareg:Simas+Barreto-Souza+Rocha:2010}. To save space, only
the parameters pertaining to $\phi_i$ are reported here
%
<<GasolineYield-phireg-coef, echo=FALSE>>=
printCoefmat(summary(gy_logit2)$coefficients$precision)
@
%
which signal a significant improvement by including the \code{temp} regressor.
Instead of using this Wald test, the models can also be compared by means
of a likelihood-ratio test (see their Table~18) that confirms the results:
%
<<GasolineYield-lrtest>>=
lrtest(gy_logit, gy_logit2)
@

\subsubsection{Household food expenditures}

For the household food expenditure data, the Breusch-Pagan test carried out
above illustrated that there is heteroskedasticity that can be captured
by the regressors \code{income} and \code{persons}. Closer investigation
reveals that this is mostly due to the number of persons in the household,
also brought out graphically by some of the outliers with high values in
this variable in Figure~\ref{fig:FoodExpenditure}. Hence, it seems natural
to consider the model employed above with \code{persons} as an additional
regressor in the precision equation.
%
<<FoodExpenditure-betareg2a>>=
<<FoodExpenditure-betareg2>>
@
%
This leads to significant improvements in terms of the likelihood and
the associated BIC.
%
<<FoodExpenditure-comparison>>=
lrtest(fe_beta, fe_beta2)
AIC(fe_beta, fe_beta2, k = log(nrow(FoodExpenditure)))
@
%
Thus, the model \code{fe_beta2} seems to be preferable. As visualized in
Figure~\ref{fig:FoodExpenditure}, it describes a similar relationship between
response and explanatory variables although with a somewhat shrinked
\code{income} slope.


\subsection{Selection of different link functions}

\subsubsection{Prater's gasoline yield data}

As in binomial GLMs, selection of an appropriate link function 
can greatly improve the model fit \citep{betareg:McCullagh+Nelder:1989},
especially if extreme proportions (close to $0$ or $1$) have been
observed in the data. To illustrate this problem in beta regressions,
we replicate parts of the analysis in Section~5 of \cite{betareg:Cribari-Neto+Lima:2007}.
This reconsiders Prater's gasoline yield data but employs a log-log
link instead of the previously used (default) logit link
%
<<GasolineYield-loglog1>>=
<<GasolineYield-loglog>>
@
%
which clearly improves pseudo $R^2$ of the model:
%
<<GasolineYield-Rsquared>>=
summary(gy_logit)$pseudo.r.squared
summary(gy_loglog)$pseudo.r.squared
@
Similarly, the AIC\footnote{Note that \cite{betareg:Cribari-Neto+Lima:2007} did not
account for estimation of $\phi$ in their degrees of freedom. Hence, their reported
AICs differ by 2.} (and BIC) of the fitted model is not only
superior to the logit model with fixed dispersion \code{gy_logit}
but also to the logit model with variable dispersion \code{gy_logit2} considered
in the previous section.
%
<<GasolineYield-AIC>>=
AIC(gy_logit, gy_logit2, gy_loglog)
@
Moreover, if \code{temp} were included as a regressor in the precision
equation of \code{gy_loglog}, it would no longer yield significant improvements.
Thus, improvement of the model fit in the mean equation by adoption of the log-log
link have waived the need for an extended precision equation.

To underline the appropriateness of the log-log specification, 
\cite{betareg:Cribari-Neto+Lima:2007} consider a sequence of diagnostic
tests inspired by the RESET \citep[regression specification error test;][]{betareg:Ramsey:1969}
in linear regression models.
To check for misspecifications, they consider powers of fitted means or linear predictors
to be included as auxiliary regressors in the mean equation. In well-specified
models, these should not yield significant improvements. For the gasoline yield
model, this can only be obtained for the log-log link while all other link
functions result in significant results indicating misspecification. Below,
this is exemplified for a likelihood-ratio test of squared linear predictors.
Analogous results can be obtained for \code{type = "response"} or higher powers.
%
<<GasolineYield-reset>>=
lrtest(gy_logit, . ~ . + I(predict(gy_logit, type = "link")^2))
lrtest(gy_loglog, . ~ . + I(predict(gy_loglog, type = "link")^2))
@
%
The improvement of the model fit can also be brought out graphically in a
display of predicted vs.\ observed values (see Figure~\ref{fig:GasolineYield-diagnostics}).
%
<<GasolineYield-diagnostics, eval=FALSE>>=
plot(gy_logit, which = 6)
plot(gy_loglog, which = 6)
@
%
This shows that especially for the extreme observations, the log-log link
leads to better predictions.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<GasolineYield-diagnostics1, echo=FALSE, fig=TRUE, width=8.5, height=4>>=
par(mfrow = c(1, 2))
<<GasolineYield-diagnostics>>
@
\caption{\label{fig:GasolineYield-diagnostics} Diagnostic plots:
  Predicted vs.\ observed values for beta regression model
  \code{gy\_logit} with logit link (left) and \code{gy\_loglog}
  with log-log link (right).}
\end{center}
\end{figure}

In principle, the link function $g_2$ in the precision equation could
also influence the model fit. However, as the best-fitting model 
\code{gy_loglog} has a constant $\phi$, all links $g_2$ lead to 
equivalent estimates of $\phi$ and thus to equivalent fitted log-likelihoods.
However, the link function can have consequences in terms of the
inference about $\phi$ and in terms of convergence of the optimization.
Typically, a log-link leads to somewhat improved quadratic approximations
of the likelihood and less iterations in the optimization. For example,
refitting \code{gy_loglog} with $g_2(\cdot) = \log(\cdot)$ converges
more quickly:
%
<<GasolineYield-loglog>>=
gy_loglog2 <- update(gy_loglog, link.phi = "log")
summary(gy_loglog2)$iterations
@
%
with a lower number of iterations than for \code{gy_loglog}
which had \Sexpr{summary(gy_loglog)$iterations} iterations.

%% equivalently with
%% \code{betareg(yield ~ batch + temp | 1, data = GasolineYield, link = "loglog")}


\subsubsection{Household food expenditures}

One could conduct a similar analysis as above for the household
food expenditure data. However, as the response takes less extreme observations
than for the gasoline yield data, the choice of link function is less important.
In fact, refitting the model with various link functions shows no large
differences in the resulting log-likelihoods.
%
<<FoodExpenditure-links>>=
sapply(c("logit", "probit", "cloglog", "cauchit", "loglog"), function(x)
  logLik(betareg(I(food/income) ~ income + persons | persons,
  link = x, data = FoodExpenditure)))
@
%
Only the Cauchy link performs slightly better than the logit link and might
hence deserve further investigation.



\section{Further replication exercises} \label{sec:replications}

In this section, further empirical illustrations of beta regressions
are provided. While the emphasis in the previous section was to present
how the various features of \pkg{betareg} can be used in pracice, we
focus more narrowly on replication of previously published research
articles below.

\subsection{Dyslexia and IQ predicting reading accuracy}

<<ReadingSkills-eda, echo=FALSE, results=hide>>=
data("ReadingSkills", package = "betareg")
rs_accuracy <- format(round(with(ReadingSkills, tapply(accuracy, dyslexia, mean)), digits = 3))
@

<<ReadingSkills-ols, echo=FALSE>>=
data("ReadingSkills", package = "betareg")
rs_ols <- lm(qlogis(accuracy) ~ dyslexia * iq, data = ReadingSkills)
@

<<ReadingSkills-beta, echo=FALSE>>=
rs_beta <- betareg(accuracy ~ dyslexia * iq | dyslexia + iq,
  data = ReadingSkills, hessian = TRUE)
@

We consider an application that models reading accuracy data for nondyslexic
and dyslexic Australian children \citep{betareg:Smithson+Verkuilen:2006}.  
The variable of interest is \code{accuracy} providing the scores on a test of 
reading accuracy taken by 44 children, which is predicted by the two regressors
\code{dyslexia} (a factor with sum contrasts separating a dyslexic and a control group)
and nonverbal intelligent quotient (\code{iq}, converted to $z$~scores), see
Figure~\ref{fig:ReadingSkills} for a visualization.
The sample includes 19 dyslexics and 25 controls who were recruited from primary 
schools in the Australian Capital Territory. The children's ages ranged from 
eight years five months to twelve years three months; mean reading accuracy was
\Sexpr{rs_accuracy[2]} for dyslexic readers and \Sexpr{rs_accuracy[1]} for controls. 

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.75\textwidth}
<<ReadingSkills-visualization, echo=FALSE, fig=TRUE, width=6, height=5.5>>=
cl1 <- hcl(c(260, 0), 90, 40)
cl2 <- hcl(c(260, 0), 10, 95)
plot(accuracy ~ iq, data = ReadingSkills, col = cl2[as.numeric(dyslexia)],
  main = "Reading skills data",
  pch = 19, xlab = "IQ score", ylab = "Reading accuracy", cex = 1.5)
points(accuracy ~ iq, data = ReadingSkills, col = cl1[as.numeric(dyslexia)], cex = 1.5)
nd <- data.frame(dyslexia = "no", iq = -30:30/10)
lines(nd$iq, predict(rs_beta, nd), col = cl1[1], lwd = 2)
lines(nd$iq, plogis(predict(rs_ols, nd)), col = cl1[1], lty = 2, lwd = 2)
nd <- data.frame(dyslexia = "yes", iq = -30:30/10)
lines(nd$iq, predict(rs_beta, nd), col = cl1[2], lwd = 2)
lines(nd$iq, plogis(predict(rs_ols, nd)), col = cl1[2], lty = 2, lwd = 2)
legend("topleft", c("control", "dyslexic", "betareg", "lm"),
  lty = c(NA, NA, 1:2), pch = c(19, 19, NA, NA), lwd = 2,
  col = c(cl2, 1, 1), bty = "n")
legend("topleft", c("control", "dyslexic", "betareg", "lm"),
  lty = c(NA, NA, 1:2), pch = c(1, 1, NA, NA),
  col = c(cl1, NA, NA), bty = "n")
@
\caption{\label{fig:ReadingSkills} Reading skills data from \cite{betareg:Smithson+Verkuilen:2006}:
Linearly transformed reading accuracy by IQ score and dyslexia status (control, blue vs.\
dyslexic, red). Fitted curves correspond to beta regression \code{rs\_beta} (solid) and OLS
regression with logit-transformed dependent variable \code{rs\_ols} (dashed).}
\end{center}
\end{figure}

\cite{betareg:Smithson+Verkuilen:2006} want to investigate whether \code{dyslexia}
contributes to the explanation of \code{accuracy} even when corrected for \code{iq} score
(which is on average lower for dyslexics). Hence, they consider separate regressions
for both groups fitted by the interaction of both regressors. To show that OLS
regression is no suitable tool in this situation, they first fit a linear regression
of the logit-transformed response:
%
<<ReadingSkills-ols1>>=
<<ReadingSkills-ols>>
coeftest(rs_ols)
@
%
The interaction effect does not appear to be significant, however this is a result
of the poor fit of the linear regression as will be shown below. Figure~\ref{fig:ReadingSkills}
clearly shows that the data are asymmetric and heteroskedastic (especially in the control
group). Hence, \cite{betareg:Smithson+Verkuilen:2006} fit a beta regression model,
again with separate means for both groups, but also allow the dispersion to depend on the
main effects of both variables.
%
<<ReadingSkills-beta1>>=
<<ReadingSkills-beta>>
coeftest(rs_beta)
@
%
This shows that precision increases with \code{iq} and is lower for controls while
in the mean equation there is a significant interaction between \code{iq} and \code{dyslexia}.
As Figure~\ref{fig:ReadingSkills} illustrates, the beta regression fit does not differ
much from the OLS fit for the dyslexics group (with responses close to $0.5$) but fits
much better in the control group (with responses close to $1$).

\readme{The circles used for control and dyslexic observations in Figure 5 (blue and red) are not easily distinguishable when printed in a black and whire laser printer. Is it possible to use filled and unfilled circles? }

The estimates above replicate those in Table~5 of \cite{betareg:Smithson+Verkuilen:2006},
except for the signs of the coefficients of the dispersion submodel which they defined
in the opposite way. Note that their results have been obtained with numeric rather than
analytic standard errors hence \code{hessian = TRUE} is set above for replication. The results
are also confirmed by \cite{betareg:Espinheira+Ferrari+Cribari-Neto:2008b}, who have also
concluded that the dispersion is variable.

\cite{betareg:Smithson+Verkuilen:2006} also consider two other applications of beta
regressions which are also replicated in the \pkg{betareg} package: see \code{?MockJurors}
and \code{?StressAnxiety} for the data description and
\code{demo("SmithsonVerkuilen2006", package = "betareg")} for a full replication
script with comments.


\subsection{Structural change testing in beta regressions}

As already illustrated in Section~\ref{sec:illustrations}, \class{betareg} objects can be plugged into
various inference functions from other packages because they provide suitable
methods to standard generic functions (see Table~\ref{tab:methods}). Hence
\fct{lrtest} could be used for performing likelihood-ratio testing
inference and similarly
\fct{coeftest}, \fct{waldtest} from \pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}
and \fct{linear.hypothesis} from \pkg{car} \citep{betareg:Fox:2002} can be
employed for carrying out different flavors of Wald tests.

In this section, we illustrate yet another generic inference approach implemented
in the \pkg{strucchange} package for structural change testing. While originally
written for linear regression models \citep{betareg:Zeileis+Leisch+Hornik:2002},
\pkg{strucchange} was extended by \cite{betareg:Zeileis:2006} to compute generalized
fluctuation tests for structural change in models that are based on suitable
estimating functions. If these estimating functions can be extracted by an
\fct{estfun} method, models can simply be plugged into the \fct{gefp} function
for computing generalized empirical fluctuation processes. To illustrate
this, we replicate the example from Section~5.3 in \cite{betareg:Zeileis:2006}.

Two artificial data sets are considered: a series \code{y1} with a change in
the mean $\mu$, and a series \code{y2} with a change in the precision $\phi$.
Both simulated series start with the parameters $\mu = 0.3$ and $\phi = 4$
and for the first series $\mu$ changes to $0.5$ at after 75\% of the observations
while $\phi$ remains constant whereas for the second series $\phi$ changes to $8$
after 50\% of the observations and $\mu$ remains constant.
%
<<strucchange-data>>=
set.seed(123)
y1 <- c(rbeta(150, 0.3 * 4, 0.7 * 4), rbeta(50, 0.5 * 4, 0.5 * 4))
y2 <- c(rbeta(100, 0.3 * 4, 0.7 * 4), rbeta(100, 0.3 * 8, 0.7 * 8))
@
%
To capture instabilities in the parameters over ``time'' (i.e., the ordering of
the observations), the generalized empirical fluctuation processes can be derived
via
%
<<strucchange-gefp>>=
library("strucchange")
y1_gefp <- gefp(y1 ~ 1, fit = betareg)
y2_gefp <- gefp(y2 ~ 1, fit = betareg)
@
%
and visualized by
%
<<strucchange-plot1, echo=FALSE, eval=FALSE>>=
plot(y1_gefp, aggregate = FALSE)
@
<<strucchange-plot2, echo=FALSE, eval=FALSE>>=
plot(y2_gefp, aggregate = FALSE)
@
<<strucchange-plot, echo=TRUE, eval=FALSE>>=
<<strucchange-plot1>>
<<strucchange-plot2>>
@
%
The resulting Figure~\ref{fig:strucchange} \citep[replicating Figure~4 from][]{betareg:Zeileis:2006}
shows two 2-dimensional fluctuation processes: one for \code{y1} (left) and one for \code{y2} (right).
Both fluctuation processes behave as expected: There is no excessive fluctuation of the 
process pertaining to the parameter that remained constant while there is a significant instability
in the parameter that changed signalled by a boundary crossing and a peak at about the time
of the change in the corresponding parameter.


\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=0.49\textwidth}
<<strucchange-plot1a, echo=FALSE, fig=TRUE, echo=FALSE, width=4.5, height=5>>=
<<strucchange-plot1>>
@
<<strucchange-plot2a, echo=FALSE, fig=TRUE, echo=FALSE, width=4.5, height=5>>=
<<strucchange-plot2>>
@
\caption{\label{fig:strucchange} Structural change tests
  for artificial data \code{y1} with change in $\mu$ (left)
  and \code{y2} with change in $\phi$ (right).}
\end{center}
\end{figure}

\section{Testing for fixed dispersion}\label{sec:dispersiontesting}

We note that it is possible to test the null hypothesis that the precision (or dispersion) is constant for all observations, i.e., $\mathcal{H}_0: \phi_1 = \cdots = \phi_n = \phi$ ($0 < \phi < \infty$), using Rao's score test. At the outset, note that we can write the null hypothesis as $\mathcal{H}_0: \gamma_{q-1}=0$, where $\gamma_{q-1}$ is the $(q-1)$-vector containing all but the first parameter in the precision submodel (Equation~\ref{eqn:precisionmodel}) 
 with $z_{i1} = 1$ for all $i$. 
Let $U_{(q-1)\gamma}$ be the vector of log-likelihood derivatives with respect to the $q-1$ parameters of interst and let $K_{*(q-1)\times(q-1)}^{\gamma\gamma}$
be the $(q-1)\times (q-1)$ matrix that contains the last $q-1$ rows and the last $q-1$ columns of Fisher's information inverse. The score test statistic is \citep{betareg:Espinheira:2007}  
\begin{equation*}\omega = \tilde{U}^{T}_{(q-1)\gamma} \left(\tilde{K}_{*(q-1)\times(q-1)}^{\gamma\gamma}\right)^{-1}
\tilde{U}_{(q-1)\gamma}, 
\end{equation*}
where tildes indicate evaluation at the restricted maximum likelihood parameter estimates. Under the null hypothesis, 
$\omega$ converges in distribution to $\chi^2_{q-1}$. So, the null hypothesis is rejected at the nominal level $0 < \alpha < 1$ if $\omega > \chi^2_{1-\alpha, q-1}$ (the appropriate quantile of the limiting null distribution). 
\readme{F: I think it would be useful to implement this test in betareg. If you agree, we could keep this section and include here an application of the test to the reading accuracy data.}


\section{Summary}\label{sec:conclusion}

This paper addressed the \proglang{R} implementation of the class of beta regression models available in the
\pkg{betareg} package. We have presented the fixed and variable dispersion beta regression models, described
how one can model rates and proportions using \pkg{betareg} and presented several empirical examples
reproducing previously published results. Future
research and implementation shall focus on the situation where the data contain zeros and/or ones
\citep[see, e.g.,][]{betareg:Kieschnick+McCullough:2003}. An additional line of research and
implementation is that of dynamic beta regression models, such as the class of 
$\beta$ARMA models
proposed by \cite{betareg:Rocha+Cribari-Neto:2010}.  




\section*{Acknowledgments}

FCN gratefully acknowledges financial support from CNPq/Brazil. 
Both authors are grateful to A.B.~Simas and A.V.~Rocha for their
work on the previous versions of the \pkg{betareg} package (up to
version~1.2).


\bibliography{betareg}

\end{document}
