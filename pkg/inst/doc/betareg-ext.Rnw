\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\author{Bettina Gr\"un\\Johannes Kepler\\Universit\"at Linz
   \And Ioannis Kosmidis\\University College London
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Bettina Gr\"un, Ioannis Kosmidis, Achim Zeileis}

\title{Extended Beta Regression in \proglang{R}: Shaken, Stirred, Mixed, and Partitioned}
\Plaintitle{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned}

\Keywords{beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, \proglang{R}}
\Plainkeywords{beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, R}

\Abstract{
shaken and stirred (bias correction and reduction),
mixed (finite mixture model),
and partitioned (tree model)
}

\Address{
  Bettina Gr\"un\\
  Institut f\"ur Angewandte Statistik\\
  Johannes Kepler Universit{\"at} Linz\\
  Altenbergerstra{\ss }e 69\\
  4040 Linz, Austria\\
  E-mail: \email{Bettina.Gruen@jku.at}\\
  URL: \url{http://ifas.jku.at/gruen/}\\

  Ioannis Kosmidis\\
  Department of Statistical Science\\
  University College London\\
  Gower Street\\
  London WC1E 6BT, United Kingdom\\
  E-mail: \email{ioannis@stats.ucl.ac.uk}\\
  URL: \url{http://www.ucl.ac.uk/~ucakiko/}\\

  Achim Zeileis\\
  Department of Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}

}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned}
%\VignetteDepends{stats,betareg,car,lmtest,sandwich,strucchange}
%\VignetteKeywords{beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, R}
%\VignettePackage{betareg}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE)
library("betareg")
combine <- function(x, sep, width) {
  cs <- cumsum(nchar(x))
  remaining <- if (any(cs[-1] > width)) combine(x[c(FALSE, cs[-1] > width)], sep, width)
  c(paste(x[c(TRUE, cs[-1] <= width)], collapse= sep), remaining)
}
prettyPrint <- function(x, sep = " ", linebreak = "\n\t", width = getOption("width")) {
  x <- strsplit(x, sep)[[1]]
  paste(combine(x, sep, width), collapse = paste(sep, linebreak, collapse = ""))
}
cache <- TRUE
enumerate <- function(x) paste(paste(x[-length(x)], collapse = ", "), x[length(x)], sep = " and ")
betamix_methods <- 
  enumerate(paste("\\\\fct{", gsub("\\.betamix", "", as.character(methods(class = "betamix"))), "}", sep = ""))
@
\begin{document}

\section{Introduction}\label{sec:intro}

Beta regression is a model for continuous response variables $y$ which
assume values in the open standard unit interval $(0, 1)$. Such
response variables may stem from rates, proportions, concentrations,
etc. A double index regression allowing to model the mean as well as
the precision through covariates was introduced by
\cite{betareg:Ferrari+Cribari-Neto:2004} and extensions were proposed
by \cite{betareg:Smithson+Verkuilen:2006} and
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}.
\cite{betareg:Ferrari+Cribari-Neto:2004} employed an alternative
parametrization of the beta distribution such that the mean and
variance of a beta-distributed variable are easily characterized by
the parameterization.  In this parameterization the beta distribution
has the density
%
\begin{equation} \label{eq:density}
f(y;\mu,\phi) = \frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}, \quad 0<y<1,
\end{equation}
%
where $\Gamma(\cdot)$ is the gamma function.  The mean of a
beta-distributed variable is then given by $\E(y) = \mu$ and the
variance by $\VAR(y) = \mu(1-\mu)/(1+\phi)$.

The double index beta regression model is specified in the following
way. Assume we have observations $i = 1, \dots, n$ of dependent
variable $y_i$.  The parameters $\mu_i$ and $\phi_i$ are linked to
linear predictors from the sets of regressors $x_i$ and
$z_i$. Different link functions can be used for the two
parameters. Suitable link functions include the logit or probit for
$g_1(\cdot)$ which links to $\mu_i$ and log or identity for
$g_2(\cdot)$ which links to $\phi_i$:
\begin{eqnarray}
  g_1(\mu_i) & = & x_i^\top \beta, \label{eq:link1} \\
  g_2(\phi_i) & = & z_i^\top \gamma. \label{eq:link2}
\end{eqnarray}

The coefficients $\beta$ and $\gamma$ can be estimated by maximum
likelihood (ML). The usual central limit theorem holds with associated
asymptotic tests, e.g., likelihood ratio, Wald, score/Lagrange
multiplier (LM).

The \proglang{R} package \pkg{betareg} which implements ML estimation
was introduced by \cite{betareg:Cribari-Neto+Zeileis:2010}. The main
model fitting function is \fct{betareg}. The interface as well as the
fitted models are designed to be similar to those for \fct{glm}. The
model specification is via a \code{formula} plus \code{data}. Because
two types of covariates need to be distinguished a two part formula is
allowed, e.g., \code{y ~ x1 + x2 + x3 | z1 + z2}. The covariates
\code{x1}, \code{x2} and \code{x3} are the covariates for the mean and
\code{z1} and \code{z1} are the covariates for the precision
parameter. The formula is processed using package \pkg{Formula}
\citep{betareg:Zeileis+Croissant:2010}. Function \fct{betareg}
internally uses function \fct{optim} as a general purpose optimizer to
maximize the log-likelihood. The fitted model has methods for several
extractor functions, e.g., \fct{coef}, \fct{vcov}, \fct{residuals},
\fct{logLik}. Base methods for the returned fitted model are
\fct{summary}, \fct{AIC}, \fct{confint}. Further methods are available
for functions from \pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}
and \pkg{car} \citep{betareg:Fox+Weisberg:2011}, e.g., \fct{lrtest},
\fct{waldtest}, \fct{coeftest} and \fct{linearHypothesis}. Multiple
testing is possible via package \pkg{multcomp}
\citep{betareg:Hothorn+Bretz+Westfall:2008} and structural change
tests can be performed using package \pkg{strucchange}
\citep{betareg:Zeileis+Leisch+Hornik:2002}.

The specified beta regression model intends to capture the
relationship between certain predictor variables and the dependent
$(0,1)$ variable. However, these relationships vary for groups in the
population if heterogeneity is present. If variables can be identified
which are directly related to these groups, these variables can be
included in the regression. Disadvantages of this approach are that
(1) the resulting regression model is very complex and hard to
understand and interpret and (2) unnecessary complexity is introduced
if the differences are only present in a subset of the combined groups
induced by several categorical variables. Model-based recursive
partitioning is an alternative approach which avoids these
drawbacks. If these groups cannot be directly related to observed
variables, the heterogeneity can be accounted for using finite mixture
models \citep[for an introduction to finite mixture models see for
example][]{betareg:McLachlan+Peel:2000,
  betareg:Fruehwirth-Schnatter:2006}.  

In this paper, we introduce extensions of the \pkg{betareg} package
where model heterogeneity is taken into account in the case where
covariates to characterize the groups are available as well as when
the heterogeneity is due to latent variables. We present model-based
recursive partitioning of beta regressions and finite mixtures of beta
regression models by building on funtionality provided by packages
\pkg{party} \citep{betareg:Hothorn+Hornik+Strobl:2011} and
\pkg{flexmix} \citep{betareg:Leisch+Gruen:2011}. Both extensions are
derived by re-using the fitting function in more complex models which
provide a general framework for extending regression models.

\section{Bias correction and reduction in beta regressions}\label{sec:bias}

Reduction of bias using the generic algorithm of \cite{betareg:Kosmidis+Firth:2010}
building on earlier work \citep{betareg:Firth:1993,betareg:Kosmidis+Firth:2009}.
While \cite{betareg:Kosmidis+Firth:2010} consider only regressions
with a fixed precision parameter $\phi$, we extend their results to 
models with a regression part for $\phi$.

\fixme{theory outline}

Support for both, bias correction and bias reduction, has been added
in the principal model fitting function \fct{betareg} starting from
\pkg{betareg}~2.4-0. The interface of \fct{betareg} is essentially
the same as described in \cite{betareg:Cribari-Neto+Zeileis:2010}, just
a \code{type} argument has been added specifying the type of estimator.
%
\begin{Code}
  betareg(formula, data, subset, na.action, weights, offset,
    link = "logit", link.phi = NULL, type = c("ML", "BC", "BR"),
    control = betareg.control(...), model = TRUE, y = TRUE, x = FALSE, ...)
\end{Code}
%
The arguments in the first line (\code{formula}, \code{data}, \dots) pertain
to the data and model specification using a formula that potentially may
have two parts pertaining to mean and precision equation, respectively.
The arguments \code{link} and \code{link.phi} specify the link functions
$g_1(\cdot)$ and $g_2(\cdot)$, respectively. The argument \code{type}
controls whether \fixme{refer to equations from theory}. Finally,
\code{control} is a list of control arguments and \code{model}, \code{y},
and \code{x} control whether the respective data components are included
in the fitted model object. See \cite{betareg:Cribari-Neto+Zeileis:2010}
for more details on all arguments except \code{type}.

While the interface of \fct{betareg} is almost the same as in previous
versions, the internal code has been substantially enhanced. Specifically,
the optimization via \fct{optim} is now (optionally) enhanced by an
additional Fisher scoring iteration. As in previous versions, the initial
optimization of the likelihood is carried out via \fct{optim}, by default
with \code{method = "BFGS"}, using analytical gradients. In recent versions,
this is followed by a Fisher scoring iteration with both analytical gradients
and expected information. This iteration is either used to further improve
the numerical maximization of the likelihood (for \code{type = "ML"} or \code{"BC"})
or to carry out the bias reduction (for \code{type = "BR"}) based on bias-adjusted
analytical gradients. To control the details of the Fisher scoring, \fct{betareg.control}
takes two additional arguments \code{fsmaxit = 200} and \code{fstol = 1e-8}
controlling the maximal number of iterations and convergance tolerance, respectively.
If the number of iterations is set to zero (\code{fsmaxit = 0}), no Fisher
scoring is carried out (allowed only for \code{type = "ML"} and \code{"BC"})
and thus exactly replicating results from previous versions of \pkg{betareg}.



\section{Beta regression trees}\label{sec:trees}

Model-based recursive partitioning
\citep[MOB,][]{betareg:Zeileis+Hothorn+Hornik:2008} builds on the more
widely known method of classification and regression trees
\citep[CART, ][]{betareg:Breiman+Friedman+Olshen:1984}. As for CART,
the idea is to split the sample recursively with respect to available
variables (called \dquote{partitioning} variables in the following) in
order to capture differences in the response variable. While CART
tries to capture differences in the distribution of the response
variable (in particular with respect to location) directly, the aim of
model-based recursive partitioning is more broadly to capture
differences in parameters describing the distribution of the response.
In particular, this allows to incorporate regressor variables in a
parametric model for the response variable.

Here, we adapt the general MOB framework to the model-based partitioning
of beta regressions, called \dquote{beta regression trees} for short.
The aim is to capture differences in the distribution that are not yet
adequately described by the regressor variables through a forward search.
Basically, the approach proceeds by (1)~fitting a beta regression model,
(2)~assessing whether its parameters are stable across all partitioning
variables, (3)~splitting the sample along the partitioning variable associated
with the highest parameter instability, (4)~repeating these steps until some
stopping criterion is met. Thus, interactions and nonlinearities can
be incorporated by locally maximizing the likelihood of a partitioned
model. More precisely, the steps of the MOB algorithm adapted to beta
regression are the following, where $c_{ij}$ is the $j$-th partitioning
variable ($j = 1, \dots, p$) for observation~$i$.
%
\begin{enumerate}
  \item Fit a beta regression model with parameters $\beta$ and $\gamma$
        by maximizing the log-likelihood for all observations $y_i$ in the
	current sample.
  \item Assess whether the parameters $\beta$ and $\gamma$ are stable across
        each partitioning variable $c_{ij}$.
  \item If there is significant parameter instability with respect to
        at least one of the partitioning variables $c_{ij}$, split the
        sample along the variable $j*$ with the strongest association:
        Choose the breakpoint with highest improvement in the fitted
	log-likelihood.
  \item Repeat steps 1--3 recursively in the resulting subsamples
        until there is no significant instability any more or the
	sample size is too small.
\end{enumerate}

The MOB framework of \cite{betareg:Zeileis+Hothorn+Hornik:2008} is generic
in that it requires only the specification of a model with additive
objective function for which a central limit theorem holds. This is the
case for beta regression under the usual ML regularity conditions and the
main building blocks that the MOB algorithm requires are the contributions to the
additive objective function (in steps~1 and~3) and its associated score function
(in step~2). Here, the objective is the log-likelihood based on
Equation~\ref{eq:density}, its contributions $\ell(y_i; x_i, z_i, \beta, \gamma)$
are provided in Equation~\ref{eq:loglik} below. The corresponding score (or
gradient) contributions $s(y_i; x_i, z_i, \beta, \gamma)$ given in Equation~\ref{eq:score}
are employed for testing whether there are significant departures from zero across
the partitioning variables. More specifically, MOB uses generalized M-fluctuation
tests for parameter instability \citep{betareg:Zeileis:2006,betareg:Zeileis+Hornik:2007}:
fluctuations in numeric variables are assessed with a $\sup$LM type test
\citep{betareg:Andrews:1993} and categorical variables with a $\chi^2$ type test
\citep{betareg:Hjort+Koning:2002}. See \cite{betareg:Zeileis+Hothorn+Hornik:2008}
for further details and references.\footnote{An example of M-fluctuation tests
for parameter instability (also known as structural change) in beta regressions is
also discussed in \cite{betareg:Zeileis:2006} and replicated in
\cite{betareg:Cribari-Neto+Zeileis:2010}. However, this uses a double-maximum type
test statistic, not a $\sup$LM or $\chi^2$ statistic.}
For beta regression models, the required building blocks mentioned above are given by:
%
\begin{eqnarray}
  \ell(y; x, z, \beta, \gamma)
    & = & \log f(y; g_1^{-1}(x^\top \beta), g_2^{-1}(z^\top \gamma))
    \label{eq:loglik} \\
  s(y; x, z, \beta, \gamma)
    & = & \left\{ \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \beta},
                  \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \gamma} \right\}^\top,
    \label{eq:score} \\
  \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \beta}
    & = & \phi (y^* - \mu^*) \cdot (g_1^{-1})^\prime (x^\top \beta) \cdot x^\top,
    \nonumber \\
  \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \gamma}
    & = & \left\{ \mu (y^* - \mu^*) + \log(1 - y) - \psi((1 - \mu) \phi) + \psi(\phi) \right\}
		 \cdot (g_2^{-1})^\prime (z^\top \gamma) \cdot z^\top
    \nonumber,
\end{eqnarray}
\fixme{Should each of the elements of the score vector also be
  transposed inside the curly brackets?}
%
where $(g_j^{-1})^\prime (\cdot)$ ($j = 1, 2$) denotes the derivative of the inverse link function
with respect to the linear predictor, $y^* = \log\{ y / (1-y)\}$ and  $\mu^* = \psi(\mu \phi)- \psi((1- \mu)\phi)$,
with $\psi(\cdot)$ denoting the digamma function.

Beta regression trees are implemented in the \pkg{betareg} package in function
\fct{betatree} taking the following arguments:
%
\begin{Code}
  betatree(formula, partition, data, subset, na.action, 
    link = "logit", link.phi = "log", control = betareg.control(), ...)
\end{Code}
%
Essentially, almost all arguments work as for the basic \fct{betareg} function.
The main difference is that a \code{partition} formula (without left-hand side),
such as \code{~ c1 + c2 + c3} has to be provided to specify the partitioning variables $c_i$.
As an alternative, \code{partition} may be omitted when \code{formula} has three parts
on the right-hand side, such as \code{y ~ x1 + z2 | z1 | c1 + c2 + c3}, specifying
mean regressors $x_i$, dispersion regressors $z_i$, and partitioning variables $c_i$,
respectively.

The \fct{betatree} function takes all arguments and carries out all data preprocessing
and then calls the function \fct{mob} from the \pkg{party} package
\citep{betareg:Hothorn+Hornik+Zeileis:2006,betareg:Hothorn+Hornik+Strobl:2011}. The
latter can perform all steps of the MOB algorithm in an object-oriented manner, provided
that a suitable model fitting function (optimizing the log-likelihood) is specified
and that extractor functions are available for the optimized log-likelihood~(\ref{eq:loglik})
and the score function~(\ref{eq:score}) at the estimated parameters. For model fitting
\fct{betareg.fit} is employed (through a suitable convenience interface call \fct{betaReg})
and for extractions the \fct{logLik} and \fct{estfun} methods \citep[see also][]{betareg:Zeileis:2006a}
are leveraged. To control details of the MOB algorithm -- such as the significance level
and the minimal subsample size in step~4 -- the \code{...} argument is passed to \fct{mob}. (Note
that this is somewhat different from \fct{betareg} where \code{...} is passed to \fct{betareg.control}.)


\section{Finite mixtures of beta regressions}\label{sec:finite-mixtures-beta}

Finite mixtures are suitable models if the data is assumed to be from
different groups, but the group memberships are not observed. If
mixture models are fitted one aims at determining the parameters of
each group as well as the group sizes. Furthermore, the model can be
used to estimate from which group each observation is. In the case of
finite mixtures of beta regression models the latent groups can be
assumed to differ in their mean and/or in their
precision. Furthermore, the group sizes can depend on further
covariates.

The mixture model with $K$ components which corresponds to $K$
groups is given by
\begin{align*}
  h(y ; x, z, c, \theta) & = \sum_{k = 1}^K \pi(k ; c, \alpha) f(y ;
  g_1^{-1}(x^{\top}\beta_k), g_2^{-1}(z^{\top}\gamma_k)),
\end{align*}
where $h(\cdot ; \cdot)$ is the mixture density and $f(y | \mu, \phi)$
is the density of the beta distribution using the mean-precision
parameterization. Further the component weights $\pi(k ; \cdot)$ are
nonnegative and sum to one for all $k$. In the following they are
assumed to be determined by
\begin{align*}
  \pi(k ; c, \alpha) &= \frac{\textrm{exp}\{w^{\top}\alpha_k\}}
  {\sum_{u=1}^K\textrm{exp}\{w^{\top}\alpha_u\}}
\end{align*}
with $\alpha_1 \equiv 0$ to ensure identifiability. 

\cite{betareg:Smithson+Segale:2009} and
\cite{betareg:Smithson+Merkle+Verkuilen:2011} consider finite mixtures
of beta regression models to analyze priming effects in judgements of
imprecise probabilities. \cite{betareg:Smithson+Segale:2009} fit
mixture models where they investigate if priming has an effect on the
size of the latent groups, i.e., they include the information on
priming as a predictor variable $w$.
\cite{betareg:Smithson+Merkle+Verkuilen:2011} assume that for at least
one component distribution the location parameter is a-priori known
due to so-called ``anchors''. E.g. for partition priming, an anchor
would be assumed at position $1/K$ if the respondents are primed to
believe that there are $K$ possible events. The component distribution
for this anchor can be either assumed to follow a beta distribution
with known parameters for the mean and the precision or a uniform
distribution with known support.

Package \pkg{flexmix} \citep{betareg:Leisch:2004,
  betareg:Gruen+Leisch:2008} implements a general framework for
estimating finite mixture models using the EM algorithm. The EM
algorithm is an iterative method for ML estimation in a missing data
setting. The missing data for mixture models is the information to
which component an observation belongs. The EM algorithm exploits the
fact that the complete likelihood for the data as well as the missing
information is easier to maximize.  In general for mixture models the
a-posteriori probabilities of an observation to be from each component
given the current parameter estimates are determined in the
E-steop. The M-step then consists of maximising the complete
likelihood where the missing component memberships are replaced by the
a-posteriori probabilities. This implies that different mixture models
only require the implementation of a suitable M-step driver. Function
\fct{betareg.fit} provides functionality for weighted ML estimation of
beta regression models and allows to easily implement the M-step. 

The function \fct{betamix} allows to fit finite mixtures of beta
regression models using the package \pkg{betareg}. It has the
following arguments:
%
\begin{Code}
betamix(formula, data, k, fixed, subset, na.action,
  link = "logit", link.phi = "log", control = betareg.control(...),
  FLXconcomitant = NULL, extra_components,
  verbose = FALSE, ID, nstart = 3, FLXcontrol = list(), cluster = NULL,
  which = "BIC", ...)
\end{Code}
%
\begin{itemize}
\item Arguments \code{formula}, \code{data}, \code{subset},
  \code{na.action}, \code{link}, \code{link.phi} and \code{control}
  are the same as for \fct{betareg}.
\item Arguments \code{cluster}, \code{FLXconcomitant} and
  \code{FLXcontrol} are the same as for function \fct{flexmix}
  (in the latter two cases without prefix \code{FLX}).
\item Arguments
  \code{k}, \code{verbose}, \code{nstart} and \code{which} are used to
  specify the repeated runs of the EM algorithm using function
  \fct{stepFlexmix} and which model should be returned.
\item Because the formula for specifying the beta regression model is
  already a two-part formula a grouping variable can be specified via
  argument \code{ID}.
\item Further arguments for the component specific model are
  \code{fixed} and \code{extra_components}. \code{fixed} can be used
  to specify via a formula interface the covariates for which
  parameters are the same over components. \code{extra_components} is
  a list of \code{"extraComponent"} objects where the distribution of
  the component needs to be completely specified via \code{type} and
  setting the parameters through \code{coef} and \code{delta}.
\begin{verbatim}
<<results=tex, echo=false>>=
cat(prettyPrint(prompt(extraComponent, filename = NA)$usage[[2]], sep = ", ", 
  linebreak = paste("\n", paste(rep(" ", nchar("extraComponent") + 1), collapse = ""), sep= ""),
  width = 60))
@ 
\end{verbatim}
\end{itemize}

\section{Illustrative application}\label{sec:illustr-appl}

<<echo=FALSE, results=hide>>=
data("ReadingSkills", package = "betareg")
mean_accuracy <- 
  format(round(with(ReadingSkills, tapply(accuracy, dyslexia, mean)), digits = 3),
         nsmall = 3)
mean_iq <- 
  format(round(with(ReadingSkills, tapply(iq, dyslexia, mean)), digits = 3),
         nsmall = 3)
@ 

In the following we re-consider an analysis of reading accuracy data
for nondyslexic and dyslexic Australian children
\citep{betareg:Smithson+Verkuilen:2006}. The data consists of
\Sexpr{nrow(ReadingSkills)} observations of children aged between
eight years five months and twelve years three months. For each child
the scores on a test of reading accuracy (variable \code{accuracy}) as
well as on a nonverbal intelligent quotient (variable \code{iq},
converted to $z$~scores) as well as if the child is dyslexic or not
(variable \code{dyslexic}) are reported. The
\Sexpr{table(ReadingSkills$dyslexia)["yes"]} dyslexic children have a
mean reading accuracy of \Sexpr{mean_accuracy["yes"]} and a mean IQ
score of $\Sexpr{mean_iq["yes"]}$, the
\Sexpr{table(ReadingSkills$dyslexia)["no"]} nondyslexic children a
mean reading accuracy of \Sexpr{mean_accuracy["no"]} and a mean IQ
score of $\Sexpr{mean_iq["no"]}$.

\cite{betareg:Smithson+Verkuilen:2006} investigated whether dyslexic
children have a different score on the reading accuracy test even when
corrected for IQ score. They fit a beta regression with main and
interaction effects for \code{iq} and \code{dyslexic} for modelling
the mean and only main effects for both variables for the
dispersion. Their model as well as the comparison to the results of an
OLS regression using the logit-transformed \code{accuracy} as response
are given in \cite{betareg:Cribari-Neto+Zeileis:2010}. 

\subsection{Bias correction and reduction}

Compute all three flavors of models for the model with interactions both in
the mean and precision equations.

<<ReadingSkills-bias>>=
data("ReadingSkills", package = "betareg")
rs_f <- accuracy ~ dyslexia * iq | dyslexia * iq
rs_ml <- betareg(rs_f, data = ReadingSkills, type = "ML")
rs_bc <- betareg(rs_f, data = ReadingSkills, type = "BC")
rs_br <- betareg(rs_f, data = ReadingSkills, type = "BR")
@

Resulting coefficient estimates, standard errors, and log-likelihood are reported
in Table~\ref{tab:ReadingSkills-bias}. All three estimators give very similar results for
the mean equation. In the precision equation, main effects are slightly damped and the interaction
effect slightly amplified when using bias correction/reduction.

\fixme{I guess one cannot say much more exciting about this data set with respect to bias?}

The results from Table~\ref{tab:ReadingSkills-bias} are reassuring and illustrate that it
is valid to employ the plain ML estimator for analysis. This may not always be the case:
Appendix~\ref{sec:gasoline} replicate the results of \cite{betareg:Kosmidis+Firth:2010}
for a beta regression where the upward bias in precision leads to overoptimistic
standard errors of all regression coefficients.


\begin{table}[t!]
\centering
\begin{tabular}{llrrr}
\hline
 & & ML & Bias-corrected & Bias-reduced \\ \hline
<<ReadingSkills-bias-table, echo=FALSE, results=tex>>=
rs_list <- list(rs_ml, rs_bc, rs_br)
cf <- paste("$", format(round(sapply(rs_list, coef), digits = 3), nsmall = 3), "$\\phantom{)}", sep = "")
se <- paste("(", format(round(sapply(rs_list, function(x) sqrt(diag(vcov(x)))), digits = 3), nsmall = 3), ")", sep = "")
ll <- paste("$", format(round(sapply(rs_list, logLik), digits = 3), nsmall = 3), "$\\phantom{)}", sep = "")
cfse <- matrix(as.vector(rbind(cf, se)), ncol = 3)
cfse <- cbind(
  c("Mean", rep("", 7), "Precision", rep("", 7)),
  rep(as.vector(rbind(c("(Intercept)", "\\code{dyslexia}", "\\code{iq}", "\\code{dyslexia:iq}"), "")), 2),
  cfse[, 1:2],
  paste(cfse[,3],
    c(rep("\\\\", 7), "\\\\ \\hline", rep("\\\\", 7), "\\\\ \\hline")))
cfse <- rbind(cfse, c("Log-likelihood", "", ll[1:2], paste(ll[3], "\\\\ \\hline")))
writeLines(apply(cfse, 1, paste, collapse = " & "))
@

\end{tabular}
\caption{\label{tab:ReadingSkills-bias} Comparison of coefficients and standard errors
(in parentheses) in interaction model for reading skills. The ML estimator from
\texttt{rs\_ml}, the bias-corrected estimator from
\texttt{rs\_bc}, the bias-reduced estimator from \texttt{rs\_br} all give very similar results for
the mean equation. In the precision equation, main effects are slightly damped and the interaction
effect slightly amplified when using bias correction/reduction.}
\end{table}

\subsection{Beta regression tree}

For illustrating the use of model-based recursive partitioning methods
we assume a beta regression model with \code{accuracy} as response and
\code{iq} as explanatory variable for the nodes. The score on the
reading accuracy test is therefore assumed to vary with the nonverbal
IQ score. In order to assess if also being dyslexic has an influence
on the relationship between these two variables the variable
\code{dyslexic} is used as a partitioning variable. Further random
noise variables are added as partitioning variables to indicate the
ability of the method to select suitable variables for
partitioning. One noise variable is drawn from a standard normal
distribution, one from a uniform distribution and the third is a
categorical variable which takes two different values with equal
probability.

<<ReadingSkills-noise, echo=TRUE>>=
set.seed(1071)
n <- nrow(ReadingSkills)
ReadingSkills$x1 <- rnorm(n)
ReadingSkills$x2 <- runif(n)
ReadingSkills$x3 <- factor(sample(0:1, n, replace = TRUE))
@ 

The model-based tree is fitted using \fct{betatree}. The first
argument is a formula which specifies the model for the node: We have
a beta regression where the mean as well as the dispersion depend on
\code{iq} to model \code{accuracy}. The second argument is a formula
for the symbolic description of the partitioning variables. The
formulas are evaluated using \code{data}. Additional control arguments
for the recursive partitioning method used in \fct{mob\_control} can
be specified via the \code{\dots} argument. In this case the minimum
number of observations in a node in order to allow a split is given by
\code{minsplit = 10}.
%
<<ReadingSkills-tree, echo=TRUE, eval=FALSE>>=
rs_tree <- betatree(accuracy ~ iq | iq,
  ~ dyslexia + x1 + x2 + x3,
  data = ReadingSkills, minsplit = 10)
@
%
<<ReadingSkills-tree0, echo=FALSE>>=
if(cache & file.exists("betareg-ext-betatree.rda")) {
  load("betareg-ext-betatree.rda")
} else {
<<ReadingSkills-tree>>
if(cache) {
  save(rs_tree, file = "betareg-ext-betatree.rda")
} else {
  if(file.exists("betareg-ext-betatree.rda")) file.remove("betareg-ext-betatree.rda")
}
}
@
%
Alternatively the model could be specified using a three-part formula
where the third part is the symbolic description of the partitioning
variables.

<<ReadingSkills-tree2, echo=TRUE, eval=FALSE>>=
rs_tree <- betatree(accuracy ~ iq | iq | dyslexia + x1 + x2 + x3,
  data = ReadingSkills, minsplit = 10)
@

The returned object is of class \code{"\Sexpr{class(rs_tree)}"} which
contains an object of class
\code{"\Sexpr{class(rs_tree[["mob"]])}"}. All methods for
\code{"\Sexpr{class(rs_tree[["mob"]])}"} can be re-used, e.g., the
\fct{plot} method (see Figure~\ref{fig:betatree}).

<<ReadingSkills-tree3, echo=TRUE>>=
plot(rs_tree)
@ 

Figure~\ref{fig:betatree} indicates that the data was only split into
two sub-samples. None of the three noise variables was selected in
order to perform a split, but only variable \code{dyslexia}. This
indicates that the relationship between the IQ score and the reading
accuracy doesn not depend on the noise variables which we would
expect. By contrast, the relationship between these two variables
differ for dyslexic and nondyslexic children. The beta regressions
fitted two each of the two groups of children are illustrated in the
two leaf nodes. Note that the fitted models use the IQ score as
predictor for the mean as well as the dispersion.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-tree-plot, fig=TRUE, height=7, width=10, echo=FALSE>>=
plot(rs_tree)
@
\caption{\label{fig:betatree} Partitioned beta regression model for the
  \code{ReadingSkills} data.}
\end{figure}

\fixme{One could add the information about the significance tests.
In node~1 only dyslexia shows significant instability while the
noise variables are all nonsignificant. In node~2, dyslexia cannot be
used for splitting anymore and all other variables are still nonsignificant.
With only 19~observations, node~3 is considered too small to warrant
further splitting and hence no tests are carried out.}

<<ReadingSkills-tree-sctest>>=
sctest(rs_tree)
@

\fixme{maybe also show something else of the fitted model, e.g.}

<<ReadingSkills-tree-coef>>=
coef(rs_tree)
@

\subsection{Latent class beta regression}

In the following we assume that the information if the children are
dyslexic or not is not available. Modelling the relationship between
reading accuracy and IQ score is now complicated by the fact that
latent groups exist in the data where this relationship is
different. 

\fixme{Comment on: The group of non-dyslexic children is challenging
as it has rather high variance induced by two subgroups -- one with
essentially perfect accuracy and one with strongly increasing accuracy
by IQ score. In a model with observed \code{dyslexia}, this can be
captured by different variances in the two components. However, if
\code{dyslexia} is unobserved and a mixture model is employed to
infer the groups, then this is problematic. The subgroup with perfect
reading score will typically be selected as one component but then
not all parameters are identified. To adress this \dots}

We fit a finite mixture model with three components where
one component is used to capture those children who have a perfect
reading accuracy test score. Following
\cite{betareg:Smithson+Merkle+Verkuilen:2011} this additional
component is assumed to follow a uniform distribution on the interval
\code{coef} $\pm$ \code{delta}.
%
<<ReadingSkills-mix, echo=TRUE, eval=FALSE>>=
rs_mix <- betamix(accuracy ~ iq, data = ReadingSkills, k = 3,
  extra_components = extraComponent(type = "uniform", 
    coef = 0.99, delta = 0.01), nstart = 10)
@

%
<<ReadingSkills-mix2, echo=FALSE>>=
if(cache & file.exists("betareg-ext-betamix.rda")) {
 load("betareg-ext-betamix.rda")
} else {
<<ReadingSkills-mix>>
if(cache) {
  save(rs_mix, file = "betareg-ext-betamix.rda")
} else {
  if(file.exists("betareg-ext-betamix.rda")) file.remove("betareg-ext-betamix.rda")
}
}
@

The argument \code{nstart} is set to
\Sexpr{rs_mix$flexmix@control@nrep}. This implies that the EM
algorithm is run \Sexpr{rs_mix$flexmix@control@nrep} times with
different random initializations and that only the best solution
according to the log-likelihood is returned in order to increase the
chance that the global optimum is detected. The EM algorithm does at
best only converge to a local optimum and the initialization
already determines the result. 

The returned fitted model is of class \code{"\Sexpr{class(rs_mix)}"}
and has methods for \Sexpr{betamix_methods}. These methods re-use
functionality already available for finite mixture models directly
fitted using \fct{flexmix} from package \pkg{flexmix}. The
\fct{print} method shows the function call as well as provides
information on how many observations are assigned to each of the
components according to the maximum a-posteriori
probability. Furthermore it is indicated if the EM algorith converged
or not and in the case of convergence how many iterations were
performed.

<<ReadingSkills-mix3>>=
rs_mix
@ 

The \fct{summary} method provides more information on the estimated
coefficients as well as their standard errors. Internally function
\fct{optim} is used to determine the Hessian matrix needed for the
standard errors.

<<ReadingSkills-mix4>>=
summary(rs_mix)
@ 

Because only two components are freely estimated and the parameters
for the third component were fixed a-priori, the detailed information
on the estimated parameters is only provided for components 1 and
2. The regression part for the mean indicates that in the first
component the IQ score does not significantly affect the achieved
accuracy, while there is a positive significant effect of the
IQ score on accuracy in the second component. 

A cross-tabulation of the cluster assignments of the mixture model
with the variable \code{dyslexia} indicates that no dyslexic children
are assigned to the third component. Furthermore, children assigned to
the first component have a high probability
(\Sexpr{round(prop.table(table(clusters(rs_mix),ReadingSkills$dyslexia),1)[1,2]*100)}\%)
of being dyslexic.

<<ReadingSkills-mix5>>=
table(clusters(rs_mix), ReadingSkills$dyslexia)
@ 

The fitted mean regression lines for each of the three components are
provided in Figure~\ref{fig:betamix} (left). The observations are
shaded according to their a-posteriori probabilities. The stronger the
shading is in blue the higher is the a-posteriori probability for this
observation to be from component 1. Red shading corresponds to the
second component and green to the third. For comparison on the right
the mean regression lines for the dyslexic and non-dyslexic children
of the model where an interaction with the variable \code{dyslexic} is
specified in the regressions for mean and precision are
provided. These mean regression lines are equivalent to those obtained
using model-based recursive partitioning. 

\fixme{Comment on: With observed dyslexia, the increased heterogeneity
in the control group can be captured by differences in the precision
equation, i.e., in the variance. However, for unobserved dyslexia, it
is more appropriate to capture the increased heterogeneity in the
control group by two components, one of which corresponds to perfect
reading accuracy independent of IQ score.}

<<ReadingSkills-betamix-plot1, eval = FALSE, echo=FALSE, fig=TRUE>>=
ix <- as.numeric(ReadingSkills$dyslexia)
col1 <- hcl(c(0, 260), 90, 40)[ix]
col2 <- hcl(c(0, 260), 10, 95)[ix]
plot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19,
  cex = 1.5, xlim = c(-2, 2), main = "Partitioned model (dyslexia observed)")
points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1, col = col1)
@

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-betamix-plot3, echo=FALSE, fig=TRUE, height=5.5, width=10>>=
par(mfrow = c(1, 2))
ix <- as.numeric(ReadingSkills$dyslexia)
prob <- 2 * (posterior(rs_mix)[cbind(seq_along(ix), clusters(rs_mix))] - 0.5)
col3 <- hcl(c(260, 0, 130), 65, 45, fixup = FALSE)
col1 <- col3[clusters(rs_mix)]
col2 <- hcl(c(260, 0, 130)[clusters(rs_mix)], 65 * abs(prob)^1.5, 95 - 50 * abs(prob)^1.5, fixup = FALSE)
plot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19, cex = 1.5,
  xlim = c(-2, 2), main = "Mixture model (dyslexia unobserved)")
points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1, col = col1)
iq <- -30:30/10
cf <- rbind(coef(rs_mix, model = "mean", component = 1:2), c(qlogis(0.99), 0))
for(i in 1:3) lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2, col = col3[i]) 
<<ReadingSkills-betamix-plot1>>
cf <- coef(rs_tree, model = "mean")
col3 <- hcl(c(0, 260), 90, 40)
for(i in 1:2) lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2, col = col3[i]) 
@
\caption{\label{fig:betamix} Fitted regression lines for the mixture
  model with three components and the observations shaded according to
  their a-posteriori probabilities (left). Fitted regression lines
  for the partitioned beta regression model with shading according to
  the observed \code{dyslexic} variable where dyslexic and non-dyslexic
  children are in blue and red, respectively (right).}
\end{figure}

\section{Conclusions}\label{sec:conclusions}


\section*{Acknowledgments}

This research was funded by the Austrian Science Fund (FWF): V170-N18.

\bibliography{betareg}

\newpage

\begin{appendix}

\section{Bias correction/reduction for gasoline yield data} \label{sec:gasoline}

Replicate results from \cite{betareg:Kosmidis+Firth:2010}

Compute all three flavors of estimators for the fixed precision beta regression
model considered in \cite{betareg:Ferrari+Cribari-Neto:2004} and replicated
in \cite{betareg:Cribari-Neto+Zeileis:2010}.

<<GasolineYield-bias>>=
data("GasolineYield", package = "betareg")
gy <- lapply(c("ML", "BC", "BR"), function(x)
  betareg(yield ~ batch + temp, data = GasolineYield, type = x))
@

\begin{table}[b!]
\centering
\begin{tabular}{lrrrrrr}
\hline
& \multicolumn{2}{c}{ML} & \multicolumn{2}{c}{Bias-corrected} & \multicolumn{2}{c}{Bias-reduced} \\ \hline
<<ReadingSkills-bias-table, echo=FALSE, results=tex>>=
cf <- matrix(paste("$", format(round(sapply(gy, coef), digits = 5), nsmall = 5), "$\\phantom{)}", sep = ""), ncol = 3)
se <- matrix(gsub(" ", "",
  paste("(", format(round(sapply(gy, function(x) sqrt(diag(vcov(x)))), digits = 5), nsmall = 5), ")", sep = ""),
  fixed = TRUE), ncol = 3)
cfse <- cbind(cf[,1], se[,1], cf[,2], se[,2], cf[,3], se[,3])
cfse <- cbind(
  c(paste("$\\beta_{", 1:11, "}$", sep = ""), "$\\phi$"),
  cfse[, 1:5],
  paste(cfse[,6],
    c(rep("\\\\", 11), "\\\\ \\hline")))
writeLines(apply(cfse, 1, paste, collapse = " & "))
@

\end{tabular}
\caption{\label{tab:GasolineYield-bias} Coefficient estimates and standard errors (in parentheses)
for fixed precision beta regression models with different types of estimators on gasoline yield data.}
\end{table}

The precision parameter is considerably dampened by bias correction/reduction

<<GasolineYield-phi>>=
sapply(gy, coef, model = "precision")
@

while the log-likelihood does not change much

<<GasolineYield-phi>>=
sapply(gy, coef, model = "precision")
@

Coefficients and standard errors in Table~\ref{tab:GasolineYield-bias}


\end{appendix}

\end{document}
