\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf, natbib}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}
\newcommand*{\hder}[2]{\partial{#1}/\partial{#2} }
\newcommand*{\hderstraight}[2]{\mathrm{d}{#1}/\mathrm{d}{#2} }
\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\trace}{\mathop{\rm tr}}
\newcommand{\topd}{{\rm D}}

\author{Bettina Gr\"un\\Johannes Kepler\\Universit\"at Linz
   \And Ioannis Kosmidis\\University College London
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Bettina Gr\"un, Ioannis Kosmidis, Achim Zeileis}

\title{Extended Beta Regression in \proglang{R}: Shaken, Stirred, Mixed, and Partitioned}
\Plaintitle{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned}

\Keywords{Beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, \proglang{R}}
\Plainkeywords{Beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, R}

\Abstract{
shaken and stirred (bias correction and reduction),
mixed (finite mixture model),
and partitioned (tree model)
}

\Address{
  Bettina Gr\"un\\
  Institut f\"ur Angewandte Statistik\\
  Johannes Kepler Universit{\"at} Linz\\
  Altenbergerstra{\ss }e 69\\
  4040 Linz, Austria\\
  E-mail: \email{Bettina.Gruen@jku.at}\\
  URL: \url{http://ifas.jku.at/gruen/}\\

  Ioannis Kosmidis\\
  Department of Statistical Science\\
  University College London\\
  Gower Street\\
  London WC1E 6BT, United Kingdom\\
  E-mail: \email{ioannis@stats.ucl.ac.uk}\\
  URL: \url{http://www.ucl.ac.uk/~ucakiko/}\\

  Achim Zeileis\\
  Department of Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}
}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned}
%\VignetteDepends{stats,betareg,car,lmtest,sandwich,strucchange}
%\VignetteKeywords{Beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, R}
%\VignettePackage{betareg}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ", useFancyQuotes = FALSE)
library("betareg")
combine <- function(x, sep, width) {
  cs <- cumsum(nchar(x))
  remaining <- if (any(cs[-1] > width)) combine(x[c(FALSE, cs[-1] > width)], sep, width)
  c(paste(x[c(TRUE, cs[-1] <= width)], collapse= sep), remaining)
}
prettyPrint <- function(x, sep = " ", linebreak = "\n\t", width = getOption("width")) {
  x <- strsplit(x, sep)[[1]]
  paste(combine(x, sep, width), collapse = paste(sep, linebreak, collapse = ""))
}
cache <- TRUE
enumerate <- function(x) paste(paste(x[-length(x)], collapse = ", "), x[length(x)], sep = " and ")
betamix_methods <-
  enumerate(paste("\\\\fct{", gsub("\\.betamix", "", as.character(methods(class = "betamix"))), "}", sep = ""))
@
\begin{document}

\section{Introduction}\label{sec:intro}

Beta regression is a model for continuous response variables $y$ which
assume values in the open standard unit interval $(0, 1)$. Such
response variables may stem from rates, proportions, concentrations,
etc. A double index regression allowing to model the mean as well as
the precision through covariates was introduced by
\cite{betareg:Ferrari+Cribari-Neto:2004} and extensions were proposed
by \cite{betareg:Smithson+Verkuilen:2006} and
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}.
\cite{betareg:Ferrari+Cribari-Neto:2004} employed an alternative
parametrization of the Beta distribution such that the mean and
variance of a Beta-distributed variable are easily characterized by
the parameterization.  In this parameterization the Beta distribution
has the density
%
\begin{equation} \label{eq:density}
f(y;\mu,\phi) =
\frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}\,
,\quad 0<y<1\, ,\quad  0<\mu<1\, , \quad \phi > 0 \, ,
\end{equation}
%
where $\Gamma(\cdot)$ is the gamma function.  The mean of a
Beta-distributed variable is then given by $\E(Y) = \mu$ and the
variance by $\VAR(Y) = \mu(1-\mu)/(1+\phi)$. Hence $\phi$ is a
precision parameter.

The double index Beta regression model is specified in the following
way. Suppose we have observations on $n$ independent Beta distributed
random variables $Y_1, \ldots, Y_n$, with $Y_i$ having mean $\mu_i$
and variance $\VAR(Y) = \mu_i(1-\mu_i)/(1+\phi_i)$ $(i = 1, \dots,
n)$.  The parameters $\mu_i$ and $\phi_i$ are linked to linear
predictors $\eta_i$ and $\zeta_i$ as follows
\begin{align}
  g_1(\mu_i) & =  \eta_i =   x_i^\top \beta \, , \label{eq:link1}  \\
  g_2(\phi_i) &  = \zeta_i =   z_i^\top \gamma\, , \label{eq:link2}
\end{align}
where $x_i$ and $z_i$ are $p$ and $q$ dimensional vectors of
covariates observed along $y_i$ $(i = 1, \ldots, n)$, and $\beta =
(\beta_1, \ldots, \beta_p)$, $\gamma = (\gamma_1, \ldots, \gamma_q)$
are the vectors of parameters associated with the means and the
precisions, respectively. The functions $g_1(.)$ and $g_2(.)$ are link
functions with the property of being monotone on their arguments. By
the range of $\mu_i$ and $\phi_i$, $g_1(.)$ should ideally be such
that the unit interval $(0, 1)$ is mapped on the real line, and
$g_2(.)$ should ideally be such that the interval $(0, \infty)$ is
mapped on the real line. Suitable candidates for $g_1(\cdot)$ are the
logit, probit and generally any inverse of a cumulative distribution
function, and for $g_2(\cdot)$ the log function. Another common
candidate function for $g_2(\cdot)$ is the identity function which,
though, when used one should take care to constrain the model to have
$\zeta_i > 0$ $(i = 1, \ldots, n)$.

The coefficients $\beta$ and $\gamma$ can be estimated by maximum
likelihood (ML). The usual central limit theorem holds with associated
asymptotic tests, e.g., likelihood ratio, Wald, score/Lagrange
multiplier (LM).

The \proglang{R} package \pkg{betareg} which implements ML estimation
was introduced by \cite{betareg:Cribari-Neto+Zeileis:2010}. The main
model fitting function is \fct{betareg}. The interface as well as the
fitted models are designed to be similar to those for \fct{glm}. The
model specification is via a \code{formula} plus \code{data}. Because
two types of covariates need to be distinguished a two part formula is
allowed, e.g., \code{y ~ x1 + x2 + x3 | z1 + z2}. The covariates
\code{x1}, \code{x2} and \code{x3} are the covariates for the mean and
\code{z1} and \code{z1} are the covariates for the precision
parameter. The formula is processed using package \pkg{Formula}
\citep{betareg:Zeileis+Croissant:2010}. Function \fct{betareg}
internally uses function \fct{optim} as a general purpose optimizer to
maximize the log-likelihood. The fitted model has methods for several
extractor functions, e.g., \fct{coef}, \fct{vcov}, \fct{residuals},
\fct{logLik}. Base methods for the returned fitted model are
\fct{summary}, \fct{AIC}, \fct{confint}. Further methods are available
for functions from \pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}
and \pkg{car} \citep{betareg:Fox+Weisberg:2011}, e.g., \fct{lrtest},
\fct{waldtest}, \fct{coeftest} and \fct{linearHypothesis}. Multiple
testing is possible via package \pkg{multcomp}
\citep{betareg:Hothorn+Bretz+Westfall:2008} and structural change
tests can be performed using package \pkg{strucchange}
\citep{betareg:Zeileis+Leisch+Hornik:2002}.

The specified Beta regression model intends to capture the
relationship between certain predictor variables and the dependent
$(0,1)$ variable. However, these relationships vary for groups in the
population if heterogeneity is present. If variables can be identified
which are directly related to these groups, these variables can be
included in the regression. Disadvantages of this approach are that
(1) the resulting regression model is very complex and hard to
understand and interpret and (2) unnecessary complexity is introduced
if the differences are only present in a subset of the combined groups
induced by several categorical variables. Model-based recursive
partitioning is an alternative approach which avoids these
drawbacks. If these groups cannot be directly related to observed
variables, the heterogeneity can be accounted for using finite mixture
models \citep[for an introduction to finite mixture models see for
example][]{betareg:McLachlan+Peel:2000,
  betareg:Fruehwirth-Schnatter:2006}.

In this paper, we introduce extensions of the \pkg{betareg} package
where model heterogeneity is taken into account in the case where
covariates to characterize the groups are available as well as when
the heterogeneity is due to latent variables. We present model-based
recursive partitioning of Beta regressions and finite mixtures of Beta
regression models by building on funtionality provided by packages
\pkg{party} \citep{betareg:Hothorn+Hornik+Strobl:2011} and
\pkg{flexmix} \citep{betareg:Leisch+Gruen:2011}. Both extensions are
derived by re-using the fitting function in more complex models which
provide a general framework for extending regression models.

Furthermore, in the light of the inferential drawbacks that the
maximum likelihood estimator may exhibit in the context of Beta
regression \citep{betareg:Kosmidis+Firth:2010}, the \pkg{betareg}
package supports estimation via three alternative methods (maximum
likelihood estimation, bias reduction and bias correction) by adopting
the unifying iteration developed in \cite{betareg:Kosmidis+Firth:2010}.

\section{Bias correction and reduction in Beta regressions}
\label{sec:bias}
\subsection{Preample}
\cite{betareg:Kosmidis+Firth:2010} show that the reduction or
correction of the bias of the maximum likelihood estimator in
parametric models may be achieved via a unifying \emph{quasi} Fisher
scoring algorithm. \cite{betareg:Kosmidis+Firth:2010} illustrate the
applicability of their algorithm at a Beta regression setting with a
common dispersion parameter $\phi$ for all subjects, also reveling
some errors in previous literature for the reduction of bias in Beta
regression models (specifically mistakes
\citealt*{betareg:Ospina+Cribari-Neto+Vasconcellos:2006} and
\citealt*{betareg:Simas+Barreto-Souza+Rocha:2010} that led to
misleading negative conclusions about the effect of the
reduction/correction of the bias on inferences for Beta regression
models). In \cite{betareg:Kosmidis+Firth:2010}, it is shown that the
reduction/correction of bias for Beta regression models can be
desirable because the maximum likelihood estimator of $\phi$
demonstrates substantial upwards bias, which in turn leads to
underestimation of asymptotic standard errors and hence
over-optimistic Wald-type inferences (for example, confidence
intervals with coverage far below the nominal levels). From a
practical point of view, covariates with significant contributions to
explaining the response may appear insignificant if the maximum
likelihood estimator is used for Wald-type inferences.

The results in \cite{betareg:Kosmidis+Firth:2010} are extended here to
cover the case of models that involve a regression part for the
dipersion parameters as shown in equation (\ref{eq:link2}).

\subsection{Shaking and stirring: Iteration}
\label{sub:iteration}
Denote by $0_{k}$ a vector of $k$ zeros and by $S(\theta)$ the vector
of the log-likelihood derivatives for a parametric model with
parameter $\theta$. \cite{betareg:Firth:1993} showed that the solution
$\tilde\theta$ of the equation
\begin{equation}
  \label{eq:adjest}
S(\tilde{\theta}) + A(\tilde{\theta}) = 0_{p+q} \, ,
\end{equation}
has smaller asymptotic bias than the maximum likelihood estimator, if
the $t$th component of the vector $A(\theta)$ has the form
\[
A_t(\theta) = \frac{1}{2}\trace\left[\{F(\theta)\}^{-1} \left\{
    P_t(\theta) + Q_t(\theta) \right\}\right] \quad (t = 1, \ldots,
p + q) \, ,
\]
where $F(\theta)$ is the expected infromation matrix and
\begin{align}
  P_t(\theta) & = \E\{S(\theta)S^\top(\theta)S_t(\theta)\}
  \quad (t = 1, \ldots, p + q)\, , \label{eq:P} \\
  Q_t(\theta) & = -\E\left\{I(\theta)S_t(\theta) \right\} \quad (t =
  p + 1, \ldots, p + q)\, , \label{eq:Q}
\end{align}
where $S_t(\theta)$ denotes the $t$th component of $S(\theta)$ $(t =
1, \ldots, p + q)$ and $I(\theta)$ is the observed information matrix
(minus the matrix of second derivatives of the log-likelihood with
respect to $\theta$).

The quasi Fisher scoring iteration that has been developed in
\cite{betareg:Kosmidis+Firth:2010} attempts to solve equation
\eqref{eq:adjest}. Specifically, at the $j$th step of the iterative
procedure, the current value $\theta^{(j)}$ of the parameter vector is
updated to $\theta^{(j+1)}$ by
\begin{equation}
  \label{eq:iteration}
\theta^{(j+1)} = \theta^{(j)} + \left\{F\left(\theta^{(j)}\right)\right\}^{-1}
S\left(\theta^{(j)}\right) - b\left(\theta^{(j)}\right) \, ,
\end{equation}
where $b(\theta) = - \{F(\theta)\}^{-1} A(\theta)$ is the vector of
the first term in the expansion of the bias of the maximum
likelihood estimator.

If the summand $b\left(\theta^{(j)}\right)$ was ignored, then
iteration \eqref{eq:iteration} would be the usual Fisher scoring
iteration that can be used to solve the maximum likelihood score
equations $S(\hat\theta) = 0_{p+q}$.

On the other hand, if the starting value $\theta^{(0)}$ is the maximum
likelihood estimator $\hat\theta$, then $\theta^{(1)}$ is the
bias-corrected estimator $\theta^\dagger$ of $\theta$ defined as
\[
\theta^\dagger = \hat\theta - b(\hat\theta) \, ,
\]
which also has smaller asymptotic bias than that of the maximum
likelihood estimator \citep{betareg:efron:75}.

Hence, iteration provides a unified framework for implementing all
maximum likelihood, bias reduction and bias correction by merely
deciding whether the summand $b\left(\theta^{(j)}\right)$ is absent or
present in the right hand side of \eqref{eq:iteration}, or whether
more than one iterations should be allowed in the latter case.

\subsection{Shaking and stirring: Ingredients}
Denote the vector of all model $p + q$ parameters by $\theta =
(\beta^\top, \gamma^\top)^\top$, and let $X$ and $Z$ be the $n\times
p$ and $n\times q$ matrices with $i$th row $x_i$ and $z_i$ $(i = 1,
\ldots, n)$, respectively. The ingredients required for setting the
iteration described in Subsection~\ref{sub:iteration} are closed-form
expressions for the vector of log-likelihood derivatives $S(\theta)$,
the expected information matrix $F(\theta)$ and the two higher-order
joint null cumulants of log-likelihood derivatives $P_t(\theta)$ and
$Q_t(\theta)$ shown in \eqref{eq:P} and \eqref{eq:Q}. Then all matrix
multiplications and inversions can be performed numerically during the
iterative procedure.

The fact that all the aforementioned quantities depend on $Z$ and $X$,
and that $S(\theta)$ and $I(\theta)$ depend additionally on the random
variables $Y_i$ $(i = 1, \ldots, n)$ has been concealed here merely
for notational simplicity.  The same convention is used for the
derivations below, additionally concealing the dependence on $\theta$
unless otherwise stated.

The log-likelihood for the Beta regression model in \eqref{eq:density}
is $l(\theta) = \sum_{i =1}^nl_i(\theta)$ with
\begin{equation}
\label{eq:loglik}
l_i(\theta) = \phi_i\mu_i
  (T_i - U_i) + \phi_i U_i +
  \log\Gamma(\phi_i) -
  \log\Gamma(\phi_i\mu_i) -\log\Gamma(\phi_i(1-\mu_i))
\end{equation}
where $\mu_i$ and $\phi_i$ are defined by inverting \eqref{eq:link1}
and \eqref{eq:link2}, respectively, and where $T_i = \log Y_i$ and
$U_i = \log(1 - Y_i)$ are the sufficient statistics for the Beta
distribution with natural parameters $\phi_i\mu_i$ and $\phi_i(1 -
\mu_i)$ $(i =1, \ldots, n)$, respectively.

Direct differentiation of the log-likelihood function gives that the
vector of log-likelihood derivatives has the form
\begin{equation}
  \label{eq:scores}
S(\theta) = \nabla_\theta l(\theta) = \left[
\begin{array}{c}
  X^\top \Phi D_1 \left(\bar{T} - \bar{U}\right) \\
  Z^\top D_2 \left\{ M\left(\bar{T} - \bar{U}\right) + \bar{U}  \right\}
\end{array}
\right]\, ,
\end{equation}
In the above expression, $\Phi = \diag\{\phi_1, \ldots, \phi_n\}$, $M
= \diag\{\mu_1, \ldots, \mu_n\}$, $D_1 = \diag\{d_{1,1}, \ldots, d_{1,
  n}\}$ and $D_2 = \diag\{d_{2,1}, \ldots, d_{2, n}\}$, where $d_{1,
  i} = \hderstraight{\mu_i}{\eta_i}$ and $d_{2,i} =
\hderstraight{\phi_i}{\zeta_i}$ $(i = 1, \ldots, n)$. Furthermore,
$\bar{T} = (\bar{T}_1, \ldots, \bar{T}_n)^\top$ and $\bar{U} =
(\bar{U}_1, \ldots, \bar{U}_n)^\top$ are the vectors of centred
sufficient statistics, with
\begin{align*}
\bar{T}_i & = T_i - \E(T_i) \, , \\
\bar{U}_i & = U_i - \E(U_i) \, ,
\end{align*}
where $\E(T_i) = \psi^{(0)}(\phi\mu_i) - \psi^{(0)}(\phi_i)$ and
$\E(U_i) = \psi^{(0)}(\phi(1-\mu_i)) + \psi^{(0)}(\phi_i)$, with
$\psi^{(r)}(k) = \hderstraight{^{r+1} \log\Gamma(k)}{k^{r+1}}$ the
polygamma function of degree $r$ $(r = 0, 1, \ldots)$.

Differentiating $l(\theta)$ one more time gives that the observed
information on $\theta$ is
\begin{equation}
\label{eq:obsinfo}
I(\theta) = F(\theta) - \left[
\begin{array}{cc}
  X^\top \Phi D_1' \diag\{\bar{T} - \bar{U}\}X & X^\top D_1\diag\{\bar{T} - \bar{U}\}D_2Z \\
  Z^\top D_2\diag\{\bar{T} - \bar{U}\}D_1X & Z^\top D_2'\left(M\diag\left\{\bar{T} -
      \bar{U}\} + \diag\{\bar{U}\right)Z \right\} \\
\end{array}
\right] \, ,
\end{equation}
where
\begin{equation}
\label{eq:expinfo}
F(\theta) = \left[
\begin{array}{cc}
  X^\top D_1\Phi K_2 \Phi D_1 X & X^\top D_1\Phi \left(MK_2 - \Psi_1\right)D_2Z \\
  Z^\top D_2 \left(MK_2 - \Psi_1\right)\Phi D_1  X &
  Z^\top D_2\left\{M^2K_2 + (1_n-2M)\Psi_1 - \Omega_1\right\}D_2Z
\end{array}
\right]\, ,
\end{equation}
is the expected information on $\theta$, because the second summand in
the right hand side of \eqref{eq:obsinfo} depends linearly to centred
sufficient statistics and hence has expectation zero. Here, $1_n$ is
the $n \times n$ identity matrix, $D_1' = \diag\{d'_{1,1}, \ldots,
d'_{1,n}\}$ with $d'_{1,i} = \hderstraight{^2\mu_i}{\eta_i^2}$ and
$D_2' = \diag\{d'_{2,1}, \ldots, d'_{2,n}\}$ with $d'_{2,i} =
\hderstraight{^2\phi_i}{\zeta_i^2}$ $(i = 1, \ldots, n)$. Furthermore,
$K_2 = \diag\{\kappa_{2,1}, \ldots, \kappa_{2,n}\}$, where
$\kappa_{2,i} = \VAR\left(\bar{T}_i -\bar{U}_i\right) =
\psi^{(1)}(\phi_i\mu_i) + \psi^{(1)}(\phi_i(1-\mu_i))$ and
\begin{align*}
  \Psi_r & = \diag\left\{\psi^{(r)}(\phi_1(1-\mu_1)), \ldots,
  \psi^{(r)}(\phi_n(1-\mu_n))\right\} \, ,\\
  \Omega_r & = \diag\left\{\psi^{(r)}(\phi_1), \ldots,
  \psi^{(r)}(\phi_n)\right\}\quad (r = 0, 1, \ldots; i =1,
\ldots, n)\, .
\end{align*}

Some tedious but straightforward algebra, along with direct use of the
results in \cite{betareg:Kosmidis+Firth:2010} for the joint cumulants
of $\bar{T}_i$ and $\bar{U}_i$ $(i = 1, \ldots, n)$, gives
\begin{equation}
  \label{eq:PQbeta}
  P_t(\theta) + Q_t(\theta) = \left[
    \begin{array}{cc}
      V_{\beta\beta, t} & V_{\beta\gamma, t} \\
      V_{\beta\gamma, t}^\top & V_{\gamma\gamma, t}
    \end{array}
  \right] \quad (t = 1, \ldots, p)\, ,
\end{equation}
\begin{equation}
  \label{eq:PQgamma}
  P_{p + s}(\theta) + Q_{p + s}(\theta)  = \left[
    \begin{array}{cc}
      W_{\beta\beta, s} & W_{\beta\gamma, s} \\
      W_{\beta\gamma, s}^\top & W_{\gamma\gamma, s}
    \end{array}
  \right] \quad (s = 1, \ldots, q) \, ,
\end{equation}
where
\begin{align*}
  V_{\beta\beta, t} & = X^\top \Phi^2 D_1 \left( \Phi D_1^2 K_3 + D_1'
    K_2 \right)X_t^\topd X \, , \\
  V_{\beta\gamma, t} & = X^\top\Phi D_1^2 D_2 \left\{\Phi\left(MK_3 + \Psi_2\right) +
  K_2\right)X_t^\topd R \, , \\
  V_{\gamma\gamma, t} & =
  R^\top\Phi D_1 \left\{ D_2^2\left(M^2 K_3 + 2M\Psi_2 -
      \Psi_2\right) + D_2'\left(MK_2 - \Psi_1\right)\right\}X_t^\topd R
\end{align*}
and
\begin{align*}
  W_{\beta\beta, s} & =
  X^\top \Phi D_2 \left\{\Phi D_1^2 \left(M K_3 + \Psi_2\right) +
    D_1'\left(MK_2 - \Psi_1\right) \right\}R_s^\topd X\, , \\
  W_{\beta\gamma, s} & = X^\top D_1 D_2^2 \left\{ \Phi \left(M^2 K_3 + 2M\Psi_2
      - \Psi_2\right) + MK_2 - \Psi_1 \right\}R_s^\topd R\, , \\
  W_{\gamma\gamma, s} & =
  R^\top D_2^3 \left\{M^3K_3 + \left( 3M^2 - 3M + 1_n \right) \Psi_2 -
  \Omega_2 \right\} R_s^\topd R  \\
& \qquad + R^\top D_2 D_2' \left\{M^2K_2 + \Psi_1 - 2M \Psi_1 -
  \Omega_1 \right\}R_s^\topd R\, ,
\end{align*}
where $K_3 = \diag\left\{\kappa_{3,1}, \ldots, \kappa_{3,n} \right\}$,
with $\kappa_{3,i} = \E\left\{\left(\bar{T}_i -
    \bar{U}_i\right)^3\right\} = \psi^{(2)}(\phi_i\mu_i) -
\psi^{(2)}(\phi_i(1 - \mu_i))$ $(i =1, \ldots, n)$. Furthermore,
$C_t^\topd$ denotes the diagonal matrix with non-zero components the
elements of $t$th column of a matrix $C$.

\subsection{Shaking and stirring with betareg}
Support for both, bias correction and bias reduction, has been added
in the principal model fitting function \fct{betareg} starting from
\pkg{betareg}~2.4-0. The interface of \fct{betareg} is essentially
the same as described in \cite{betareg:Cribari-Neto+Zeileis:2010}, just
a \code{type} argument has been added specifying the type of estimator.
%
\begin{Code}
  betareg(formula, data, subset, na.action, weights, offset,
    link = "logit", link.phi = NULL, type = c("ML", "BC", "BR"),
    control = betareg.control(...), model = TRUE, y = TRUE, x = FALSE, ...)
\end{Code}
%
The arguments in the first line (\code{formula}, \code{data}, \dots)
pertain to the data and model specification using a formula that
potentially may have two parts pertaining to mean and precision
equation, respectively.  The arguments \code{link} and \code{link.phi}
specify the link functions $g_1(\cdot)$ and $g_2(\cdot)$,
respectively. The argument \code{type} controls which estimates are
produced between the maximum likelihood (\code{type = "ML"}) , the
bias-reduced (\code{type = "BR"}) and the bias-corrected (\code{type =
  "BC"}) estimates. Internally, this is done by either neglecting or
not the summand $b\left(\theta^{(j)}\right)$ in iteration
\eqref{eq:iteration}, and by setting the number of allowed quasi
Fisher scoring iterations to zero in the latter case. Finally,
\code{control} is a list of control arguments and \code{model},
\code{y}, and \code{x} control whether the respective data components
are included in the fitted model object. See
\cite{betareg:Cribari-Neto+Zeileis:2010} for more details on all
arguments except \code{type}.

While the interface of \fct{betareg} is almost the same as in previous
versions, the internal code has been substantially
enhanced. Specifically, the optimization via \fct{optim} is now
(optionally) enhanced by an additional Fisher scoring iteration. As in
previous versions, the initial optimization of the likelihood is
carried out via \fct{optim}, by default with \code{method = "BFGS"},
using analytical gradients. In recent versions, this is followed by a
Fisher scoring iteration with both analytical gradients and expected
information. This iteration is either used to further improve the
numerical maximization of the likelihood (for \code{type = "ML"} or
\code{type = "BC"}) or to carry out the bias reduction (for \code{type
  = "BR"}) as detailed in Subsection~\ref{sub:iteration}. To control
the details of the (quasi) Fisher scoring, \fct{betareg.control} takes
two additional arguments \code{fsmaxit = 200} and \code{fstol = 1e-8}
controlling the maximal number of iterations and convergance
tolerance, respectively.  If the number of iterations is set to zero
(\code{fsmaxit = 0}), no Fisher scoring is carried out (allowed only
for \code{type = "ML"} and \code{"BC"}) and thus exactly replicating
results from previous versions of \pkg{betareg}.


\section{Beta regression trees}
\label{sec:trees}
Model-based recursive partitioning
\citep[MOB,][]{betareg:Zeileis+Hothorn+Hornik:2008} builds on the more
widely known method of classification and regression trees
\citep[CART, ][]{betareg:Breiman+Friedman+Olshen:1984}. As for CART,
the idea is to split the sample recursively with respect to available
variables (called \dquote{partitioning} variables in the following) in
order to capture differences in the response variable. While CART
tries to capture differences in the distribution of the response
variable (in particular with respect to location) directly, the aim of
model-based recursive partitioning is more broadly to capture
differences in parameters describing the distribution of the response.
In particular, this allows to incorporate regressor variables in a
parametric model for the response variable.

Here, we adapt the general MOB framework to the model-based partitioning
of Beta regressions, called \dquote{beta regression trees} for short.
The aim is to capture differences in the distribution that are not yet
adequately described by the regressor variables through a forward search.
Basically, the approach proceeds by (1)~fitting a Beta regression model,
(2)~assessing whether its parameters are stable across all partitioning
variables, (3)~splitting the sample along the partitioning variable associated
with the highest parameter instability, (4)~repeating these steps until some
stopping criterion is met. Thus, interactions and nonlinearities can
be incorporated by locally maximizing the likelihood of a partitioned
model. More precisely, the steps of the MOB algorithm adapted to Beta
regression are the following, where $c_{ij}$ is the $j$-th partitioning
variable ($j = 1, \dots, p$) for observation~$i$.
%
\begin{enumerate}
  \item Fit a Beta regression model with parameters $\beta$ and $\gamma$
        by maximizing the log-likelihood for all observations $y_i$ in the
	current sample.
  \item Assess whether the parameters $\beta$ and $\gamma$ are stable across
        each partitioning variable $c_{ij}$.
  \item If there is significant parameter instability with respect to
        at least one of the partitioning variables $c_{ij}$, split the
        sample along the variable $j*$ with the strongest association:
        Choose the breakpoint with highest improvement in the fitted
	log-likelihood.
  \item Repeat steps 1--3 recursively in the resulting subsamples
        until there is no significant instability any more or the
	sample size is too small.
\end{enumerate}

The MOB framework of \cite{betareg:Zeileis+Hothorn+Hornik:2008} is
generic in that it requires only the specification of a model with
additive objective function for which a central limit theorem
holds. This is the case for Beta regression under the usual ML
regularity conditions and the main building blocks that the MOB
algorithm requires are the contributions to the additive objective
function (in steps~1 and~3) and its associated score function (in
step~2). Here, the objective is the log-likelihood $l(\theta)$ and its
contributions $l_i(\theta)$ are given in \eqref{eq:loglik} . By
\eqref{eq:scores} and using the notation in Section \ref{sec:bias}, the
corresponding score (or gradient) contributions have the form
\begin{equation*}
  S_{i}(\theta) = \left[
    \begin{array}{c}
      \mu_i \phi_i d_{1,i} \left(\bar{T}_i -
  \bar{U}_i\right)x_{i1} \\
\vdots \\
      \mu_i \phi_i d_{1,i} \left(\bar{T}_i -
  \bar{U}_i\right)x_{ip} \vspace{1em} \\
d_{2,i}\left\{ \mu_i\left(\bar{T}_i -
  \bar{U}_i\right) + \bar{U}_i \right\}z_{i1} \\
\vdots \\
d_{2,i}\left\{ \mu_i\left(\bar{T}_i -
  \bar{U}_i\right) + \bar{U}_i \right\}z_{iq} \\
    \end{array}
    \right]\quad (i = 1, \ldots, n) \, .
  \end{equation*}
  The above contributions are employed for testing whether there are
  significant departures from zero across the partitioning
  variables. More specifically, MOB uses generalized M-fluctuation
  tests for parameter instability
  \citep{betareg:Zeileis:2006,betareg:Zeileis+Hornik:2007}:
  fluctuations in numeric variables are assessed with a $\sup$LM type
  test \citep{betareg:Andrews:1993} and categorical variables with a
  $\chi^2$ type test \citep{betareg:Hjort+Koning:2002}. See
  \cite{betareg:Zeileis+Hothorn+Hornik:2008} for further details and
  references.\footnote{An example of M-fluctuation tests for parameter
    instability (also known as structural change) in Beta regressions
    is also discussed in \cite{betareg:Zeileis:2006} and replicated in
    \cite{betareg:Cribari-Neto+Zeileis:2010}. However, this uses a
    double-maximum type test statistic, not a $\sup$LM or $\chi^2$
    statistic.}

Beta regression trees are implemented in the \pkg{betareg} package in function
\fct{betatree} taking the following arguments:
%
\begin{Code}
  betatree(formula, partition, data, subset, na.action,
    link = "logit", link.phi = "log", control = betareg.control(), ...)
\end{Code}
%
Essentially, almost all arguments work as for the basic \fct{betareg} function.
The main difference is that a \code{partition} formula (without left-hand side),
such as \code{~ c1 + c2 + c3} has to be provided to specify the partitioning variables $c_i$.
As an alternative, \code{partition} may be omitted when \code{formula} has three parts
on the right-hand side, such as \code{y ~ x1 + z2 | z1 | c1 + c2 + c3}, specifying
mean regressors $x_i$, dispersion regressors $z_i$, and partitioning variables $c_i$,
respectively.

The \fct{betatree} function takes all arguments and carries out all
data preprocessing and then calls the function \fct{mob} from the
\pkg{party} package
\citep{betareg:Hothorn+Hornik+Zeileis:2006,betareg:Hothorn+Hornik+Strobl:2011}. The
latter can perform all steps of the MOB algorithm in an
object-oriented manner, provided that a suitable model fitting
function (optimizing the log-likelihood) is specified and that
extractor functions are available for the optimized
log-likelihood \eqref{eq:loglik} and the score function
\eqref{eq:scores} at the estimated parameters. For model fitting
\fct{betareg.fit} is employed (through a suitable convenience
interface call \fct{betaReg}) and for extractions the \fct{logLik} and
\fct{estfun} methods \citep[see also][]{betareg:Zeileis:2006a} are
leveraged. To control details of the MOB algorithm -- such as the
significance level and the minimal subsample size in step~4 -- the
\code{...} argument is passed to \fct{mob}. (Note that this is
somewhat different from \fct{betareg} where \code{...} is passed to
\fct{betareg.control}.)


\section{Finite mixtures of Beta regressions}\label{sec:finite-mixtures-beta}

Finite mixtures are suitable models if the data is assumed to be from
different groups, but the group memberships are not observed. If
mixture models are fitted one aims at determining the parameters of
each group as well as the group sizes. Furthermore, the model can be
used to estimate from which group each observation is. In the case of
finite mixtures of Beta regression models the latent groups can be
assumed to differ in their mean and/or in their
precision. Furthermore, the group sizes can depend on further
covariates.

The mixture model with $K$ components which corresponds to $K$
groups is given by
\begin{align*}
  h(y ; x, z, c, \theta) & = \sum_{k = 1}^K \pi(k ; c, \alpha) f(y ;
  g_1^{-1}(x^{\top}\beta_k), g_2^{-1}(z^{\top}\gamma_k)),
\end{align*}
where $h(\cdot ; \cdot)$ is the mixture density and $f(y | \mu, \phi)$
is the density of the Beta distribution using the mean-precision
parameterization. Further the component weights $\pi(k ; \cdot)$ are
nonnegative and sum to one for all $k$. In the following they are
assumed to be determined by
\begin{align*}
  \pi(k ; c, \alpha) &= \frac{\textrm{exp}\{w^{\top}\alpha_k\}}
  {\sum_{u=1}^K\textrm{exp}\{w^{\top}\alpha_u\}}
\end{align*}
with $\alpha_1 \equiv 0$ to ensure identifiability.

\cite{betareg:Smithson+Segale:2009} and
\cite{betareg:Smithson+Merkle+Verkuilen:2011} consider finite mixtures
of Beta regression models to analyze priming effects in judgements of
imprecise probabilities. \cite{betareg:Smithson+Segale:2009} fit
mixture models where they investigate if priming has an effect on the
size of the latent groups, i.e., they include the information on
priming as a predictor variable $w$.
\cite{betareg:Smithson+Merkle+Verkuilen:2011} assume that for at least
one component distribution the location parameter is a-priori known
due to so-called ``anchors''. E.g. for partition priming, an anchor
would be assumed at position $1/K$ if the respondents are primed to
believe that there are $K$ possible events. The component distribution
for this anchor can be either assumed to follow a Beta distribution
with known parameters for the mean and the precision or a uniform
distribution with known support.

Package \pkg{flexmix} \citep{betareg:Leisch:2004,
  betareg:Gruen+Leisch:2008} implements a general framework for
estimating finite mixture models using the EM algorithm. The EM
algorithm is an iterative method for ML estimation in a missing data
setting. The missing data for mixture models is the information to
which component an observation belongs. The EM algorithm exploits the
fact that the complete likelihood for the data as well as the missing
information is easier to maximize.  In general for mixture models the
a-posteriori probabilities of an observation to be from each component
given the current parameter estimates are determined in the
E-steop. The M-step then consists of maximising the complete
likelihood where the missing component memberships are replaced by the
a-posteriori probabilities. This implies that different mixture models
only require the implementation of a suitable M-step driver. Function
\fct{betareg.fit} provides functionality for weighted ML estimation of
Beta regression models and allows to easily implement the M-step.

The function \fct{betamix} allows to fit finite mixtures of Beta
regression models using the package \pkg{betareg}. It has the
following arguments:
%
\begin{Code}
betamix(formula, data, k, fixed, subset, na.action,
  link = "logit", link.phi = "log", control = betareg.control(...),
  FLXconcomitant = NULL, extra_components,
  verbose = FALSE, ID, nstart = 3, FLXcontrol = list(), cluster = NULL,
  which = "BIC", ...)
\end{Code}
%
\begin{itemize}
\item Arguments \code{formula}, \code{data}, \code{subset},
  \code{na.action}, \code{link}, \code{link.phi} and \code{control}
  are the same as for \fct{betareg}.
\item Arguments \code{cluster}, \code{FLXconcomitant} and
  \code{FLXcontrol} are the same as for function \fct{flexmix}
  (in the latter two cases without prefix \code{FLX}).
\item Arguments
  \code{k}, \code{verbose}, \code{nstart} and \code{which} are used to
  specify the repeated runs of the EM algorithm using function
  \fct{stepFlexmix} and which model should be returned.
\item Because the formula for specifying the Beta regression model is
  already a two-part formula a grouping variable can be specified via
  argument \code{ID}.
\item Further arguments for the component specific model are
  \code{fixed} and \code{extra_components}. \code{fixed} can be used
  to specify via a formula interface the covariates for which
  parameters are the same over components. \code{extra_components} is
  a list of \code{"extraComponent"} objects where the distribution of
  the component needs to be completely specified via \code{type} and
  setting the parameters through \code{coef} and \code{delta}.
\begin{verbatim}
<<results=tex, echo=false>>=
cat(prettyPrint(prompt(extraComponent, filename = NA)$usage[[2]], sep = ", ",
  linebreak = paste("\n", paste(rep(" ", nchar("extraComponent") + 1), collapse = ""), sep= ""),
  width = 60))
@
\end{verbatim}
\end{itemize}

\section{Illustrative application}\label{sec:illustr-appl}

<<echo=FALSE, results=hide>>=
data("ReadingSkills", package = "betareg")
mean_accuracy <-
  format(round(with(ReadingSkills, tapply(accuracy, dyslexia, mean)), digits = 3),
         nsmall = 3)
mean_iq <-
  format(round(with(ReadingSkills, tapply(iq, dyslexia, mean)), digits = 3),
         nsmall = 3)
@

In the following we re-consider an analysis of reading accuracy data
for nondyslexic and dyslexic Australian children
\citep{betareg:Smithson+Verkuilen:2006}. The data consists of
\Sexpr{nrow(ReadingSkills)} observations of children aged between
eight years five months and twelve years three months. For each child
the scores on a test of reading accuracy (variable \code{accuracy}) as
well as on a nonverbal intelligent quotient (variable \code{iq},
converted to $z$~scores) as well as if the child is dyslexic or not
(variable \code{dyslexic}) are reported. The
\Sexpr{table(ReadingSkills$dyslexia)["yes"]} dyslexic children have a
mean reading accuracy of \Sexpr{mean_accuracy["yes"]} and a mean IQ
score of $\Sexpr{mean_iq["yes"]}$, the
\Sexpr{table(ReadingSkills$dyslexia)["no"]} nondyslexic children a
mean reading accuracy of \Sexpr{mean_accuracy["no"]} and a mean IQ
score of $\Sexpr{mean_iq["no"]}$.

\cite{betareg:Smithson+Verkuilen:2006} investigated whether dyslexic
children have a different score on the reading accuracy test even when
corrected for IQ score. They fit a Beta regression with main and
interaction effects for \code{iq} and \code{dyslexic} for modelling
the mean and only main effects for both variables for the
dispersion. Their model as well as the comparison to the results of an
OLS regression using the logit-transformed \code{accuracy} as response
are given in \cite{betareg:Cribari-Neto+Zeileis:2010}.

\subsection{Bias correction and reduction}

Compute all three flavors of models for the model with interactions both in
the mean and precision equations.

<<ReadingSkills-bias>>=
data("ReadingSkills", package = "betareg")
rs_f <- accuracy ~ dyslexia * iq | dyslexia * iq
rs_ml <- betareg(rs_f, data = ReadingSkills, type = "ML")
rs_bc <- betareg(rs_f, data = ReadingSkills, type = "BC")
rs_br <- betareg(rs_f, data = ReadingSkills, type = "BR")
@


\begin{table}[t!]
\centering
\begin{tabular}{llrrr}
\hline
 & & Maximum likelihood & Bias-corrected & Bias-reduced \\ \hline
<<ReadingSkills-bias-table, echo=FALSE, results=tex>>=
rs_list <- list(rs_ml, rs_bc, rs_br)
cf <- paste("$", format(round(sapply(rs_list, coef), digits = 3), nsmall = 3), "$\\phantom{)}", sep = "")
se <- paste("(", format(round(sapply(rs_list, function(x) sqrt(diag(vcov(x)))), digits = 3), nsmall = 3), ")", sep = "")
ll <- paste("$", format(round(sapply(rs_list, logLik), digits = 3), nsmall = 3), "$\\phantom{)}", sep = "")
cfse <- matrix(as.vector(rbind(cf, se)), ncol = 3)
cfse <- cbind(
  c("Mean", rep("", 7), "Precision", rep("", 7)),
  rep(as.vector(rbind(c("(Intercept)", "\\code{dyslexia}", "\\code{iq}", "\\code{dyslexia:iq}"), "")), 2),
  cfse[, 1:2],
  paste(cfse[,3],
    c(rep("\\\\", 7), "\\\\ \\hline", rep("\\\\", 7), "\\\\ \\hline")))
cfse <- rbind(cfse, c("Log-likelihood", "", ll[1:2], paste(ll[3], "\\\\ \\hline")))
writeLines(apply(cfse, 1, paste, collapse = " & "))
@

\end{tabular}
\caption{\label{tab:ReadingSkills-bias} Comparison of coefficients and standard errors
(in parentheses) in interaction model for reading skills. The ML estimator from
\texttt{rs\_ml}, the bias-corrected estimator from
\texttt{rs\_bc}, the bias-reduced estimator from \texttt{rs\_br} all give very similar results for
the mean equation. In the precision equation, main effects are slightly damped and the interaction
effect slightly amplified when using bias correction/reduction.}
\end{table}


\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-phi-plot, fig=TRUE, height=7, width=10, echo=FALSE>>=
pr_phi <- sapply(list("Maximum Likelihood" = rs_ml,
                      "Bias Correction" = rs_bc,
                      "Bias Reduction" = rs_br), predict, type = "precision")
pairs(log(pr_phi), panel = panel.smooth)
@
\caption{\label{fig:readingskillsbias} Scatterlots of the logarithm of
  the estimated logarithms of the precision parameters based on the
  maximum likelihood, bias-corrected and bias-reduced estimate.}
\end{figure}


Resulting coefficient estimates, standard errors, and log-likelihood
are reported in Table~\ref{tab:ReadingSkills-bias}. All three
estimators give very similar results for the mean equation. In the
precision equation, main effects are slightly damped and the
interaction effect slightly amplified when using bias
correction/reduction. Figure~\ref{fig:readingskillsbias} shows the
scatter plots of the logarithm of the estimated precision parameters
based on the maximum likelihood, the bias-corrected and the
bias-reduced estimates. It is apparent that the logarithms of the
estimated precision parameters based on the bias-corrected and
bias-reduced estimates are mildly shrunk towards zero. This is a
similar but much milder effect to the one described in
\cite{betareg:Kosmidis+Firth:2010} where substantial upwards bias was
detected on the maximum likelihood estimator of the precision
parameter, which in turn causes underestimated asymptotic standard
errors (note the direct dependence of the expected information matrix
on the precision parameters in \eqref{eq:expinfo}). The reason that
the effect is milder in this particular example relates to the fact
that bias for the precision parameters is reduced/corrected on the
log-scale where the maximum likelihood estimator has a more symmetric
distribution than in the original scale. Appendix~\ref{sec:gasoline}
replicates the corresponding results in
\cite{betareg:Kosmidis+Firth:2010} for a Beta regression where the
upward bias in precision leads to severely overoptimistic estimated
standard errors for all regression coefficients.

For the reading accuracy data, the similarity of the results in
Table~\ref{tab:ReadingSkills-bias} between the three different
estimation methods and Figure \ref{fig:readingskillsbias} are
reassuring and illustrate that analysis based on the ML estimator
would not be affected by any serious inferential disadvantages.

\subsection{Beta regression tree}

For illustrating the use of model-based recursive partitioning methods
we assume a Beta regression model with \code{accuracy} as response and
\code{iq} as explanatory variable for the nodes. The score on the
reading accuracy test is therefore assumed to vary with the nonverbal
IQ score. In order to assess if also being dyslexic has an influence
on the relationship between these two variables the variable
\code{dyslexic} is used as a partitioning variable. Further random
noise variables are added as partitioning variables to indicate the
ability of the method to select suitable variables for
partitioning. One noise variable is drawn from a standard normal
distribution, one from a uniform distribution and the third is a
categorical variable which takes two different values with equal
probability.

<<ReadingSkills-noise, echo=TRUE>>=
set.seed(1071)
n <- nrow(ReadingSkills)
ReadingSkills$x1 <- rnorm(n)
ReadingSkills$x2 <- runif(n)
ReadingSkills$x3 <- factor(sample(0:1, n, replace = TRUE))
@

The model-based tree is fitted using \fct{betatree}. The first
argument is a formula which specifies the model for the node: We have
a Beta regression where the mean as well as the dispersion depend on
\code{iq} to model \code{accuracy}. The second argument is a formula
for the symbolic description of the partitioning variables. The
formulas are evaluated using \code{data}. Additional control arguments
for the recursive partitioning method used in \fct{mob\_control} can
be specified via the \code{\dots} argument. In this case the minimum
number of observations in a node in order to allow a split is given by
\code{minsplit = 10}.
%
<<ReadingSkills-tree, echo=TRUE, eval=FALSE>>=
rs_tree <- betatree(accuracy ~ iq | iq,
  ~ dyslexia + x1 + x2 + x3,
  data = ReadingSkills, minsplit = 10)
@
%
<<ReadingSkills-tree0, echo=FALSE>>=
if(cache & file.exists("betareg-ext-betatree.rda")) {
  load("betareg-ext-betatree.rda")
} else {
<<ReadingSkills-tree>>
if(cache) {
  save(rs_tree, file = "betareg-ext-betatree.rda")
} else {
  if(file.exists("betareg-ext-betatree.rda")) file.remove("betareg-ext-betatree.rda")
}
}
@
%
Alternatively the model could be specified using a three-part formula
where the third part is the symbolic description of the partitioning
variables.

<<ReadingSkills-tree2, echo=TRUE, eval=FALSE>>=
rs_tree <- betatree(accuracy ~ iq | iq | dyslexia + x1 + x2 + x3,
  data = ReadingSkills, minsplit = 10)
@

The returned object is of class \code{"\Sexpr{class(rs_tree)}"} which
contains an object of class
\code{"\Sexpr{class(rs_tree[["mob"]])}"}. All methods for
\code{"\Sexpr{class(rs_tree[["mob"]])}"} can be re-used, e.g., the
\fct{plot} method (see Figure~\ref{fig:betatree}).

<<ReadingSkills-tree3, echo=TRUE>>=
plot(rs_tree)
@

Figure~\ref{fig:betatree} indicates that the data was only split into
two sub-samples. None of the three noise variables was selected in
order to perform a split, but only variable \code{dyslexia}. This
indicates that the relationship between the IQ score and the reading
accuracy doesn not depend on the noise variables which we would
expect. By contrast, the relationship between these two variables
differ for dyslexic and nondyslexic children. The Beta regressions
fitted two each of the two groups of children are illustrated in the
two leaf nodes. Note that the fitted models use the IQ score as
predictor for the mean as well as the dispersion.

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-tree-plot, fig=TRUE, height=7, width=10, echo=FALSE>>=
plot(rs_tree)
@
\caption{\label{fig:betatree} Partitioned Beta regression model for the
  \code{ReadingSkills} data.}
\end{figure}

\fixme{One could add the information about the significance tests.
In node~1 only dyslexia shows significant instability while the
noise variables are all nonsignificant. In node~2, dyslexia cannot be
used for splitting anymore and all other variables are still nonsignificant.
With only 19~observations, node~3 is considered too small to warrant
further splitting and hence no tests are carried out.}

<<ReadingSkills-tree-sctest>>=
sctest(rs_tree)
@

\fixme{maybe also show something else of the fitted model, e.g.}

<<ReadingSkills-tree-coef>>=
coef(rs_tree)
@

\subsection{Latent class Beta regression}

In the following we assume that the information if the children are
dyslexic or not is not available. Modelling the relationship between
reading accuracy and IQ score is now complicated by the fact that
latent groups exist in the data where this relationship is
different.

\fixme{Comment on: The group of non-dyslexic children is challenging
as it has rather high variance induced by two subgroups -- one with
essentially perfect accuracy and one with strongly increasing accuracy
by IQ score. In a model with observed \code{dyslexia}, this can be
captured by different variances in the two components. However, if
\code{dyslexia} is unobserved and a mixture model is employed to
infer the groups, then this is problematic. The subgroup with perfect
reading score will typically be selected as one component but then
not all parameters are identified. To adress this \dots}

We fit a finite mixture model with three components where
one component is used to capture those children who have a perfect
reading accuracy test score. Following
\cite{betareg:Smithson+Merkle+Verkuilen:2011} this additional
component is assumed to follow a uniform distribution on the interval
\code{coef} $\pm$ \code{delta}.
%
<<ReadingSkills-mix, echo=TRUE, eval=FALSE>>=
rs_mix <- betamix(accuracy ~ iq, data = ReadingSkills, k = 3,
  extra_components = extraComponent(type = "uniform",
    coef = 0.99, delta = 0.01), nstart = 10)
@

%
<<ReadingSkills-mix2, echo=FALSE>>=
if(cache & file.exists("betareg-ext-betamix.rda")) {
 load("betareg-ext-betamix.rda")
} else {
<<ReadingSkills-mix>>
if(cache) {
  save(rs_mix, file = "betareg-ext-betamix.rda")
} else {
  if(file.exists("betareg-ext-betamix.rda")) file.remove("betareg-ext-betamix.rda")
}
}
@

The argument \code{nstart} is set to
\Sexpr{rs_mix$flexmix@control@nrep}. This implies that the EM
algorithm is run \Sexpr{rs_mix$flexmix@control@nrep} times with
different random initializations and that only the best solution
according to the log-likelihood is returned in order to increase the
chance that the global optimum is detected. The EM algorithm does at
best only converge to a local optimum and the initialization
already determines the result.

The returned fitted model is of class \code{"\Sexpr{class(rs_mix)}"}
and has methods for \Sexpr{betamix_methods}. These methods re-use
functionality already available for finite mixture models directly
fitted using \fct{flexmix} from package \pkg{flexmix}. The
\fct{print} method shows the function call as well as provides
information on how many observations are assigned to each of the
components according to the maximum a-posteriori
probability. Furthermore it is indicated if the EM algorith converged
or not and in the case of convergence how many iterations were
performed.

<<ReadingSkills-mix3>>=
rs_mix
@

The \fct{summary} method provides more information on the estimated
coefficients as well as their standard errors. Internally function
\fct{optim} is used to determine the Hessian matrix needed for the
standard errors.

<<ReadingSkills-mix4>>=
summary(rs_mix)
@

Because only two components are freely estimated and the parameters
for the third component were fixed a-priori, the detailed information
on the estimated parameters is only provided for components 1 and
2. The regression part for the mean indicates that in the first
component the IQ score does not significantly affect the achieved
accuracy, while there is a positive significant effect of the
IQ score on accuracy in the second component.

A cross-tabulation of the cluster assignments of the mixture model
with the variable \code{dyslexia} indicates that no dyslexic children
are assigned to the third component. Furthermore, children assigned to
the first component have a high probability
(\Sexpr{round(prop.table(table(clusters(rs_mix),ReadingSkills$dyslexia),1)[1,2]*100)}\%)
of being dyslexic.

<<ReadingSkills-mix5>>=
table(clusters(rs_mix), ReadingSkills$dyslexia)
@

The fitted mean regression lines for each of the three components are
provided in Figure~\ref{fig:betamix} (left). The observations are
shaded according to their a-posteriori probabilities. The stronger the
shading is in blue the higher is the a-posteriori probability for this
observation to be from component 1. Red shading corresponds to the
second component and green to the third. For comparison on the right
the mean regression lines for the dyslexic and non-dyslexic children
of the model where an interaction with the variable \code{dyslexic} is
specified in the regressions for mean and precision are
provided. These mean regression lines are equivalent to those obtained
using model-based recursive partitioning.

\fixme{Comment on: With observed dyslexia, the increased heterogeneity
in the control group can be captured by differences in the precision
equation, i.e., in the variance. However, for unobserved dyslexia, it
is more appropriate to capture the increased heterogeneity in the
control group by two components, one of which corresponds to perfect
reading accuracy independent of IQ score.}

<<ReadingSkills-betamix-plot1, eval = FALSE, echo=FALSE, fig=TRUE>>=
ix <- as.numeric(ReadingSkills$dyslexia)
col1 <- hcl(c(0, 260), 90, 40)[ix]
col2 <- hcl(c(0, 260), 10, 95)[ix]
plot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19,
  cex = 1.5, xlim = c(-2, 2), main = "Partitioned model (dyslexia observed)")
points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1, col = col1)
@

\begin{figure}[t!]
\centering
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-betamix-plot3, echo=FALSE, fig=TRUE, height=5.5, width=10>>=
par(mfrow = c(1, 2))
ix <- as.numeric(ReadingSkills$dyslexia)
prob <- 2 * (posterior(rs_mix)[cbind(seq_along(ix), clusters(rs_mix))] - 0.5)
col3 <- hcl(c(260, 0, 130), 65, 45, fixup = FALSE)
col1 <- col3[clusters(rs_mix)]
col2 <- hcl(c(260, 0, 130)[clusters(rs_mix)], 65 * abs(prob)^1.5, 95 - 50 * abs(prob)^1.5, fixup = FALSE)
plot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19, cex = 1.5,
  xlim = c(-2, 2), main = "Mixture model (dyslexia unobserved)")
points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1, col = col1)
iq <- -30:30/10
cf <- rbind(coef(rs_mix, model = "mean", component = 1:2), c(qlogis(0.99), 0))
for(i in 1:3) lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2, col = col3[i])
<<ReadingSkills-betamix-plot1>>
cf <- coef(rs_tree, model = "mean")
col3 <- hcl(c(0, 260), 90, 40)
for(i in 1:2) lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2, col = col3[i])
@
\caption{\label{fig:betamix} Fitted regression lines for the mixture
  model with three components and the observations shaded according to
  their a-posteriori probabilities (left). Fitted regression lines
  for the partitioned Beta regression model with shading according to
  the observed \code{dyslexic} variable where dyslexic and non-dyslexic
  children are in blue and red, respectively (right).}
\end{figure}

\section{Conclusions}\label{sec:conclusions}


\section*{Acknowledgments}

This research was funded by the Austrian Science Fund (FWF): V170-N18.

\bibliography{betareg}

\newpage

\begin{appendix}

\section{Bias correction/reduction for gasoline yield data} \label{sec:gasoline}

Replicate results from \cite{betareg:Kosmidis+Firth:2010}

Compute all three flavors of estimators for the fixed precision Beta regression
model considered in \cite{betareg:Ferrari+Cribari-Neto:2004} and replicated
in \cite{betareg:Cribari-Neto+Zeileis:2010}.

<<GasolineYield-bias>>=
data("GasolineYield", package = "betareg")
gy <- lapply(c("ML", "BC", "BR"), function(x)
  betareg(yield ~ batch + temp, data = GasolineYield, type = x))
@

\begin{table}[b!]
\centering
\begin{tabular}{lrrrrrr}
  \hline
  & \multicolumn{2}{c}{Maximum likelihood} & \multicolumn{2}{c}{Bias-corrected} & \multicolumn{2}{c}{Bias-reduced} \\ \hline
<<ReadingSkills-bias-table, echo=FALSE, results=tex>>=
cf <- matrix(paste("$", format(round(sapply(gy, coef), digits = 5), nsmall = 5), "$\\phantom{)}", sep = ""), ncol = 3)
se <- matrix(gsub(" ", "",
  paste("(", format(round(sapply(gy, function(x) sqrt(diag(vcov(x)))), digits = 5), nsmall = 5), ")", sep = ""),
  fixed = TRUE), ncol = 3)
cfse <- cbind(cf[,1], se[,1], cf[,2], se[,2], cf[,3], se[,3])
cfse <- cbind(
  c(paste("$\\beta_{", 1:11, "}$", sep = ""), "$\\phi$"),
  cfse[, 1:5],
  paste(cfse[,6],
    c(rep("\\\\", 11), "\\\\ \\hline")))
writeLines(apply(cfse, 1, paste, collapse = " & "))
@

\end{tabular}
\caption{\label{tab:GasolineYield-bias} Coefficient estimates and standard errors (in parentheses)
for fixed precision Beta regression models with different types of estimators on gasoline yield data.}
\end{table}

The precision parameter is considerably dampened by bias correction/reduction

<<GasolineYield-phi>>=
sapply(gy, coef, model = "precision")
@

while the log-likelihood does not change much

<<GasolineYield-phi>>=
sapply(gy, coef, model = "precision")
@

Coefficients and standard errors in Table~\ref{tab:GasolineYield-bias}


\end{appendix}

\end{document}
