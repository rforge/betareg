\documentclass[nojss]{jss}
\usepackage{amsmath,amssymb,amsfonts,thumbpdf}

%% additional commands
\newcommand{\squote}[1]{`{#1}'}
\newcommand{\dquote}[1]{``{#1}''}
\newcommand{\fct}[1]{{\texttt{#1()}\index{#1@\texttt{#1()}}}}
\newcommand{\class}[1]{\dquote{\texttt{#1}}}
%% for internal use
\newcommand{\fixme}[1]{\emph{\marginpar{FIXME} (#1)}}
\newcommand{\readme}[1]{\emph{\marginpar{README} (#1)}}

\author{Bettina Gr\"un\\Johannes Kepler\\Universit\"at Linz
   \And Ioannis Kosmidis\\University College London
   \And Achim Zeileis\\Universit\"at Innsbruck}
\Plainauthor{Bettina Gr\"un, Ioannis Kosmidis, Achim Zeileis}

\title{Extended Beta Regression in \proglang{R}: Shaken, Stirred, Mixed, and Partitioned}
\Plaintitle{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned}

\Keywords{beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, \proglang{R}}
\Plainkeywords{beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, R}

\Abstract{
shaken and stirred (bias correction and reduction),
mixed (finite mixture model),
and partitioned (tree model)
}

\Address{
  Bettina Gr\"un\\
  Institut f\"ur Angewandte Statistik / IFAS\\
  Johannes Kepler Universit{\"at} Linz\\
  Altenbergerstra{\ss }e 69\\
  4040 Linz, Austria\\
  E-mail: \email{Bettina.Gruen@jku.at}\\
  URL: \url{http://ifas.jku.at/gruen/}\\

  Ioannis Kosmidis\\
  Department of Statistical Science\\
  University College London\\
  Gower Street\\
  London WC1E 6BT, United Kingdom\\
  E-mail: \email{ioannis@stats.ucl.ac.uk}\\
  URL: \url{http://www.ucl.ac.uk/~ucakiko/}\\

  Achim Zeileis\\
  Department of Statistics\\
  Universit\"at Innsbruck\\
  Universit\"atsstr.~15\\
  6020 Innsbruck, Austria\\
  E-mail: \email{Achim.Zeileis@R-project.org}\\
  URL: \url{http://eeecon.uibk.ac.at/~zeileis/}

}

%% Sweave/vignette information and metadata
%% need no \usepackage{Sweave}
\SweaveOpts{engine = R, eps = FALSE, keep.source = TRUE}
%\VignetteIndexEntry{Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned}
%\VignetteDepends{stats,betareg,car,lmtest,sandwich,strucchange}
%\VignetteKeywords{beta regression, bias correction, bias reduction, recursive partitioning, finite mixture, R}
%\VignettePackage{betareg}

<<preliminaries, echo=FALSE, results=hide>>=
options(width = 70, prompt = "R> ", continue = "+  ")
library("betareg")
combine <- function(x, sep, width) {
  cs <- cumsum(nchar(x))
  remaining <- if (any(cs[-1] > width)) combine(x[c(FALSE, cs[-1] > width)], sep, width)
  c(paste(x[c(TRUE, cs[-1] <= width)], collapse= sep), remaining)
}
prettyPrint <- function(x, sep = " ", linebreak = "\n\t", width = getOption("width")) {
  x <- strsplit(x, sep)[[1]]
  paste(combine(x, sep, width), collapse = paste(sep, linebreak, collapse = ""))
}
cache <- TRUE
@
\begin{document}

\section{Introduction}\label{sec:intro}

Beta regression is a model for
continuous response variables $y$ which assume values in the open
standard unit interval $(0, 1)$. Such response variables may stem from
rates, proportions, concentrations, etc. A double index regression allowing
to model the mean as well as the precision through covariates was introduced
by \cite{betareg:Ferrari+Cribari-Neto:2004}
and extensions by \cite{betareg:Smithson+Verkuilen:2006} and
\cite{betareg:Simas+Barreto-Souza+Rocha:2010}.
They employed an alternative parametrization of the beta distribution
such that the mean and variance of a random variable $y$ following a
beta distribution with parameters $\mu$ and $\phi$ (i.e.,
$y \,\sim\, \mathcal{B}(\mu, \phi)$) are given by $\E(y) = \mu$ and
$\VAR(y) = \mu(1-\mu)/(1+\phi)$. The beta distribution has the density
%
\begin{equation} \label{eq:density}
f(y;\mu,\phi) = \frac{\Gamma(\phi)}{\Gamma(\mu\phi)\Gamma((1-\mu)\phi)}y^{\mu\phi-1}(1-y)^{(1-\mu)\phi-1}, \quad 0<y<1,
\end{equation}
%
where $\Gamma(\cdot)$ is the gamma function. The double index beta
regression model is specified in the following way. Assume we have observations $i = 1, \dots, n$ of dependent
variable $y_i$.  The parameters $\mu_i$ and $\phi_i$ are linked to
linear predictors from the sets of regressors $x_i$ and
$z_i$. Different link functions can be used for the two
parameters. Suitable link functions include the logit or probit
for $g_1$ which links to $\mu_i$ and log or identity for $g_2$ which
links to $\phi_i$:
\begin{eqnarray}
  g_1(\mu_i) & = & x_i^\top \beta, \label{eq:link1} \\
  g_2(\phi_i) & = & z_i^\top \gamma. \label{eq:link2}
\end{eqnarray}

The coefficients $\beta$ and $\gamma$ can be estimated by maximum
likelihood (ML). The usual central limit theorem holds with associated
asymptotic tests, e.g., likelihood ratio, Wald, score/Lagrange
multiplier (LM).

The \proglang{R} package \pkg{betareg} which implements ML estimation
was introduced by \cite{betareg:Cribari-Neto+Zeileis:2010}. The main
model fitting function is \fct{betareg}. The interface as well as the
fitted models are designed to be similar to those for \fct{glm}. The
model specification is via a \code{formula} plus \code{data}. Because
two types of covariates need to be distinguished a two part formula is
allowed, e.g., \code{y ~ x1 + x2 + x3 | z1 + z2}. The covariates
\code{x1}, \code{x2} and \code{x3} are the covariates for the mean and
\code{z1} and \code{z1} are the covariates for the precision
parameter. The formula is processed using package \pkg{Formula}
\citep{betareg:Zeileis+Croissant:2010}. Function \fct{betareg}
internally uses function \fct{optim} as a general purpose optimizer to
maximize the log-likelihood. The fitted model has methods for the
following extractor functions: \fct{coef}, \fct{vcov},
\fct{residuals}, \fct{logLik}, \dots. Base methods for the returned
fitted model are \fct{summary}, \fct{AIC}, \fct{confint}. Further
methods are available for functions from \pkg{lmtest} \citep{betareg:Zeileis+Hothorn:2002}
and \pkg{car} \citep{betareg:Fox+Weisberg:2011},
e.g., \fct{lrtest}, \fct{waldtest}, \fct{coeftest} and
\fct{linearHypothesis}. Multiple testing is possible via package
\pkg{multcomp} \citep{betareg:Hothorn+Bretz+Westfall:2008} and structural
change tests can be performed using package \pkg{strucchange}
\citep{betareg:Zeileis+Leisch+Hornik:2002}.

\fixme{probably need somewhere to say something about model heterogeneity
that is not captured by the mean/dispersion regressors, and that it
may stem from observed or unobserved variables etc. Possibly, 
this can be done in an intro/motivation section or in the intro of
a section that encompasses both trees and mixture}

In this paper, we introduce extensions of the \pkg{betareg} package
where the fitting function is re-used in more complex models. We focus
on model-based recursive partitioning of beta regressions and finite
mixtures of beta regression models by building on funtionality
provided by packages \pkg{party}
\citep{betareg:Hothorn+Hornik+Strobl:2011} and \pkg{flexmix}
\citep{betareg:Leisch+Gruen:2011}.

\section{Bias correction and reduction in beta regressions}\label{sec:bias}

Reduction of bias using the generic algorithm of \cite{betareg:Kosmidis+Firth:2010}
building on earlier work \citep{betareg:Firth:1993,betareg:Kosmidis+Firth:2009}.
While \cite{betareg:Kosmidis+Firth:2010} consider only regressions
with a fixed precision parameter $\phi$, we extend their results to 
models with a regression part for $\phi$.



\section{Beta regression trees}\label{sec:trees}

Model-based recursive partitioning
\citep[MOB,][]{betareg:Zeileis+Hothorn+Hornik:2008} builds on the more
widely known method of classification and regression trees
\citep[CART, ][]{betareg:Breiman+Friedman+Olshen:1984}. As for CART,
the idea is to split the sample recursively with respect to available
variables (called \dquote{partitioning} variables in the following) in
order to capture differences in the response variable. While CART
tries to capture differences in the distribution of the response
variable (in particular with respect to location) directly, the aim of
model-based recursive partitioning is more broadly to capture
differences in parameters describing the distribution of the response.
In particular, this allows to incorporate regressor variables in a
parametric model for the response variable.

Here, we adapt the general MOB framework to the model-based partitioning
of beta regressions, called \dquote{beta regression trees} for short.
The aim is to capture differences in the distribution that are not yet
adequately described by the regressor variables through a forward search.
Basically, the approach proceeds by (1)~fitting a beta regression model,
(2)~assessing whether its parameters are stable across all partitioning
variables, (3)~splitting the sample along the partitioning variable associated
with the highest parameter instability, (4)~repeating these steps until some
stopping criterion is met. Thus, interactions and nonlinearities can
be incorporated by locally maximizing the likelihood of a partitioned
model. More precisely, the steps of the MOB algorithm adapted to beta
regression are the following, where $c_{ij}$ is the $j$-th partitioning
variable ($j = 1, \dots, p$) for observation~$i$.
%
\begin{enumerate}
  \item Fit a beta regression model with parameters $\beta$ and $\gamma$
        by maximizing the log-likelihood for all observations $y_i$ in the
	current sample.
  \item Assess whether the parameters $\beta$ and $\gamma$ are stable across
        each partitioning variable $c_{ij}$.
  \item If there is significant parameter instability with respect to
        at least one of the partitioning variables $c_{ij}$, split the
        sample along the variable $j*$ with the strongest association:
        Choose the breakpoint with highest improvement in the fitted
	log-likelihood.
  \item Repeat steps 1--3 recursively in the resulting subsamples
        until there is no significant instability any more or the
	sample size is too small.
\end{enumerate}

The MOB framework of \cite{betareg:Zeileis+Hothorn+Hornik:2008} is generic
in that it requires only the specification of a model with additive
objective function for which a central limit theorem holds. This is the
case for beta regression under the usual ML regularity conditions and the
main building blocks that the MOB algorithm requires are the contributions to the
additive objective function (in steps~1 and~3) and its associated score function
(in step~2). Here, the objective is the log-likelihood based on
Equation~\ref{eq:density}, its contributions $\ell(y_i; x_i, z_i, \beta, \gamma)$
are provided in Equation~\ref{eq:loglik} below. The corresponding score (or
gradient) contributions $s(y_i; x_i, z_i, \beta, \gamma)$ given in Equation~\ref{eq:score}
are employed for testing whether there are significant departures from zero across
the partitioning variables. More specifically, MOB uses generalized M-fluctuation
tests for parameter instability \citep{betareg:Zeileis:2006,betareg:Zeileis+Hornik:2007}:
fluctuations in numeric variables are assessed with a $\sup$LM type test
\citep{betareg:Andrews:1993} and categorical variables with a $\chi^2$ type test
\citep{betareg:Hjort+Koning:2002}. See \cite{betareg:Zeileis+Hothorn+Hornik:2008}
for further details and references.\footnote{An example of M-fluctuation tests
for parameter instability (also known as structural change) in beta regressions is
also discussed in \cite{betareg:Zeileis:2006} and replicated in
\cite{betareg:Cribari-Neto+Zeileis:2010}. However, this uses a double-maximum type
test statistic, not a $\sup$LM or $\chi^2$ statistic.}
For beta regression models, the required building blocks mentioned above are given by:
%
\begin{eqnarray}
  \ell(y; x, z, \beta, \gamma)
    & = & \log f(y; g_1^{-1}(x^\top \beta), g_2^{-1}(z^\top \gamma))
    \label{eq:loglik} \\
  s(y; x, z, \beta, \gamma)
    & = & \left\{ \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \beta},
                  \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \gamma} \right\}^\top,
    \label{eq:score} \\
  \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \beta}
    & = & \phi (y^* - \mu^*) \cdot (g_1^{-1})^\prime (x^\top \beta) \cdot x^\top,
    \nonumber \\
  \frac{\partial \ell(y; x, z, \beta, \gamma)}{\partial \gamma}
    & = & \left\{ \mu (y^* - \mu^*) + \log(1 - y) - \psi((1 - \mu) \phi) + \psi(\phi) \right\}
		 \cdot (g_2^{-1})^\prime (z^\top \gamma) \cdot z^\top
    \nonumber,
\end{eqnarray}
%
where $(g_j^{-1})^\prime (\cdot)$ ($j = 1, 2$) denotes the derivative of the inverse link function
with respect to the linear predictor, $y^* = \log\{ y / (1-y)\}$ and  $\mu^* = \psi(\mu \phi)- \psi((1- \mu)\phi)$,
with $\psi(\cdot)$ denoting the digamma function.

Beta regression trees are implemented in the \pkg{betareg} package in function
\fct{betatree} taking the following arguments:
%
\begin{Code}
  betatree(formula, partition, data, subset, na.action, 
    link = "logit", link.phi = "log", control = betareg.control(), ...)
\end{Code}
%
Essentially, almost all arguments work as for the basic \fct{betareg} function.
The main difference is that a \code{partition} formula (without left-hand side),
such as \code{~ c1 + c2 + c3} has to be provided to specify the partitioning variables $c_i$.
As an alternative, \code{partition} may be omitted when \code{formula} has three parts
on the right-hand side, such as \code{y ~ x1 + z2 | z1 | c1 + c2 + c3}, specifying
mean regressors $x_i$, dispersion regressors $z_i$, and partitioning variables $c_i$,
respectively.

The \fct{betatree} function takes all arguments and carries out all data preprocessing
and then calls the function \fct{mob} from the \pkg{party} package
\citep{betareg:Hothorn+Hornik+Zeileis:2006,betareg:Hothorn+Hornik+Strobl:2011}. The
latter can perform all steps of the MOB algorithm in an object-oriented manner, provided
that a suitable model fitting function (optimizing the log-likelihood) is specified
and that extractor functions are available for the optimized log-likelihood~(\ref{eq:loglik})
and the score function~(\ref{eq:score}) at the estimated parameters. For model fitting
\fct{betareg.fit} is employed (through a suitable convenience interface call \fct{betaReg})
and for extractions the \fct{logLik} and \fct{estfun} methods \citep[see also][]{betareg:Zeileis:2006a}
are leveraged. To control details of the MOB algorithm -- such as the significance level
and the minimal subsample size in step~4 -- the \code{...} argument is passed to \fct{mob}. (Note
that this is somewhat different from \fct{betareg} where \code{...} is passed to \fct{betareg.control}.)


\section{Finite mixtures of beta regressions}\label{sec:finite-mixtures-beta}

Finite mixtures are suitable models if the data is assumed to be from
different groups, but the group memberships are not observed. If
mixture models are fitted one aims at determining the parameters of
each group as well as the group sizes. Furthermore, the model can be
used to estimate from which group each observation is. In the case of
finite mixtures of beta regression models the latent groups can be
assumed to differ in their mean and/or in their
precision. Furthermore, the group sizes can depend on further
covariates.

The mixture model with $K$ components which corresponds to $K$
groups is given by
\begin{align*}
  h(y ; x, z, c, \theta) & = \sum_{k = 1}^K \pi(k ; c, \alpha) f(y ;
  g_1^{-1}(x^{\top}\beta_k), g_2^{-1}(z^{\top}\gamma_k)),
\end{align*}
where $h(\cdot ; \cdot)$ is the mixture density and $f(y | \mu, \phi)$
is the density of the beta distribution using the mean-precision
parameterization. Further the component weights $\pi(k ; \cdot)$ are
nonnegative and sum to one for all $k$. In the following they are
assumed to be determined by
\begin{align*}
  \pi(k ; c, \alpha) &= \frac{\textrm{exp}\{w^{\top}\alpha_k\}}
  {\sum_{u=1}^K\textrm{exp}\{w^{\top}\alpha_u\}}
\end{align*}
with $\alpha_1 \equiv 0$ to ensure identifiability. 

\cite{betareg:Smithson+Segale:2009} and
\cite{betareg:Smithson+Merkle+Verkuilen:2011} consider finite mixtures
of beta regression models to analyze priming effects in judgements of
imprecise probabilities. \cite{betareg:Smithson+Segale:2009} fit
mixture models where they investigate if priming has an effect on the
size of the latent groups, i.e., they include the information on
priming as a predictor variable $w$.
\cite{betareg:Smithson+Merkle+Verkuilen:2011} assume that for at least
one component distribution the location parameter is a-priori known
due to so-called ``anchors''. E.g. for partition priming, an anchor
would be assumed at position $1/K$ if the respondents are primed to
believe that there are $K$ possible events. The component distribution
for this anchor can be either assumed to follow a beta distribution
with known parameters for the mean and the precision or a uniform
distribution with known support.

Package \pkg{flexmix} \citep{betareg:Leisch:2004,
  betareg:Gruen+Leisch:2008} implements a general framework for
estimating finite mixture models using the EM algorithm. The EM
algorithm is an iterative method for ML estimation in a missing data
setting. The missing data for mixture models is the information to
which component an observation belongs. The EM algorithm exploits the
fact that the complete likelihood for the data as well as the missing
information is easier to maximize.  In general for mixture models the
a-posteriori probabilities of an observation to be from each component
given the current parameter estimates are determined in the
E-steop. The M-step then consists of maximising the complete
likelihood where the missing component memberships are replaced by the
a-posteriori probabilities. This implies that different mixture models
only require the implementation of a suitable M-step driver. Function
\fct{betareg.fit} provides functionality for weighted ML estimation of
beta regression models and allows to easily implement the M-step. 

The function \fct{betamix} allows to fit finite mixtures of beta
regression models using the package \pkg{betareg}. It has the
following arguments:
%
\begin{Code}
betamix(formula, data, k, fixed, subset, na.action,
  link = "logit", link.phi = "log", control = betareg.control(...),
  FLXconcomitant = NULL, extra_components,
  verbose = FALSE, ID, nstart = 1, FLXcontrol = list(), cluster = NULL,
  which = "BIC", ...)
\end{Code}
%
\begin{itemize}
\item Arguments \code{formula}, \code{data}, \code{subset},
  \code{na.action}, \code{link}, \code{link.phi} and \code{control}
  are the same as for \fct{betareg}.
\item Arguments \code{cluster}, \code{FLXconcomitant} and
  \code{FLXcontrol} are the same as for function \fct{flexmix}
  (in the latter two cases without prefix \code{FLX}).
\item Arguments
  \code{k}, \code{verbose}, \code{nstart} and \code{which} are used to
  specify the repeated runs of the EM algorithm using function
  \fct{stepFlexmix} and which model should be returned.
\item Because the formula for specifying the beta regression model is
  already a two-part formula a grouping variable can be specified via
  argument \code{ID}.
\item Further arguments for the component specific model are
  \code{fixed} and \code{extra_components}. \code{fixed} can be used
  to specify via a formula interface the covariates for which
  parameters are the same over components. \code{extra_components} is
  a list of \code{"extraComponent"} objects where the distribution of
  the component needs to be completely specified via \code{type} and
  setting the parameters through \code{coef} and \code{delta}.
\begin{verbatim}
<<results=tex, echo=false>>=
cat(prettyPrint(prompt(extraComponent, filename = NA)$usage[[2]], sep = ", ", 
  linebreak = paste("\n", paste(rep(" ", nchar("extraComponent") + 1), collapse = ""), sep= ""),
  width = 60))
@ 
\end{verbatim}
\end{itemize}

\section{Illustrative application}\label{sec:illustr-appl}

<<echo=FALSE, results=hide>>=
data("ReadingSkills", package = "betareg")
mean_accuracy <- 
  format(round(with(ReadingSkills, tapply(accuracy, dyslexia, mean)), digits = 3),
         nsmall = 3)
mean_iq <- 
  format(round(with(ReadingSkills, tapply(iq, dyslexia, mean)), digits = 3),
         nsmall = 3)
@ 

In the following we re-consider an analysis of reading accuracy data
for nondyslexic and dyslexic Australian children
\citep{betareg:Smithson+Verkuilen:2006}. The data consists of
\Sexpr{nrow(ReadingSkills)} observations of children aged between
eight years five months and twelve years three months. For each child
the scores on a test of reading accuracy (variable \code{accuracy}) as
well as on a nonverbal intelligent quotient (variable \code{iq},
converted to $z$~scores) as well as if the child is dyslexic or not
(variable \code{dyslexic}) are reported. The
\Sexpr{table(ReadingSkills$dyslexia)["yes"]} dyslexic children have a
mean reading accuracy of \Sexpr{mean_accuracy["yes"]} and a mean IQ
score of $\Sexpr{mean_iq["yes"]}$, the
\Sexpr{table(ReadingSkills$dyslexia)["no"]} nondyslexic children a
mean reading accuracy of \Sexpr{mean_accuracy["no"]} and a mean IQ
score of $\Sexpr{mean_iq["no"]}$.

\cite{betareg:Smithson+Verkuilen:2006} investigated whether dyslexic
children have a different score on the reading accuracy test even when
corrected for IQ score. They fit a beta regression with main and
interaction effects for \code{iq} and \code{dyslexic} for modelling
the mean and only main effects for both variables for the
dispersion. Their model as well as the comparison to the results of an
OLS regression using the logit-transformed \code{accuracy} as response
are given in \cite{betareg:Cribari-Neto+Zeileis:2010}. 

For illustrating the use of model-based recursive partitioning methods
we assume a beta regression model with \code{accuracy} as response and
\code{iq} as explanatory variable for the nodes. The score on the
reading accuracy test is therefore assumed to vary with the nonverbal
IQ score. In order to assess if also being dyslexic has an influence
on the relationship between these two variables the variable
\code{dyslexic} is used as a partitioning variable. Further random
noise variables are added as partitioning variables to indicate the
ability of the method to select suitable variables for
partitioning. One noise variable is drawn from a standard normal
distribution, one from a uniform distribution and the third is a
categorical variable which takes two different values with equal
probability.

<<ReadingSkills-noise, echo=TRUE>>=
data("ReadingSkills", package = "betareg")
set.seed(1071)
n <- nrow(ReadingSkills)
ReadingSkills$x1 <- rnorm(n)
ReadingSkills$x2 <- runif(n)
ReadingSkills$x3 <- factor(sample(0:1, n, replace = TRUE))
@ 

The model-based tree is fitted using \fct{betatree}. The first
argument is a formula which specifies the model for the node: We have
a beta regression where the mean as well as the dispersion depend on
\code{iq} to model \code{accuracy}. The second argument is a formula
for the symbolic description of the partitioning variables. The
formulas are evaluated using \code{data}. Additional control arguments
for the recursive partitioning method used in \fct{mob\_control} can
be specified via the \code{\dots} argument. In this case the minimum
number of observations in a node in order to allow a split is given by
\code{minsplit = 10}.
%
<<ReadingSkills-tree, echo=TRUE, eval=FALSE>>=
rs_tree <- betatree(accuracy ~ iq | iq,
  ~ dyslexia + x1 + x2 + x3,
  data = ReadingSkills, minsplit = 10)
@
%
<<ReadingSkills-tree0, echo=FALSE>>=
if(cache & file.exists("betareg-ext-betatree.rda")) {
  load("betareg-ext-betatree.rda")
} else {
<<ReadingSkills-tree>>
if(cache) {
  save(rs_tree, file = "betareg-ext-betatree.rda")
} else {
  if(file.exists("betareg-ext-betatree.rda")) file.remove("betareg-ext-betatree.rda")
}
}
@
%
Alternatively the model could be specified using a three-part formula
where the third part is the symbolic description of the partitioning
variables.

<<ReadingSkills-tree2, echo=TRUE, eval=FALSE>>=
rs_tree <- betatree(accuracy ~ iq | iq | dyslexia + x1 + x2 + x3,
  data = ReadingSkills, minsplit = 10)
@

The returned object is of class \code{"\Sexpr{class(rs_tree)}"} which
contains an object of class
\code{"\Sexpr{class(rs_tree[["mob"]])}"}. All methods for
\code{"\Sexpr{class(rs_tree[["mob"]])}"} can be re-used, e.g., the
\fct{plot}-method (see Figure~\ref{fig:betatree}).

<<ReadingSkills-tree3, echo=TRUE>>=
plot(rs_tree)
@ 

Figure~\ref{fig:betatree} indicates that the data was only split into
two sub-samples. None of the three noise variables was selected in
order to perform a split, but only variable \code{dyslexia}. This
indicates that the relationship between the IQ score and the reading
accuracy doesn not depend on the noise variables which we would
expect. By contrast, the relationship between these two variables
differ for dyslexic and nondyslexic children. The beta regressions
fitted two each of the two groups of children are illustrated in the
two leaf nodes. Note that the fitted models use the IQ score as
predictor for the mean as well as the dispersion.

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-tree-plot, fig=TRUE, height=7, width=10, echo=FALSE>>=
plot(rs_tree)
@
\caption{\label{fig:betatree}Partitioned beta regression model for the
  \code{ReadingSkills} data.}
\end{center}
\end{figure}

\fixme{One could add the information about the significance tests.
In node~1 only dyslexia shows significant instability while the
noise variables are all nonsignificant. In node~2, dyslexia cannot be
used for splitting anymore and all other variables are still nonsignificant.
With only 19~observations, node~3 is considered too small to warrant
further splitting and hence no tests are carried out.}

<<ReadingSkills-tree-sctest>>=
sctest(rs_tree)
@

\fixme{maybe also show something else of the fitted model, e.g.}

<<ReadingSkills-tree-coef>>=
coef(rs_tree)
@

In the following we assume that the information if the children are
dyslexic or not is not available. Modelling the relationship between
reading accuracy and IQ score is now complicated by the fact that
latent groups exist in the data where this relationship is
different. We fit a finite mixture model with three components where
one component is used to capture those children who have a perfect
reading accuracy test score. This additional component is assumed to
follow a uniform distribution on the interval \code{coef} $\pm$
\code{delta}.
%
<<ReadingSkills-mix, echo=TRUE, eval=FALSE>>=
rs_mix <- betamix(accuracy ~ iq, data = ReadingSkills, k = 3,
  nstart = 10, extra_components = extraComponent(
  type = "uniform", coef = 0.99, delta = 0.01))
@
%
<<ReadingSkills-mix2, echo=FALSE>>=
if(cache & file.exists("betareg-ext-betamix.rda")) {
 load("betareg-ext-betamix.rda")
} else {
<<ReadingSkills-mix>>
if(cache) {
  save(rs_mix, file = "betareg-ext-betamix.rda")
} else {
  if(file.exists("betareg-ext-betamix.rda")) file.remove("betareg-ext-betamix.rda")
}
}
@

<<ReadingSkills-betamix-plot1, eval = FALSE, echo=FALSE, fig=TRUE>>=
ix <- as.numeric(ReadingSkills$dyslexia)
col1 <- hcl(c(0, 260), 90, 40)[ix]
col2 <- hcl(c(0, 260), 10, 95)[ix]
plot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19, cex = 1.5, xlim = c(-2, 2))
points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1, col = col1)
@

\begin{figure}[t!]
\begin{center}
\setkeys{Gin}{width=\textwidth}
<<ReadingSkills-betamix-plot3, echo=FALSE, fig=TRUE, height=5.5, width=10>>=
par(mfrow = c(1, 2))
ix <- as.numeric(ReadingSkills$dyslexia)
prob <- 2 * (posterior(rs_mix)[cbind(seq_along(ix), clusters(rs_mix))] - 0.5)
col3 <- hcl(c(260, 0, 130), 65, 45, fixup = FALSE)
col1 <- col3[clusters(rs_mix)]
col2 <- hcl(c(260, 0, 130)[clusters(rs_mix)], 65 * abs(prob)^1.5, 95 - 50 * abs(prob)^1.5, fixup = FALSE)
plot(accuracy ~ iq, data = ReadingSkills, col = col2, pch = 19, cex = 1.5, xlim = c(-2, 2))
points(accuracy ~ iq, data = ReadingSkills, cex = 1.5, pch = 1, col = col1)
iq <- -30:30/10
cf <- rbind(coef(rs_mix, model = "mean", component = 1:2), c(qlogis(0.99), 0))
for(i in 1:3) lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2, col = col3[i]) 
<<ReadingSkills-betamix-plot1>>
cf <- coef(rs_tree, model = "mean")
col3 <- hcl(c(0, 260), 90, 40)
for(i in 1:2) lines(iq, plogis(cf[i, 1] + cf[i, 2] * iq), lwd = 2, col = col3[i]) 
@
\caption{\label{fig:betamix}Fitted regression lines for the mixture
  model with three components and the observations shaded according to
  their a-posteriori probabilities (left) and fitted regression lines
  for the partitioned beta regression model (right).}
\end{center}
\end{figure}


\section{Conclusions}\label{sec:conclusions}


\section*{Acknowledgments}

This research was funded by the Austrian Science Fund (FWF): V170-N18.

\bibliography{betareg}

\end{document}
