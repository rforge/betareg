We would like to thank the reviewers for their feedback. Please find
below our detailed reply to each of the points raised by them. 

%------------------------------------------------------------------------

Reviewer 1:

The paper:

1. It isn't clear to me what the phrase "double index" means (on
   pg. 2, "The double index beta regression model is ...")

Reply: We rephrased this sentence and now explain that in contrast to
a single index model there are in a double index model not only one,
but two linear predictors which are in the case of beta regressions
for the mean and the precision.

2. On pg. 5, the two occurrences of "gives that" are not grammatically
   correct and should be replaced with "reveals that".

Reply: Changed.

3. On pp. 8 and 9, I would recommend that the equations be given
   numbers, i.e., continuing the numbering on from (13).

Reply: Done.

4. For those users not familiar with betareg or flexmix, it would be
   helpful if the text on pg. 10 included an explanation of how
   covariates that determine the component weights can be entered into
   the model via the FLXconcomitant argument.

Reply: We added this explanation.

5. On pg. 15 (in the betatree example) it would be helpful to mention
   the print() function to users. It provides a nice summary of the
   partitioning and its associated statistics.

Reply: Added.

6. On pg. 19 (4th line of the Conclusions section) "allows to" should
   be "allows users to".

Reply: Done.

The package:

1. It would be nice to have an example for betatree of a quantitative
   variable that is used for splitting, although this is not essential.

Reply: We added a numerical variable which contains information for a
split in the example of the documentation for betatree.

2. Pursuant to comment 4 above, the betamix arguments are not entirely
   consistent with the format of betareg or betatree. The latter
   allows a 3-part formula with the third part specifying the
   partitioning variable(s), whereas with betamix the mixture
   component covariates have to be entered via the FLXconcomitant
   argument. However, after email correspondence with two of the
   authors, they have implemented the 3-part formula in betamix as
   well, and I presume the paper and documentation will be updated
   accordingly if the next release of CRAN occurs before the paper
   goes to production.

Reply: The new version has been submitted to CRAN along with
re-submitting the revised version.

3. That said, it also initially was a mystery to me how one extracts
   the coefficients and standard errors for the mixture component
   covariates. For users not familiar with flexmix, describing how to
   do this via either the coef(betamix_model, which = "concomitant")
   or summary(betamix_model, which = "concomitant") commands would be
   helpful.

Reply: We added code in the example where a concomitant variable model
is fitted using a variable based on dyslexia and showing the use of
these two functions for inspecting a fitted concomitant variable
model.

%------------------------------------------------------------------------

Reviewer 2:

Minor remarks:

o In the mixture regression, an additional component (as proposed by Smithson et
  al. 2011) is introduced to model (nearly) perfect scores. From my experience,
  practitioners do not make such assumptions a priori but rather fit a couple of
  models with different numbers of mixture components. I have tried this naive
  approach below and ended up with confusing results.
    > library("betareg"); data("ReadingSkills")
    > read3 <- betamix(accuracy~iq, ReadingSkills, k=3, nstarts=1000)
    > read2 <- betamix(accuracy~iq, ReadingSkills, k=2, nstarts=1000)
    > read3; read2
    [...]
    Cluster sizes:
     1  2 
    22 22 
    [...]
    Cluster sizes:
     1  2 
    10 34 
    [...]
    > table(clusters(read3), ReadingSkills$dyslexia)
        no yes
      1 21   1
      2  4  18
    > table(clusters(read2), ReadingSkills$dyslexia)
        no yes
      1  9   1
      2 16  18
    > c(logLik(read3), BIC(read3))
    [1]  44.02645 -61.56358
    > c(logLik(read2), BIC(read2))
    [1]  35.38319 -44.27705
  Both models yield two latent classes (in the case of k=3 presumably because
  the probability of one classed dropped below the threshold set in flexmix),
  but with vastly different estimates (k=3 reproduces dyslexia almost perfectly).
  I tried 1000 random starts to be sure that those are not effects of local
  maxima, but there seems to be a systematic difference - can you explain why
  this happens? Is this possibly an artifact of the data, or because of the
  comparably large number of near-perfect scores?

Reply: This is an interesting phenomenon, we did not realize
ourselves. Obviously this good two-component solution is hard to
detect using the EM algorithm with random initilization starting with
two components. That this is a hard problem for the EM algorithm is
not surprising because there is a subsample of respondents who have
identical response levels regardless of the explanatory variable
IQ. However, repeated runs of the EM algorithm will also detect this
good two-component solution when randomly initialized with two
components, e.g., see for example

library("betareg"); data("ReadingSkills")
set.seed(17)
read2 <- betamix(accuracy~iq, ReadingSkills, k=2, nstart=100)
read3 <- betamix(accuracy~iq, ReadingSkills, k=3, nstart=3)

Please note that the correct specification of the argument for
repeated runs of the EM algorithm is nstart and not nstarts.

By chance starting with tree components easily detects this good
two-component solution when dropping one component which becomes too
small. However, if the minimum size of the components is set to zero
(i.e., no components are dropped), in most cases an error occurs
because the EM algorithm converges against the degenerate solution
where the respondents with the perfect scores are assigned to the same
component which then has infinite precision. The model presented in
the manuscript fits this three-component solution where a special
component absorbs the respondents with perfect reading accuracy
scores.

o Model selection for mixture models is oftentimes based on the BIC (as in GrÃ¼n
  and Leisch 2008). Is this also sensible for beta regression models, since the
  log-likelihood in those is usually positive? 
  In a model with a negative log-likelihood, greater values indicate better fit
  and values close to zero are desirable, so -2*logL generates the deviance
  where higher values indicate worse models. In addition, the number of
  parameters multiplied by the log of observations (k*log(n)) is added, so that
  larger models get penalized.
  If the log-likelihood is positive, higher values are also desirable, but now
  they should lie above zero. By computing the generic BIC with -2*logL, the
  values get larger, therefore suggest "better" fit. Additionally, the
  interpretation is reversed (smaller values are desirable), and - my main
  concern - adding k*log(n) would be beneficial for larger models. A comment on
  that and how to deal with it would be fine.

Reply: To the best of our knowledge BIC works in general when fitting
and comparing statistical models (given the usual regularity
conditions). The log-likelihood is always maximized and hence -2 times
the log-likelihood minimized.

